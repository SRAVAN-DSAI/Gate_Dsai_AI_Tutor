this in-depth course provides a comprehensive exploration of all critical linear algebra Concepts necessary for machine learning you'll learn the mathematical foundations to excel in AI tdiv from lunar Tech developed this course she has created many popular machine learning courses machine learning is at the Forefront of the Innovation powering the most advanced and transformative systems for the companies like apple Tesla Netflix Amazon open Ai and many others it enables the creation of the intelligent systems that can predict Trends personalized user experience and automate complex tasks to develop these practical applications a deep understanding of the underlying mechanics is important this requires a solid grasp of mathematics behind the machine learning so all these technical details with a particular focus on linear algebra this all-encompassing course explores the linear algebra in an interactive and machine learning Focus manner welcome to the linear algebra for machine learning course you will acquire the critical principles needed to build optimize and analyze sophisticated machine learning models from designing customer algorithms to enhancing curent Technologies this course provides the mathematical foundations with vital interest for the those for pioneering advancements in machine learning for those dedicated to mastering the mathematical aspect and the technical details behind machine learning our extensive 26 plus hour course of fundamentals of machine learning within the mathematics boot camp as well as a separate course offers an in-depth exploration this extensive program includes certification and is tailored for individuals that are serious about advancing their career in the field of machine learning Andi engineering this crash course in mathematics will serve you as a great starting point by establishing a robust foundation in linear algebra you will be well prepared to excel as machine learning practitioner equipped with the mathematical knowledge that drives the Innovation and efficiency in this field so if you're ready I'm really excited and without further Ado let's get started welcome to the course on the fundamentals of linear algebra presented by Lun Tech Academy my name is D Vasan and today we are going to start with some basic concepts that are important for understanding linear algebra linear algebra is one of the most applicable areas of mathematics it is used by pure mathematicians that you will see in a universities doing research publishing research papers but also by the mathematically trained scientists of all disciplines this is really one of those areas in mathematics that you will see time and time again appearing in your professional life if you want to become a job ready data scientist or you want to do some handson machine learning deep learning and AI stuff but also linear algebra is used in cryptology it is used in cyber security and in many other areas of computer science and artificial intelligence so if you want to become this well-rounded professional you want to go beyond using libraries and you want to truly understand the uh mathematics and the technical side of these different machine learning algorithms from very basic ones like linear regression to most complex ones coming from Deep learning like architectures in neural network how the optimization algorithms work how the gradient descent works and all these other different methods and models then you are in the right place because you must know linear algebra such that you will understand these different concepts from very basic ones to most advanced ones in data science machine learning deep learning artificial intelligence data analytics but also in many other applied science disciplines so before starting this comprehensive course that will give you everything that you need to know about linear algebra first I'm going to tell you what we assume that you already know because linear algebra it comes from about third PA of Bachelors of different highly technical studies and here we are assuming that you already know certain Concepts so to ensure that this course stays really on the topic of linear algebra and that you understand all these Concepts really well for that we need to be able to know different topics so before we dive into this Concepts let's familiarize ourselves with the basic prerequisites and notation used throughout this course and you will really need to know this in order to understand these Concepts really well such that instead of memorizing you'll actually just hear me once or maybe twice and then every time you hear later on or you see it in the papers or in some algorithms you will recognize this is something that we already learned so some key prerequisites overview is here first of all to fully grasp the upcoming material you should be familiar with some basic concept like real numbers Vector spaces so you don't need to know this idea of vectors though you already most likely are familiar with this given that you know how to plot different lines you know the idea of exes and wise and how to plot these different graphs but here we are going to touch base on this every time when we come close to these Concepts I will refresh you your memory and we will go through this numbers the idea of norms and distance measures because when it comes to the vectors when it comes to the magnitude and all these different topics that we are going to discuss as part of linear algebra knowing the what Norm is and what is the definition of distance what is the length between two points when we plot it into two-dimensional space or three-dimensional space those are all very basic concept that usually use as part of a basic pre-algebra or just common algebra courses and lessons in order to truly understand what the new algebra is about to understand the direction of vectors the angle and then the dimensionality reduction how linear algebra is applied for instance in different algorithms in machine learning deep learning data science statistics you really need to understand this Cartesian coordinate system so this is not only important for linear algebra but I assume you already know it given that you have passed those other courses like calcul or usually they are covered as part of pre-algebra or algebra so the cartisian coordinate system I mean here understanding what is for instance the the common description of them for instance when you when we write like X and then y on the vertical axis and then we can we have here zero and then we can always plot this different plots you know we have a clear understanding what is this Y is equal to X line we understand how by knowing certain points we can plot different plots for instance that this is the Y is equal to X line that here it means that if we have here one then this is just one two this is two so we understand when we have the function of the line and we have a certain value at is our y coordinate or x coordinate then the corresponding coordinate can be found then you also need to know some basic things that I just didn't mention right now so for instance that the numbers here can be like 1 2 three up to Infinity so you understand this concepts of infinity and then here the same story then here we have minus one you know minus two uh and then this is then used later on and we will be pouch basing this one we will be describing our vectors and how we can visualize our vectors either two dimensional space like we have here because this is two dimensional so we have X and Y but we can also of course visualize it in three-dimensional Etc so this idea of basic coordinate system is really important usually covered as part of algebra if not pre-algebra then we have basic triog genetry which means that you need to have a clear understanding what sinus is what cosine is what tangent is and their reciprocals and here I mean that you know for instance what is cosine function what is s function you know that you have an understanding for instance that what is this line you know whether it's a sinus line or cosine line you have also an understanding what this Pi is one thing that I didn't mention but it it just goes around all these topics some basic things that you understand what is X what is y why we use them and this idea of variables and also you need to understand this idea of square or you know 90 degree angle and then Pythagoras Theorem here we have the same so what is this relationship between different sides of a triangle that is a very unique triangle and that has one of the angles as 90° and this idea of you know the sides how this relates to the sinus cosinus tangent cotangent and also how the Pythagorean Pythagorean theorem applies when we have triangular but it is is no longer with angle that is 90° what is the sum of all the angles of triangle so those are basic stuff that are com commonly covered as part of trigonometric lessons or part of General geometry then another prerequisite is this understanding of identities and equations in triog genometric lessons something part of which I already covered and this is goes around of basic having a basic un uh understanding of algebra and geometry those are super important to understand more Advanced Techniques from linear algebra then we have finally this idea of orthogonality perpendicularity in vectors so this also comes from geometry and from a trigonometric lessons so you understand that if we have for instance the two lines that don't have any intersections then we are talking about two orthogonal lines and otherwise for instance if we have and the two lines like this then we are talking about perpendicular vectors when you have two lines that are actually parallel so they don't have any intersection and you won't find any point that is common for the two so when it comes to this R so as part of real numbers and Vector spaces R represents the set of all real numbers so you can be dealing with for instance an integers like 1 2 three this can Al this will also cover all the negative numbers like -1 - 2 - 3 but also the floting numbers like 1. 223 and all the other numbers that you can think of those are the set of all real numbers so this is in one dimensional space right so you can see that I'm writing just one number you know two three and other numeric numbers then we have the idea of R2 R3 up to RN where now all these numbers they represent represent in this case the N it represents the N dimensional aidian space so when it comes to this idea of n dimensional numbers so for instance R2 here we just mean 2D plane so I'm pretty sure you are familiar with this idea of for instance xais and Y AIS here we are dealing with two dimensional plane so for every point that we can find here we can describe them by assigning them a value X so coordinate X and a coordinate y that's exactly what we mean by saying that the number can be represented in a 2d plane so here we are dealing with this two dimensional space this is our two dimensional Elan space and every number in here that is part of this R2 can be pictured here can be represented in this visualization so for instance if I have this number and let's assume that the value on the x-axis is two and we can see here that the corresponding Y is zero I can describe this number which I will call a I can describe this by writing down first the x coordinate which is two and then the y-coordinate which is zero so I'm then saying that a which is a point with x coordinate 2 and y coordinate Z it is part of my R2 and it's part of my two dimensional alian space when it comes to R3 similar thing we can do with that only in that case we need not just x axis and y AIS but we need to add our third dimension so here for instance when it comes to the r Tre then we need to do y AIS we need to have xaxis but also we need to have some Z axis so such that every time every point in the space we can then describe by x y and Zed coordinates so if we write it in terms of the vector something that we will see very soon as part of our first unit of this course we will then need to represent every number in this three-dimensional Alan Space by writing down first the x coordinate let's say one and then y coordinate let's say another one and then Z coordinate which is one or even better even easier let's use 0 0 0 which means that we are dealing with this initial number which is the center of this three-dimensional Alan space when it comes to the N dimensional or the higher dimensional spaces it's much harder to visualize therefore usually when it comes to visualizations we do usually we usually only visualize the onedimensional two dimensional and thre dimensional spaces above then it just no longer does make sense to visualize it but we definitely deal with them and they are part of Applied linear algebra so understanding this spaces is very important for analyzing vectors for their interactions and this holds not just for this two-dimensional and three-dimensional but really for multidimensional spaces let's now quickly Define this idea of Norm so the norm of a vector denoted by this V which you can see kind of like similar to the absolute value from pre-algebra you can see here that we have this double straight lines like from absolute value then we have the name of the vector or the variable name that we are assigning to our vector and then you might notice here on the top of this this Arrow this basically says that we are deing not with just a variable but really we are dealing with a vector this is really important because you can see that there makes a huge difference if we have for instance just V or V1 I have to say or just V those are really important and things that you need to keep in mind when it comes to Leading your algebra and trying to differentiate vectors from a point you will notice that when it comes to Norm we can represented it either by this notation or this usually it's a common notation in machine learning or in data science with this two bars and when we do this we automatically also know that we are dealing with aladine distance we call it also L2 norm and this is something very common and usually used as part of retrogression which is an application of linear algebra and it's used in regularization so we are regularizing our machine learning algorithms so when you get into machine learning you will see time and time again this notation so next time when you see this then you know automatically that you are dealing with L2 norm and L2 Norm which is also used a lot in machine learning it is referring to the usage of L2 Norm to uh in the retrogression and retrogression or L2 regularization is a very popular regularization techniques as part of machine learning so right now even you can see this intersection or linear algebra or this idea of norms in machine learning all right so now let's see why we call it actually L2 Norm or often referred as Eline distance so Eline distance you can see here which is also the in this case this V which describes the norm of the vector v is equ Al to square roof and then we have all these coordinates assuming that the vector comes from an N dimensional space so you can see here the RN the V Vector the idian distance or the norm of this vector v is equal to square roof and then V1 2 plus vs2 S Plus and all this in between numbers plus VN squ so here basically it means take square root of V1 2 V2 S Plus plus V3 2 blah blah blah plus VN s so basically take all the units that form this vector and then so are on this vector and use them Square them and then add them and then take the square root of that that's the distance or I have to say the norm of this Vector so why this is important this idea of norms and equity in distance beside of being used in machine learning and why is it used so Norms they provide a way to measure the size or the length of a vector in Vector spaces which means that when we want to measure a distance a similarity relationship between for instance vectors then it becomes much easier to use this idea an Elan distance is not only used in regularization techniques like L2 regularization or retrogression but it's also used in other machine learning or deep learning Al items as a way to measure the distance or the relationship or the similarity between two different entities those can be variables those can be two people that we want to compare in our algorithm or two entities um for instance the Norms or the Al and distance they are also used as part of K me algorithm something that you might have heard and if you follow later on the machine learning and the clustering section of machine learning you will see that Alan distance is used as part of C's algorithm that aims to Cluster observations into different groups so this also yet another highly applicable uh topic that you must know in order to understand different linear algebra top topics but also machine learning topics let's now talk about simple topic that we must know about and refresh our memory very quickly before moving forward to our next topic that is a prerequisite for this course so the cartisian coordinate system is just a fancy word of describing this idea of X and Y or XY Z when we just want to visualize them and showcase this numbers related to the space so we just learned the and I just quickly was talking about this idea of X and and Y and how we can visualize that in plain so the cial coordinate system is a framework for specifying points in a plane or a space using ordered list of numbers so we know for instance when we plot this then here we need to put X and Y in our two dimensional space R2 and we know that here in the middle we have zero and here we have 1 2 three four and the same here one two and then three four which means that everyone that is in the industry whether it's in mathematics in physics in data science or ml or AI we all universally agree on this system we know this is this ordered list of numbers and we know that if we have for instance a point here then for this point we know that the xaxis and Y AIS is definitely positive even if we know don't know the corresponding numbers and then once we have more General lines here so not General but specific lines then we even know the exact coordinates and values here and we definitely know that this number should be so the x coordinate should be between two and three so first we have the two and then tree and not the other way around so this ordered nature helps us to understand how we can put all these different numbers and organize them in our two dimensional space and we also know the corresponding y so we know that for instance our Y is not minus three because it's lying in here in this part of our coordinate system and not somewhere here where the y axis are negative and why do we know that because it's an ordered list of numbers that we can visualize in this 2D plane and here you also need to keep in mind and we need to remind ourselves about this idea of these four different parts that we got so we have our here the first part the second part the third part and then the four you know part of our coordinate system and here we we are dealing with a two dimensional plane but if we were to deal with the three-dimensional plane we no longer have just x-axis and y axis where X AIS were on the horizontal and y- axis on the vertical but we have our third line which is the Z so we have now three different dimensions so X Y and Z and we are basically extending our two-dimensional plane to three-dimensional so this system is fundamental for visualizing and working with vectors geometrically so then we can just use this two dimension uh plane in order to visualize this Vector for instance knowing what are all these points that appear on this Vector what is its direction where is it headed you know what is the beginning and then we can also find out all the so the relationship of these vectors with all the other vectors for instance if we have an other Vector here then we can use the coordinates of them and information about vectors to understand that we are dealing with two parallel vectors that don't have anything in common so no intersection points where to say if we have another Vector like this and we know that here we are dealing with perpendicular you know orthogonal vectors so this is why those this coordinates Cartesian coordinate system is important and it's not just important for linear algebra but just in general for mathematics and for data science and for AI and you will see this coordinate system time and time again in different visualizations even when you want to visualize the mean of your data or you want to visualize the probability distribution function describing your population from statistics or from data science you want to visualize for instance how your optimization is working or you want to visualize how your model is performing in terms of its evaluation Matrix for all these cases and for any visualizations this idea of the Cartesian coordinate system is going to become very handy let's now talk about this idea of angles and the idea of circles radian the pi as well as this degree sign so this comes usually from geometry or tonometry and this is very important when it comes to the vectors because when we have two different vectors then we want to understand their relationship do they form this less than 90° or so are we dealing with sharp corner sharp angle or with we are dealing with 90° angle so we are dealing with this type of vectors where we have you know 90° or we are dealing with um this type of vectors when the angle is 180° which is by the way uh something that we are referring as Pi and here is one thing that is important here is that it's not just Pi but it's Pi radians why because in mathematics we also have this idea of Pi which is usually a number that is 3.14 so we should not confuse this Pi with pi radians so the relationship between the two is something that we have also seen as part of our pre-algebra and algebra courses so if it's something that you want to just refresh your memory on this will be super helpful to check our very initial course on um all these Basics so pre-algebra so this number comes from per algebra and then this idea of P radians and just in general all this information about what is 180° what is this angle what is 360° and all the information that comes from triogen metry and geometry can be found in our corresponding course so the next topic is the unit circle unit circle is highly related to this idea of radians degrees cosine sign but also understanding the Cartesian coordinate system will help you to understand the unit circle so this also comes from theog gometry and geometry and it's basically a fancy way of saying we have x-axis we have y AIS we have here zero so our common Cartesian coordinate system only we are trying to focus on this part of the system where we have here one we have here one so on the x-axis we have one and then here minus one here minus one for y AIS and here y the Y is equal to 1 so we have here all these points and then we have the circle with the radius of one so here is this you know this is the radius and here we plot this circle and this will help us to understand this concepts of sinus cosinus you know the Theta is just variable that we use to describe the angle and for instance here we are dealing with 45° this angle is 90° this entire thing is 360° and half of it so this part only is 180° so those are all important part of understanding this idea of unit circle so you might have already guessed that unit circle refers to this idea that we have here one unit here one unit one unit one unit forming this entire circle so with the radius that is equal to one all right so this is something that is very easy and this comes from the geometry and pre and triog gometry uh you also need to understand this concept of the sinus and and cosinus and how sinus and cosinus are related to this what do we refer by the sinus and cosine you know what is this what are these points so 1 Z for instance we understand that here the x is equal to one and Y is equal to zero so here this point is simply 1 Z so this point and then we have 2 p radians so what is this idea of P so we know that a p Radian so P radians is simply the 180° which means that you also need to understand this concept of P2 which is simply the 90° so you can see here one thing that I forgot to mention you need to understand this concept the relationship between the pi and so Pi radians and radians and this unit circle you need to know that here the pi ided two is simply this angle and then the entire Pi is this angle and then this entire thing the entire angle with 360° is equal to 2 pi so 2 pi radians is simply this entire thing so those are very easy Concepts that come from geometry and trometry and if you want to refresh them then head towards those sores because this will help you to understand all this concept from scratch let's now continue our refreshment when it comes to so genometric identities and we just spoke about this unit circle we talked about the sinus cosinus it's really important to relate this back to bit more advanced topics coming from the same do domain and from the same area of mathematics and here we we need to know this concept before learning linear algebra few other things that um would be really great if you know but it's actually not a must to understand all these different topics it is the idea of Pythagorean identity so don't confuse this with Pythagoras Theorem this is the Pagan identity so this one that the square of the S of an angle plus the cosine squared is equal to one and all these different rules that go around the S and cosine and also the what is for instance the S 2 Theta which is equal to 2 s of theta and cosine of theta you know those are all different rules that would be handed to know and if you are so far I assume that you also know geometry and fundamentals to triogen ometry which means that you also know these trues but this might be just a great time to go ahead and quickly refresh your memory on these Concepts because those might become handy in your applied linear algebra and Applied Mathematics Journey but for now I would say this is not one of the most important things to know to learn this and to go through this course but just something to keep in mind so when it comes to the triog genometric equations uh this can become very handy later on when we want to prove something in linear algebra so to follow along it's actually a good idea to know for instance what is how you can solve this different equations and this will go back and refer to the unit circle that we just saw for instance if the sinus Theta is equal to 1 / 2 then you will need to quickly remember what is that angle for which the sinus is equal to 1 / 2 then you realize that is actually the angle where you take the p and remember that Pi is equal to 180° and that is the one corresponding to and then Pi / to 6 is simply 180 / to 6 so this is basically the 30 degree so those are things that you can do when you know for instance all these different sinus and cosinus so you have memorized for these different angles so what is the sinus and cosinus for 30° for 60° um let me actually remove this to make it easier so this type of problems is very easy to solve when we keep in mind and we memorize what are these different values for sinus and cosinus when it comes to different angles for instance for the angle equal to zero let me actually remove this and clean this part for better understanding so if we have for instance 0 degrees then we know that the sinus for this is zero and the cosine of this is one so we are basically dealing so if I plot a unit circle we are dealing with this number so remember that sinus and cosinus those refer to the Y and X on our unit circle so keep this one in mind so if the cosine Theta is then equal to 1 and the sinus so Y is equal to Z we are dealing automatically this number with this number and you can see that here the angle is also zero so here we are dealing with one and zero coordinate so this is our cosine of zero angle and this is then our sinus of zero angle so we automatically even from this graph can see very easy easily that the S of 0° is equal to 0 and the cosine is equal to 1 all right so let's quickly also refresh our memory on few other degrees so for the 30° which is simply the Pi / to 6 so this is 30° then the sinus or the Y AIS is equal to 1 / 2 and the cosine or the X x value x coordinate is equal to square root of 3 2 so we are dealing with this this corner or angle so 30° so even from here you can see that the coordinates make sense make sense then we have the pi for another famous value which is corresponding to the 45° it's simply this angle and for this angle the X AIS which is the coine so this number is equal to 1 / to 2 and then for the sinus the so the y coordinate is equal to 1 / 2 square root of 2 as you might have guessed because in this number the x-axis and y axis is equal to is the same so you can see that this distance and this distance is the same because we are dealing with this type of figure so here we have 45° here we have 45° so this values are the same and this is something that you would know knowing the pag Ian Pagan theorem so then you can go ahead and refresh your memory for the 60° so here I'm referring to the Pi / to three and then the 90° which is the very easy case this one obviously the x-axis is equal to zero so here you should have zero and the y axis is equal to one so here you should have one and so on all right so we went into quite detailed here but I think this is a very important topic knowing this idea of a trigonometric equations identities this idea of unit circle are super important because they are highly applicable to different fields in artificial intelligence data science machine learning and will definitely set you apart all right let's now talk about the law of signs and cosiness those are things that I won't be going on into too much details I just wanted to quickly showcase to you if you want to get the proof of those definitely check out our corresponding courses but for here I'm assuming that you already know so you know the law of signs which means that if you have this triangle you know you have this different sides so you have an angle a the corresponding side is a and then you have angle B corresponding side is B and then here C and the corresponding side is C then you know that a / to sinus of that angle is equal to B / to the sign of that angle and then is equal to C divided to the sign of that angle so basically take this value divide it to the sinus of this angle you know right in front of it is equal to taking this value and then dividing into the sinus of this angle so the proof of this low is outside so out of the scope of this course but knowing this will help you to understand different concepts and then the law of cosine is simply saying take the side of a Target angle so in our triangular we have here a we have here angle B and the C and if we go and look into this specific angle so angle C just randomly picking one of the three angles then the side right in front of that angle so the C c^ squ is equal to if we take this you know the other two sides forming that angle so A and B is equal to a s so this is just a constant a distance of this side a 2 + b 2 so this side squar minus 2 * a * B times the cosine of that angle this is what we are referring as the law of cosin quite easy we are not going to prove it again if you want to get the proofs make sure to check our other courses on the geometry and triog genetry we're almost done with the prerequisites just a quick refreshment we saw already the norm here is just a not EX exle what Norm is and on a specific two dimensional Vector when we have for instance that a vector is equal to three and four which means for the First Dimension let's say on xaxis we have three and then on Y axis is equal to four then the norm or the Alid distance so this is equal to we take the x value so three and then we Square it so V you can see here this is the case when n is equal to 2 this is simply equal to square Ro of v1^2 + v2^ 2 and as V1 is equal to 3 so this is our maybe I can make this just V1 and this is my V2 then the norm or the equan distance for this Vector so this thing is equal to V1 2 + v2^ 2 which is equal to 3^ 2 + 4 S and this value is square root of 25 and it's equal to 5 so let's now see the difference between aladine distance and the norm so you could see here the norm here we have just one vector like here and this Norm it has just two corresponding values into two dimensional space you see here we have just three and then four so this is V1 and V2 when it comes to the Alan distance this is kind of the generalization of this idea of Norm so the aladine distance between two points a and B in RN so in the N dimensional space is the norm of the vector connecting a to B so we see that the norm and the elidan distance are highly related to each other only we are talking about the norm when it comes to one vector but when we have this Vector a and the vector B this is simply the Alan distance so for the Aline distance we know already this idea of distance how we can measure it and you can see that this comes very similar to what we see here notation and here we are saying well we have this vector and then it has the two coordinates in N is equal to 2 in two dimensional space when it comes to the Alan distance Alan distance helps you understand what is this distance between two points in an N dimensional space so the aladan distance between two points let's say A and B in N dimensional space is the norm of the vector connecting a to B so for instance if we have a point a and we have a point B we are connecting this and this is the vector connecting these two points then the aladan distance is simply the norm of this Vector so this is the aladan distance so we can see that nor and the distance they are highly related to each other in the Alan distance we are using this idea of norm and specifically the norm two as I mentioned before so here you can see that the definition of aladine distance so the distance between A and B the two point is equal to square root of A1 - B1 2 + a and then here we have basically A2 - b 2^ 2 and then plus A3 - B 3 squ those are things that we cover as part of this dot dot dot and then plus up to the last point when we have a n minus BN 2 so here what we mean basically is that if we have two points here is a and here's B and this s vector and we know all these different points so A1 B1 A2 B2 A3 B3 blah blah blah and then here a n BN we know all these points lying here in this distance then we are taking them and using them to calculate the line distance so here for instance if we have point A and B so in this example let's do quick one specific example when we have a point a which has coordinates 1 and two so this is basically A1 A2 and then point B with points in it like B1 B2 you can notice that the da AB so the distance or the Eid in distance of these two points which is equal to the norm of this vector or here this is a and this is B and this is this Vector this is equal to Square < t of 4 - 1 so it takes the B1 so this is B1 and this is A1 takes the square and then says plus B2 minus H ^ 2 takes the square root of that and says this equal to 5 now you might be wondering but hey why do we do then instead of 1 - B1 2 we do B1 - A1 2 and the answer to this question lies in the uh properties that we learn as part of prealgebra because it doesn't matter when we take A1 - B1 squ or B1 - A1 squared because this squared ensures that it doesn't matter which one we take first and subtract the other now the proof of that is outside of the scope of this of course is this is part of pre-algebra but I just wanted to put this out there to ensure that you are seeing what we are seeing here because here it says A1 minus B1 but in this example we are taking instead depth B1 and we are subtracting A1 this is a common thing that we do in prealgebra and just in general in different cting distance or distance related cases so I just wanted to put this here to ensure that later on this is something that can be clear from the first view right and in here we will quickly refresh our memory on the Pythagorean theorem which basically says in the right angle triangle so if we have this type of triangle so here we have 90° this is a right angle triangle the square of the length of the the side opposite to the right angle so this side this we over refer C and this as B uh and then a those two are not very important but this is commonly referred by C so the the side opposite to the right angle then we know that the square of the C so c^ s is equal to a 2 + b^2 this is super important theorem and a fundamental principle for defining the Norms the distances in equity and spaces in and in many other applications so the angles play Cru Ro in understanding the direction of the vectors and you know how they can be measured in degrees or in radians we saw also the pi radian this idea of you know that the P radian is equal to 180° those are all very important when it comes to linear algebra and just in general application of mathematics in machine learning in Ai and other applications the relationships between this angle measurements and the triog genometric functions is foundational in solving different problems that are about these vectors and their orientations for instance this angle of s cosine you know what is this idea of tangent they are very important just to give you an idea the um uh Tang tangent is specifically used as part of the activation functions we call it tank activation function and knowing this tank will help you to understand the activation functions that I use as part of deep learning which are more advanced machine learning type of models and they are fundamentals in all these different new and Cutting Edge techniques large like large large language models Transformers encoder and decoder based algorithms Etc they're also important in this idea of computing dot products so very important and must know when it comes to linear algebra so this is just an simple example when it comes to this right angle triangle and Pythagorean theorem and how is applied I will skip this for now it's also important to understand this idea of orthogonality so the two vectors let's say A and B they are orthogonal to each other if their dotproduct is zero so later on as part of the vectors when we will talk about dot product we will see what we mean when we say that the dot product is equal to zero and here you can even see that if the a norm if the a vector so you see here and B Vector if those vectors if we multiply them to each other their dot product is equal to zero it means they are orthogonal so this angle that they form is equal to 90° orthogonality implies that the vectors from the from a right angle with each other they are in you know we we are dealing with that in R2 in R Tre so they are super important when it comes also to visualizing them correctly this concept is visually represented all this you know Vector a and then Vector B and they are perpendicular in the 2D uh coordinate system all right so when it comes to the applications of orthogonality orthogonality plays a crucial role in various aspect of linear algebra it's fundamental in defining Vector spaces subspaces in solving a systems of linear equation later on when we pass the vector ideas and we go on to the matrices solving linear systems so equations with many unknowns and then we use this idea of reductions or gausian reductions we will see how this idea of orthogonality can be important and how also it relates back to the norm of two vectors so it's fundamental in defining all these different identities and solving system of linear equations and also orthogonal vectors are used in finding the shortest distance from a point to the plane um something that is important when it comes to the optimizations and here you can see an example the vector a which is equal to 2 three and then Vector B which is equal to minus 3 and 2 you can see that when we multiply 2 by minus 3 so we obtain basically the dot product by the way this is something that we are going to cover also as part of this course but for now you can see that if we take this number we multiply with this so 2 * - 3 we take this number multiply with this so we take three and multiply with two you can see that this equal to minus 6 this is equal to 6 so - 6 + 6 is equal to zero so you can see that the dotproduct of these two vectors is simply equal to zero and this is what we are referring as orthogonality this means that these two vectors form a right angle where we see here this angle is equal to 90° why this prerequisites matter and why I meant those understanding this concept is very crucial they underpin this geometric interpretation of linear algebra they will help you to better understand these Concepts and not just to memorize them but really understand and later on when you go into your machine learning and AI journey and in your data science Journey seeing these Concepts will help you to better understand those different alori this optimization techniques what we mean when we say we want our optimization algorithm to move towards local minimum Global minimum but this idea of movement this idea of vectors later on will you will also understand this different concepts in deep learning how these models work how the neural networks work and those are essential Concepts that you need for solving different systems of linear equation a core part of this course they also help you in visualizing vectors spaces which are critical to understand this concept of linear algebra the applications of linear algebra when it comes to the real world applications so those are things that you can definitely must by following some of our other courses but for this course I assume that you are already familiar with this Concepts right so now we are ready to actually begin and with this prerequisites in mind you are prepared to start your linear arbra Journey we are going to learn everything in the most efficient way in such a way that you will learn the theory you are going to see many examples we are going to learn everything in detail but at the same time you're going to learn the must know Concepts and I'm not going to overwhelm you with this most difficult concept that you will not be seeing in your career I'm going to give you this bare minimum when it comes to really knowing and the must know for linear algebra such that you will be ready to apply linear algebra in your professional Journey whether you want to get into machine learning deep learning artificial intelligence data science knowing these different concepts in linear algebra you will be a pro in your field going to give you everything that you need the theory examples implementations everything in detail but at the same time you will be doing that in the most efficient and time-saving way so without further Ado let's get started let's Now quickly Define this idea of norm so the normal of a vector denoted by this uh uh V which you can see kind of like similar to the absolute value from pre-algebra you can see here that we have this double straight lines like from absolute value then we have the name of the vector or the variable name that we are assigning to our vector and then you might notice here on the top of this this Arrow this basically says that we are dealing not with just a variable but really we are dealing with a vector this is really important because you can see that there makes a huge difference if we have for instance just V or V1 I have to say or just V those are really important and things that you need to keep in mind when it comes to linear algebra and trying to differentiate vectors from a point you will notice that when it comes to Norm we can uh represented it either by this not ation or this usually it's a common um notation uh in machine learning or in data science um with this uh two bars and um when we do this we automatically also know L2 norm and this is something very common and uh usually used as part of um retrogression which is an application of um linear algebra uh and it's used in uh regularization so we are regularizing our machine learning algorithms so when you get into machine learning you will see time and time again this um notation so uh next time when you see this then you know automatically that you are dealing with L2 norm and L2 Norm which is also used a lot in machine learning it is referring to the usage of L2 Norm to uh in the uh regression and regression or L2 regularization is a very popular regularization techniques as part of machine learning so right now even you can see this uh intersection or linear algebra or um this uh idea of norms in machine learning the norm of this vector v is equal to square roof and then V1 S Plus V2 squ plus and all this in between numbers plus VN squ so here basically it means take square root of V1 squ then V2 squar plus V3 squ blah blah BL plus VN 2 so basically take all the units that form this vector and then so are on this vector and use them Square them and then add them and then take the square root of that that's the distance or I have to say the norm of this Vector we saw already the norm here is just a not example what Norm is in um on a specific two dimensional Vector when we have for instance that the vector is equal to three and four which means for the First Dimension let's say on xaxis we have three and then on Y axis is equal to four then the norm or the AL in distance so this is equal to we take the x value so three and then we Square it so V you can see here this is the case when n is equal to 2 this is simply equal to square root of V1 2 + v2^ 2 and as V1 is equal to 3 so this is our maybe I can make this just V1 and this is my V2 then the norm or the in distance for this Vector so this thing is equal to V1 2 + V2 s which is equal to 3^ 2 + 4^ 2 and this value is square root of 25 and it's equal to five so let's now see the difference between aladine distance and the norm so you you could see here the norm here we have just one vector like here and this Norm it has just two corresponding values into two dimensional space you see here we have just three and then four so this is V1 and V2 when it comes to the Alan distance this is kind of the generalization of this idea of Norm so the Alan distance between two points A and B in RN so in the N dimensional space is the norm of the the vector connecting a to B so we see that the norm and the elidan distance are highly related to each other only we are talking about the norm when it comes to one vector but when we have this Vector a and the vector B this is simply the Alan distance so for the Aline distance we know already this idea of distance how we can measure it and you can see that this comes very similar to what we see here notation and here we are saying well we have this vector and then it has this two coordinates in N is equal to two in two dimensional space when it comes to the AQ in distance Eline distance helps you understand what is this distance between two points in an N dimensional space so the aladan distance between two points let's say A and B in n dimens space is the norm of the vector connecting a to B so for instance if we have a point a and we have a point B we are connecting this and this is the vector connecting these two points then the aladan distance is simply the norm of this Vector so this is the aladine distance so we can see that the norm and the distance they are highly related to each other in the Alan distance where using this idea of norm and specifically the norm two as I mentioned before so here you can see that the definition of alodine distance so the distance between A and B the two point is equal to square root of A1 minus B1 2qu plus a and then here we have basically A2 minus b 2 2ar and then plus A3 minus B3 2 those are things that we cover as part of this dot dot dot and then plus up to the last point when we have a n minus bn^ 2 so here what we mean basically is that if we have two points here is a and here is B and this s vector and we know all these different points so A1 B1 A2 B2 A3 B3 blah blah blah and then here a n BN we know all these points lie in here in this distance then we are taking them and using them to calculate the Lan distance so here for instance if we have um point A and B so in this example let's do a quick one specific example when we have a point a which has coordinates 1 and two so this is basically A1 A2 and then point B with u points in it like B1 B2 you can notice that the da AB so the distance or the equid distance of these two points which which is equal to the norm of this um vector or here this is a and this is B and this is this Vector this is equal to square root of 4 - 1 so it takes the B1 so this is B1 and this is A1 takes the square and then says plus B2 - A2 2 takes the square root of that and says this equal to 5 now you might be wondering but hey why do we do then instead of 1 - B1 2 we do B1 - A1 2 and the answer to this question lies in the um uh properties that we learn as part of pre-algebra because it doesn't matter when we take uh A1 - B1 squ or B1 - A1 squ because this squared ensures that it doesn't matter which one we take first and subtract the other now the proof of that is outside of the scope of this um course is this is part of pre-algebra but I just wanted to put this out there to ensure that uh you are uh seeing what we are seeing here because here it says A1 minus B1 but in this example we are taking instead depth uh B1 and we are subtracting A1 this is a common thing that we do in um pre-algebra and just in general uh in different um eling distance or distance related cases so I just wanted to put this here to ensure that uh later on this is something that can be clear um from the first view why this is important this idea of norms and Al IND distance beside of being used in machine learning and why is it used so Norms they provide a way to measure the size or the length of a vector in Vector spaces which means that when we want to measure a distance a similarity a relationship between for instance vectors then it becomes much easier to use this idea an Alan distance is not only used in regularization techniques like L2 regularization or retrogression but it's also used in other machine learning or deep learning algorithms as a way to measure the distance or the relationship or the similarity between two different entities those can be variables those can be two people that we want to compare in our algorithm or two entities um for instance the um Norms or the Al and distance they are also used as part of K algorithm something that you might have heard and if you follow later on the machine learning and the clustering section of machine learning you will see that Elin distance is used as part of C's algorithm that aims to Cluster observations into different groups so this is also yet another highly applicable uh topic that you must know in order to understand different linear algebra top topics but also machine learning topics welcome to the course on the fundamentals of linear arbra my name is D Vasan and today we are going to start with some basic concepts that are important for understanding linear algebra linear algebra is one of the most applicable areas of mathematics it is used by pure mathematicians that you will see in universities doing research publishing research papers but also by the mathematically trained scientists of all disciplines this is really one of those areas in mathematics that you will see time and time again appearing in your professional life if you want to become a job ready uh data scientist or you want to do some handson machine learning deep learning and AI stuff but also linear algebra is used in cryptology it is used in cyber security and in many other areas of computer science and artificial intelligence so if you want to become this well-rounded professional you want to go beyond using libraries and you want to truly understand the uh mathematics and the technical side of this different machine learning algorithms from very basic was like linear regression to most complex ones coming from Deep learning like architectures in neural network how the optimization algorithms work how the gradient descent works and all these other uh different methods and models then you are in the right place because you must know linear algebra such that you will understand these different concepts from very basic ones to most advanced ones in the data science machine learning deep learning artificial intelligence data analytics but also in many other applied science disciplines so before starting this comprehensive course that will give you everything that you need to know about linear algebra first I'm going to tell you what we assume that you already know because linear algebra it comes from about third uh year of bachelors's um of different uh highly technical studies and um here um we are assuming that you already know certain Concepts so uh to ensure that this course Tes really on the topic of linear algebra and that you uh understand all these Concepts really well for that we need to uh be able to know different topics so before we dive into this Concepts uh let's familiarize ourselves with the basic prerequisites and notations used throughout this course and you will really need to know this in order to understand these Concepts really well such that instead of memorizing you will actually just hear me once or maybe twice and then every time you hear later on or you see it in the papers or in some algorithms you will recognize ah this is something that we already learned so uh some key prerequisites overview is here um first of all to fully grasp the upcoming material you should be familiar with some basic concept like real numbers Vector spaces so you don't need to know this idea of vectors though you uh already most likely are familiar with this given that you know how to plot different uh lines you know the idea of x's and y's and how to plot these different graphs but um here we are going to touch base on this every time when we come close to this Concepts I will refresh you uh your memory and we will go through this numbers the idea of norms and distance measures because when it comes to the vectors when it comes to the magnitude and all these different uh topics that we are going to discuss as part of linear algebra knowing the what Norm is and um what is the definition of distance what is the length between uh two points when we plot it in the two-dimensional space or three-dimensional space those are all very basic concept that usually you see as part of a basic pre-algebra or is uh common algebra ques and um lessons in order to truly understand what the your algebra is about to understand the direction of vectors the angle and then um the uh dimensionality reduction how linear algebra is applied for instance in different algorithms in machine learning deep learning data science statistics you really need to understand this Cartesian coordinate system so uh this is not only important for linear algebra but I assume you already know it given that you have passed those um uh other courses like calculus or usually they are covered as part of pre-algebra or algebra so the cartisian coordinate system I mean here understanding uh what is for instance the the common um description of them for instance when you when we write like X and then y on the vertical axis and then we can uh we have here zero and um then uh we can always PL this different plots you know we we have a clear understanding what is this um Y is equal to X line we understand how by knowing certain points we can plot different plots for instance that this is the Y is equal to X line that here it means that if we have here one then this is just one two this is two so we understand when we have the function of the line and we have a certain value where is our y coordinate or x coordinate then the corresponding uh coordinate can be found then um you also need to know um some basic things that I just didn't mention uh right now so for instance that the numbers here can be like 1 2 three up to Infinity so you understand this concepts of infinity and then here the same uh story then here we have minus one you know minus two uh and then this is then used later on and we will be uh touch basing this is when we will be describing our vectors and how uh we can visualize our vectors either two dimensional space like we have here because this is two dimensional so we have X and Y but we can also of course visualize it in three-dimensional Etc so this idea of basic coordinate system is really important um usually covered as part of algebra if not pre-algebra then we have basic triog genetry which means that you need to have a clear understanding what sinus is what cosine is what tangent is and their reciprocals and here I mean uh that you know for instance um what is cosine function what is s function um you know that you have an understanding for instance that um uh what is this line you know um whether it's a sinus line or cosine line you have also an understanding what this Pi is um one thing that I didn't mention but it it just goes um around all these topics some basic things that you understand what is X what is y why we uh use them and this idea of uh variables uh and also uh you need to understand this idea of a square uh or you know a 90° uh angle and then uh Pythagoras Theorem here we have the same so what is this relationship between different sides of the triangle uh that is a very unique triangle and that has one of the uh angles as 90° um and uh this idea of um you know the sides how this relates to the sinus cosinus tangent cotangent um and also um how the Pythagorean um Pythagorean theorem applies when we have uh a triangular but it is no longer with a angle that is 90° what is the sum of all the angles of triangle so those are basic stuff that are com commonly covered as part of uh trigonometric uh lessons or part of General geometry then another prerequisite um is this uh understanding of uh identities and equations in triog genometric um lessons something part of which I already covered and this is goes around of basic having a basic understanding of algebra and geometry those are super important to understand more Advanced Techniques uh from linear algebra then we have finally this idea of orthogonality perpendicularity in vectors for instance if we have um the two lines like this then we are talking about uh perpendicular vectors when you have two lines that are actually parallel so they don't have any intersection and you won't find any point that is common for the tube hi there so let's get started with our first module which is foundations of vectors in this module we are going to talk about fundamentals of linear algebra vectors we are going to make a differentiation with between scalers and vectors we are going to Define them so first we will learn the theory then we will Implement them into practice by plotting them by looking into different examples then we will look into this representation of vectors by looking into the magnitude and the direction of it and the representation of them just in general we are going to plot them in our coordinate system then we are going to see the common notational vectors and indexing of them vectors are super important when it comes to linear algebra and application of it and uh they matter not only in mathematics but beyond so uh vectors help us in many ways from figuring out how objects move to solving math problems in science and just in general in technology including in data science machine learning artificial intelligence Etc they are super useful tool so uh let's start our journey with looking into scalers so scalers they are just plain numbers and by definition a scaler is a single numeric volume often representing magnitude or quantity for example uh scalers can be describing um the temperature outside for instance the temperature of um a 22° uh can be represented by a scaler or a height of a person can be represented it's a scaler so let's assume we have a scaler that we will Define by a letter s it's just a variable this scaler is then equal to 22 for instance and we are measuring it in degrees so it means that uh if this s measures a room temperature then the scaler s which is equal to 22° which represents the room temperature it can be for instance 18° or 9° if it's very called uh it just measures a single volume it represents just a single number or it can be for instance 17 100 2.22 so all these they are just scalers they represent a single numeric volume they often represent a magnitude or a quantity very we will see that scalers they are a value that represent the magnitude of a vector so uh now when we are clear on this very basic concept of scalers let's actually move to this idea of vectors so by definition a vector is an ordered array of numbers which can represent both magnitude and direction in space so uh vectors they are bit more they represent bit more than scalers there are numbers that also show show direction like a car spitting down the highway or a bow uh being thrown for instance uh when it comes to our previous example we were using this uh uh room temperature as a way to uh think about the scaler a scaler for instance scaler that we just saw was this room temperature room temperature which was 22° when it comes to the vector Vector is different for Vector for instance we can have an example when a bird for instance bird it flies flies at 10 kilomet per hour and I also add here another information which will make this as a vector which is that it flies South so here as you can see what I'm doing is that I'm not just oh let me actually remove this part to make it easier to understand okay so uh in this example let me write it down that the example bird FES s at 10 kilomet per hour so you can see that I'm not just adding the scaler which is in this case the magnitude we will see very soon the formal definition of it so I'm writing down the speed I'm defining the speed but also the direction so I'm saying I know that the bird is flying south that's the direction and I know also the speed of it which is the magnitude so 10 kilomet per hour so here in the vector I have much more information than in the scaler because in the scaler I just got temperature room temperature single volue but in case of a vector I not only have um magnitude or speed like 10 kilm per hour but I have extra information which is the direction of it for instance flying to the South so let's now look into some real examples and plotting them to make more sense out of this idea of vectors and what is this magnitude what is the direction so let's assume we have a 2d plane so we have xaxis we have y AIS here like usual we have our z0 Center and we want to plot a simple Vector so uh usually the way we represent Vector in tutorials or just writing down is by writing the name of the vector this can be just a a random name let's assume that it's a v letter v and then on the top we are always adding this Arrow so this Arrow it says and it tells the person who is reading that we are dealing with the vector arrow on the top is that reference so let's assume this uh vector v it starts from the center of our coordinate system and it goes to this point so let's say in here this is our vector v so let's assume that this point in here is equal to 4 which means that the x coordinate is four and the y-coordinate is zero as the um uh Arrow it just as the point in here it has a a y value of zero so you can see that it goes straight from zero to this one to this point okay so what tells this Vector uh to us is that we have a value that describes the length of the vector so it goes from 0 to 4 which means that the length is equal to unit four so it's equal to four um and we have just learned and we were just talking about that the magnitude is the length in this case so the length describes the magnitude in this case so this means that the magnit ude of this Vector is equal to 4 and then um what else we can see here we can see the direction of the vector which means that the direction is also something that we can see here this is the direction of the vector so this going straight from this point to this point in a horizontal way so independent whether I plot this Vector from 0 to 4 in here or in here here or in here or in here or in here in all cases as long as the length is this I'm dealing with the same Vector because I am basically in this entire R2 space I have exactly the same Vector all I care is about the magnitude and the Direction Where will this Vector start and where will it end I am not interested I'm interested that the uh that the magnitude in this case the length is equal to the direction of the vector so let's now look into another example where we go a bit more difficult on our coordinates and on our Vector we already saw that we had this Vector where we went let me change the color so this was our vector v and it went from zero till 4 so this point to be more specific is so this Vector it goes the vector B it goes from 0 0 to 40 so the coordinate X was 4 and the Y was Zero now let's plot another one um where the direction is no longer horizontal for this Vector let's call it Vector W and for this Vector w we will again start with Z 0 so we will start again in here but this time we will go bit like this so let's say we go all the way to this point so this point has a value for an x- axis of three and for y axis it has a value of four which means it goes from this point to this point and this is the direction of our vector v so it goes to 34 because this point is 3 0 and this point is 04 so xaxis is 0o x coordinate and y coordinate is 4 so now you can see that the direction of this Vector is like this while the direction of the vector v was like this and like in case of vector v i again no longer care about where exactly my Vector W stars and ends but all I care is about its magnitude so the length and the direction so for instance I can have the same Vector in here the same Vector in here as long as the length the magnitude is the same and the direction I am dealing with the same Vector that's all I care so the magnitude and the direction is all that you care about all right so now about the length um that's uh something that you can see very easily from this specific example because by using the Pythagoras Theorem or P Pythagorean theorem we can see very quickly that as the length of this side of our uh right angle 30° so right triangle we can see that this side is three this side is four which means that this side is 5 because 4 2 + 3^ 2 then we take the square root of that square root of 25 and it's equal to 5 so the length or the magnitude of this vector v is simply equal to 5 all right this was about this uh specific vectors let's now look into the uh common representation of the vectors so we always use the magnitude as well as the direction you know to represent the vectors and they commonly are represented by two different uh ways let's now look into the first way that the vectors can be represented and then we will move on to the next one so when it comes to the vector v so we saw that vector v was moving from 0 till uh to the point of 40 so we can represent the vector B by 4 and zero when it comes to the vector w we can represent that uh Vector so Vector w we can again do the parenthesis and we can say that it's equal to 3 four so by using the coordinates from the coordinate system we can then represent our uh vectors so this is just one way of representing a vector another way of representing these vectors is by using this Square braces given that we are in a two dimensional space first we will mention here the four then we will mention the zero in here twoo so we can say three and four this is yet another way of represented the vectors in a two dimensional space so if we were to have a threedimensional space so let me actually show it on a new page so if we were to um if we were um to have vectors in three dimensional space so we are dealing with R3 so we have points that can be described by X Y and Z so coordinate space like this so X and and the Y and then the Z then every point so let's say we have this Vector then we had to represent it by a value let's say x uh X1 y1 and Z1 or um better let me actually use a different letters a b and c and this would be my vector v and I could also represent this Vector B is the same so vector v can be represented as a b and c so one thing that you can notice is that unlike the R2 now I have three different entries what we are also referring as rows and we just got one column so um we can uh often represent and usually that's a common way of representing vectors by using this um columns so columns help us to represent our vectors and you can see very clearly then when it comes to the two dimensional space so when we have R2 so then our vectors have just two rows so three and four four zero like in here when it comes to three dimensional space we have three entries and so on so the same holds of course also for for instance R5 then for R5 um our vectors so coordinate space can be for instance x y Zed and then GMA and then let's say Delta and then the coordinates uh of a vector in that space can be V and then arrow is equals sh and then we would have uh let's say A B C D E you get the idea so depending on the space the coordinate space and the dimension of that space then the corresponding vectors can be represented accordingly so the vectors are quantities that have both magnitude and direction as we just so distinguishing them from scalers which only have magnitude so we saw that the scalers got only magnitude while in case of vectors we saw both for the vector v and for the vector w we didn't we didn't only have the magnitude so the length of the vector but also the corresponding Direction so uh when it comes to the um vectors so the this is exactly what we just saw in our example a vector in a two dimensional space so in R2 uh can be represented by using this Square braces and the corresponding entries for X and Y where X is basically the x coordinate in our coordinate system so in our X and Y system whenever you have this x and y coordinate then uh this x coordinate Will then describe your magnitude and the y coordinate Will then describe your second entry that you need to put when representing your vectors so here the X and Y indicate the movement in the horizontal and in the vertical Dimensions respectively so for X's it's always the x coordinate so how far you move towards the horizontal Direction in here in here or independent in here so always take the x coordinate that is the value that you need to put first and then the Y need to be put it in here so indexing in vectors when it comes to the um indexing the standard mathematical notation uh indices in the N vectors goes from I is equal to 1 to I is equal to n so the um notation here can be bit ambiguous so AI uh could mean the E element of AI uh the a vector or the each Vector in a collection so let's start with a simple one and then move move on to this next part so what this means and what this means we will look into now so uh usually uh when we have a um n dimensional space we are having hard time visualizing it therefore we use this two dimensional space or maximum three-dimensional space in order to get an understanding of what these vectors are so we just s examples of them uh when uh creating our vectors in um V and V uh and W in uh R2 and also in R3 but we can have similar vectors also in R4 in R5 or all the way down to RN where n can be 100 200 500 any number as large as you want the thing is is that visualizing R 4 R5 RN is very hard but we can still benefit from this great properties of the vectors metrices and in general linear algebra in order to describe different things that have more than three dimensions therefore we have this a bit more ambiguous notation where we use r n and this n can be any real number and it can be all the way to Infinity so very large number and uh let's say we have a vector in this RN then this Vector is usually described by using similar Square uh brackets like before only with uh more entries so like before we got just one column so that's something that we didn't uh change but here we have instead of just two entries or three entries like in the two dimensional or three dimensional spaces now we have A1 A2 A3 all the way down to a n minus one and a n so we got in total n elements in our column and this describes our uh single Vector so this Vector in an N dimensional space this we can call also a so one thing that we just saw is that it was saying in our definition and notation that uh we might also be dealing with the E Vector in a collection which means that sometimes you will see this while here the A1 A2 they are vector themselves so here we saw that these are just entries so A1 is a number A2 is a number A3 is a number a n is just a number but it's also possible uh when you have a much more difficult and complicated case that you got an A let's write it down with a capital letter A which is equal to A1 or let's actually remove this so we got let's say A1 A2 A3 all the way down to a n minus one and a n where you can already see what is going on so instead of having just a number as an entries instead we have vectors in here so our first element is actually Vector our second element is actually Vector so A2 Arrow A3 Arrow all the way down to a n arrow so while here this can be for instance some numbers let's say one one one all the way down to one one here we have a vector vector another vector and all the way down here yet another Vector where for instance let me remove this part where for instance A1 arrow is actually equal to A1 1 A1 2 A1 3 all the way down to A1 n one thing that you will notice here is that unlike in here here I got double indices so I got here a11 and then A1 2 and then a13 all the way to A1 n so the first index it doesn't change as I have here a one so I'm writing down the index corresponding to this Vector but the second index it changes per entry indicating which element specifically in the vector I'm talking about so from the first index you can identify the vector that I'm referring to which is A1 and from the second index you can see the corresponding um entry or the volume you that that um element is positioned in this Vector so you can see that this values for instance in the um Vector one so A1 to be more specific but then it is in the first position this is in the second position in the third position all the way down to the end position so this is something that is really important to understand well because this notation is going to appear time and time again across various applications of matrices and vectors so is really important to understand well therefore I want to go one more time through this to make sure that we are clear on what this indexes represent so whenever we have an index uh an a vector that we want to uh represent and it's um it has just um it is just a vector which means that it's not a nested vector vector in a vector then um we can Define it by let's say a and then on top an array and it's equal to and here we can have A1 A2 all the way down to a n so you can see what we are also referring as dimension of this Vector is equal to n by 1 so I got n entries and just one column so n by one which means that this already gives me an indication that most likely this A1 is a number this A2 is a number this a and is a number so let's say this equal to 1 2 uh three blah blah blah and then here I have let's say 100 but if I'm dealing with the nested Vector later we will see that this can be represented by a matrix then um I can also Define this by capital letter A which is a common way to refer to either matrices or nested vectors and then this is equal to A1 Arrow A2 Arrow A3 Arrow this already sends a message to the reader that we are dealing with no longer U constants within a vector but rather vectors in a vector and uh what can we see here is that the dimension of this nested vector or which we can also refer to as a matrix here the number of rows so the number of entries this elements we can see it's equal to n but then this time the number of values that form these vectors is no longer one because we are not dealing with just a constant this is not some constant but rather this is yet another Vector so let's assume this Vector has a length of M so let's say this has a length of M then the dimension of this Matrix a is equal to M so something that we will see also when talking about matrices so let me actually clarify this bit more for better understanding let's say we look into one of those um one uh one other example of an entry so let's say we look into this specific Vector which is in the uh the third uh vector within this Vector capital A so this A3 Vector so one thing to see here already is that I assumed that these vectors they got M elements and keep in mind that all these vectors they should be of the same size so it means that I already know that this specific Vector A3 has M elements so m elements so I'm representing this uh A3 Vector from here I'm taking this out from this entire uh nested a vector and I just want to represent this and now unlike this elements that got an arrow on the top this time I will have uh constants forming the A3 Vector so I no longer have vectors but I have elements in it so in here I will have a a let me actually write down all the A's but to refer and to make sure that I recognize that I'm dealing with the third a vector so a Tre Arrow here I will put three Tre all the way here Tre so they all come from the same third A3 Vector but then their positions is different because this is let's say uh one two and then all the way down to Ed position so this indices help us to keep track what are the um position that these values are taking part in the vector A3 errow this might seem bit complicated at the moment but once we move on onto bit more complex material like uh matrices it will make much more sense this is bit of an extra I just wanted to Showcase this but this is what uh is at its core and what you need to uh understand at the moment to understand this concept of vectors so you need to know that vectors can be represented by this arrow on the top so let's say Vector a and it has let's say n elements then you can write the square brackets and then you will need to mention A1 A2 all the way to a n which means that you have n different entries describing your vector so you have A1 which is the first element in your vector A2 the second element all the way to a n which is the end element where here you can see for instance so if I had here A3 that uh A1 is simply equal to one A2 is equal to 2 A3 is equal to 3 all the way to a n is equal to 100 so this numbers I'm basically taking and I'm representing them I'm putting them in here within Square braces in order to get a representation of my Vector so my Vector a has all these different entries and different entries and it starts with one and it ends with 100 this is a vector and then when it comes to the vectors within vectors here we need to be a bit more careful CU here we not just have uh constant values forming a vector but we have vectors that form yet not vectors so our Vector a our nested Vector a which we uh later will refer as Matrix a has actually entries that also are vectors so we have a 1 Vector A2 Vector A3 Vector they are not just constants but only own they are vectors so here for instance we have defined also an example of it we have said let's look into this third specific Vector that is part of a which is A3 uh vector and uh that one has M different elements here we have then the index referring to the which Vector from the nested Vector a it is which is the third one because we have taken it from here but then on its own this Vector has different members and different members to be more specific therefore we have also an index to keep track of the position of this value one to up to M and this can be yet another uh this time it can contain some elements an example of which is for instance Z 1 2 all the way to let's say 500 and this can be different numbers it doesn't need to be ordered it doesn't need to have a specific pattern they can be just random numbers describing this A3 Vector so hopefully this makes sense if it doesn't don't worry because we are going to see this time and time again I just wanted to give you a brief of an intro such that you can uh remember this when we come uh back to bit more uh complex topics like uh indexing in matrices so now let's talk about special vectors and operation here we are going to talk about zero vectors unit vectors the concept of sparcity in vectors as well as vectors in higher Dimensions like we just saw about this n dimensional space we will also talk about different operations we can apply when it comes to vectors like uh addition subtraction and then later on in the next module we will also talk about multiplication we will also be looking into the properties of vector addition after we have looked into some detailed examples when it comes to operations on vectors all right so let's start with the zero vectors and unit vectors when it comes to zero vectors you can see here already that um the zero and arrow on the top it basically refers to the vector like we saw before only with the difference that all its members are zero so you can see here that we have zero and then an arrow and then underneath here we have some number tree and then this is described by this common representation with the square braces and then three different members z0 0 so all zero and then it says in R Tre okay so why are we doing this well uh when it comes to uh different linear Lal operation sometimes we just need to add zero vectors or we just want to create zero vectors it's just easier to work with you we want to uh just create an empty uh Vector we want we know the length but we want to keep it empty such Laton we can add something on the top or knowing that when we add a zero on a number the number stays the same we can make use of this property to uh do different um uh tricks when it comes to programming in Python in SCAR or in C++ Etc so therefore this idea of zero vectors can become very handy now one thing that you need to notice here is that we are not just writing down this zero to emphasize we are dealing with the vector but like uh before we have this error on the top emphasizing that we are dealing with a vector then what we are doing is that we are also adding the dimension of this Vector so in what dimension in what space are we um uh creating this zero Vector that this Vector is located is it in r R 2 in RN in R3 in this specific case you can see that in this example the uh index that we got here is three which basically indicates we are dealing with a zero Vector in threedimensional space so in the R3 uh in general we would just note this by n keeping the uh notation general which means that we are dealing with 0 0 all the way down to zero so it has n one dimension in r n all right so this is about zero vectors it is just a way to uh make our programming life easier also to use it in different uh algorithms when it comes to bit more advanced algebra uh the next type of special vectors that we will look into is this unit vectors so vectors with a single element equal to one and all the others zero denoted as EI for the E unit Vector in N dimensions are referred by unit vectors so uh what we mean here when it comes to the unit vectors um if we have for instance E1 it means that we have a vector where the e in this case the first element is equal to one so you can see that E1 is equal to 1 0 0 so in the first element we got one and the remaining is zero and this is really important that we are dealing with vectors that contain only elements of zeros and ones and the only member that is equal to the only element in that Vector that is equal to one is the E element in the entire Vector all the remaining ones are zero and you can see here that the dimension is no longer specified but just the um index of the entry where the um uh the uh one is located so let's look at another example in here for instance when it comes to the um uh unit Vector yet another unit Vector is E2 which basically means that in the second element so in the second place uh the uh Vector contains one and all the other members are zero so here you can see Zero here it can see Zero only in the second element we have one and then in the E3 what we have here is that the third element is one and all the other ones are zero so let's actually look into uh one um bigger Vector uh in higher Dimension to make it even more sense so first I will Define and assume that we are dealing with a vector in RN so in an N dimensional space this gives me an idea that we are dealing with um so we are not dealing with nested Vector we are dealing with a simple and dimensional Vector so it has n rows and one column so using the square braces I'm going to represent my Vector so I have all these different members n members C so e let's say it is E5 so what does this mean it means that I is equal to 5 and this I element so the fifth element is equal to one and all the other entries the elements in this Vector are zeros so let's look into this is z0 0 Z I'm approaching the fifth element in my Vector so it's this one this is one and the remaining all zeros so this is a unit Vector in an N dimensional space and I'm defining it by E5 because my fifth element is equal to one now those are very handy when it comes to some other uh techniques in linear algebra and just in general think about techniques like um uh row etum form solving linear equation something that we will see as part of the next unit so many things um we can do by using unit vectors unit vectors are super important so you need to understand this concept uh very well such that later on you will understand uh more advanced concepts in linear algebra let now look into the topic of sparsity in vectors so by definition a sparse Vector is characterized by having many of its entries as zero so its parity pattern indicates the position of a nonzero entries so uh what we are basically saying is that if we are dealing with a vector that contains too many zeros we are dealing with the sparse Vector so uh this sparsity pattern indicates uh all Al the positions of a nonzero elements so um if we have um unit Vector it means that we are already dealing with a sparse uh Vector this is a concept that is super important when it comes to linear algebra but also in general data science machine learning and AI because having a spity in your vector it means that you don't have much of an information usually a value zero it means you don't know much about that specific volum and if you got just too many of zeros and too few numbers which do um provide information it means that you are dealing with a vector that doesn't provide you much information and there's always a problem when it comes to data science machine learning and AI so sparcity is something that you need to be aware of you need to know how to recognize it and you also need to know whe there's a problem in your specific case or not so let's look into an example let's say we are dealing with this Vector X that has five different elements so X is a vector coming from um five dimensional space so we have for instance an element of three the first entry then we have z0 in the second and third uh entries then we have an entry um four which coincident also contains value four and then the last element in our five dimensional Vector X is equal to zero now what do we see here we see that the majority of elements of a vector X is equal to zero because we got in total five elements and then we got three of it actually uh being equal to zero and only two of them containing information like equal to three and four so only two elements that are not zero so non-zero elements it means that 3 / to 4 which is basically 60% 60% of all the entries in the vector X are equal to zero so the 60% it means that is above half so above 50% 60% of all the information in this Vector um the majority is simply equal to zero this type of vectors we are uh calling sparse vectors and sparcity is really important concept uh that we need to keep in mind later on so uh while we can visualize vectors in two and three dimensions in linear algebra like we just saw in case of this n dimensional vectors visualizing uh the this type of higher dimensional vectors becomes very difficult so uh this mathematical flexibility uh to work with uh this type of uh information so when we can represent uh information many with many entries we can represent it by vector s which we can actually not visualize becomes very handy for complex data structures for different simulations in physics and much more so uh we just saw in couple of examples uh how we can represent vectors in a high dimensional space using this Square braces and this common Vector notation representation we saw that in an N dimensional space we could uh very easily represent this uh very large Matrix or vectors uh by just um using this Vector representation for instance if we got a vector that had any different entries where n is for instance thousand so let's say we have thousand then uh we can represent uh this uh vector or this information by using common Vector notation so A1 A2 all the way to a th000 so of course we cannot visualize this it just doesn't make sense we can visualize two dimensional vectors we can visualize three dimensional vectors but we cannot uh visualize thousand dimensional vectors so Vector that comes from uh r, but what we can do is still make use of this very useful information in order to uh do different operations when and later on we will see that uh this property and specifically this part of linear algebra it helps us to work with vectors in any number of Dimensions whether thousands million billions this mathematical flexibility is super important for more complex data structures uh for metrix multiplications when for instance we are doing different uh algorithms including how we can represent uh very large matrices very large feature spaces all this different information we can represent just by making use of vectors coming from this specific uh part of linear algebra let's now finish of this module by looking into some applications of vectors so one common application of making use of vectors is uh when we are performing different operations while having words and we want to count those words so this is a super common application of vectors and we can account this words and you can even plot a histogram over how often each of these words appear in a document so a vector of a length n for instance can represent the number of times each of these words in a dictionary of n words appears in a document so uh just for the sake of Simplicity let's assume that we got um dictionary that contains only three words of course in the reality uh the um dictionary what we also refer often as Corpus it contains much many much more many words but for the Simplicity we will assume that we just got three different words in our dictionary so that's a total now let's assume that we got a document uh with these different words and we want to count how many times each of those words that we got in dictionary actually appear in our document so uh if our document is described by this Vector so it contains an entry of 25 2 and zero it means that in our our document we got 25 word one in now from our dictionary so in the position one two * word two and zero * word three so basically we have a predetermined set of words in our dictionary in this case three words word one word two and word three and they have a specific IND specific position in our vector and when we are putting these values in here then the machine or the uh computer the program will understand that if we have 25 in the first position then the word one in the dictionary appeared 25 times in our document whereas the second word appeared only two times and the last word for three didn't appear at all so zero times in the entire document so let's look into a practical example actually to make even more sense so um this is by the way a common practice to count different variations of a word there are common application in engrams large language models Transformers they are just the Cornerstone of many language models when we want to count the words in the document to understand how often the word appears because this gives us a idea what this document is about knowing how many times the same word appears in that uh document it gives us an indication of the topic of uh the do document also we can make use of a to do sentiment analysis to understand what this document is about not only in terms of the topic but also is it a positive is it a natural or a negative uh document so to say so uh for instance if we got uh the following words uh that correspond to our dictionary and in our dictionary we got just um let's say 6 different words then what we can do is that we can say 3 2 1 let's say Zer 4 two and the corresponding words are word row [Music] number horse is and then document what this means is that we have a text what we refer as a document that contains three times the word word that contains two time the word row contains one time the word number zero times the word horse and four times the word eel and two times the word document so uh this is basically a common way represent presenting the uh frequency of the words in the document let me actually give you uh another example and in here I want to emphasize another thing the concept of stop words so uh let's say I make this 10 and then here I say there is a three times the word I two times the word uh reading two * the word library four times the word book 0er * the word shower and 10 times the word uh so uh you can see a that in here we are dealing with the document that contains 10 times the word uh which is what something that we refer as a stop word so those are things that actually don't give us too much information about what the document is about because uh it's just used everywhere but it is appearing too often so you can see 10 times the most frequently appearing word this is what we refer as a stop word and then another thing that we can observe the second thing we can observe is that we are dealing most like ly with a document that describes library reading uh because you see the words like reading you see the word like book library but another word shower that is totally unrelated to reading book or library is appearing zero times so even by looking at discounts we can already get an idea what a topic of this document is about so uh you can see already know from this very basic example where I made too many assumptions regarding how small the the uh dictionary should be uh you can even see now how we can use discounts in our dictionary from our text in order to get idea about the topic of the document or topic of the conversation it can be topic of the uh tweets if you have a tweet data it can be topic uh regarding book if you have many book um uh book text it can be for instance the topic of the review if you got a reviews from uh Amazon for instance using this count can help you to get a topic regarding topic from that text then you can also use it to remove the stop wordss because usually the stop words are the most frequently P words it can also give you an idea about the sentiment for instance here we are dealing with natural sentiment it's not positive it's not negative it's just reading a book in library that kind of topic so all this can be super helpful when it comes to natural language processing that's a field where this uh text processing text cing and then using that for modeling purposes is what uh what plays a central role it also plays a super important role in the large language models in the Transformer models and uh in simple matters like uh back of words or uh in the uh TF IDF all these they are based on this idea of counting words and how we can use it information and you can see how vectors come into play in the different applications of linear algebra in data science natural language processing in artificial intelligence in machine learning so they are super important another application of vectors can be representing customer purchases for example an N Vector P so let's say p can record a customer purchases over time with pi being the quantity or dollar value of an item I now what does this mean so let's say we have Vector P that represents the customer purchases and we are dealing with a single customer and we are just saving over time that information how many time this customer has made purchases over time so the quantity is in the um dollars so the dollar value of item I purchase so we are basically keeping track of uh what is the value of the item I that the customer has purchased so what we can do is we can assume that in here actually it already Mak that assumption it says n Vector which means that the number of rows or number of um items that the customer purchases is n now what the um the problem says that it represents is that in each entry and here we have in total n entries we got a dollar value of item I which means that here if I have P1 P2 all the way to PN and here somewhere in the middle I got Pi in the East position it means p Pi represents the value of item I so for example if I'm dealing with a customer that buys um let's say uh courses and uh the first item that the customer is buying is a mathematics course so I'm writing mathematics course and this is the first course that it buys e is by the way just a um way to refer to the E purchase so let's say um here somewhere in the middle the um customer decides to buy a deep learning course deep learning learning course and then it continues buying uh the customer continues buying courses and the last course that a customer buys is let's say um career coaching course now let's say the mathematics course costs uh around $1,000 let's say the uh deep learning course costs $33,000 and then let's say the career coaching service which is usually one of the most apply and personalized one can cost all the way to $5,000 now we see that in the each position this is the East position let me change the color by the way so let's say this is the East position this is the first position and this is the last position so those are just indices we can see that in the E position we got the 3,000 which means that the p e is equal equal to $3,000 so this indicates that in the East purchase the customer purchased deep learning course and the value of that item was equal to $3,000 all right so now we are ready to go on to next major topic which is about vector addition and subtraction so we are going to do some operations and apply this operations two vectors so let's first formally Define this ideal of vector addition so uh two vectors of the same size are added by adding their corresponding elements the result is a vector of the same size so uh let's unpack this it says two vectors of the same size are added by their corresponding elements so here it refers to two different vectors let's say vector v and Vector W and it says let's add them what we refer as vector addition and says for that what we need to do is to take all the elements of v and then all the elements of w and using their corresponding elements so indices that helps us to understand where those elements are located we are using in order to add each element in the vector v to the element of the vector W in the same position and do note that in the second part it says the result is a vector of the same size because we are adding two different vectors of the same size it's mentioning here it means if we add two different vectors to the same uh that have the same size we are going to end up with a vector that has the same size now once I go into the examples it will make much more per let's quickly also look into this concept of substraction so on its own uh substraction is very similar to this idea of addition so if we have a substraction let's say we have vector v We substract Vector W then we are doing basically uh what we just did to the addition only instead of uh doing add we are doing subtract so again we are just uh we are just subtracting from vector v Vector W they have the same size so we end up having the result which is a vector of the same size only one thing that you can see is that this can be also written as V Vector plus and then minus W so we basically can represent subtraction um on its own as a way of adding only we take the negative so the um opposite directed Vector so this will make even much more uh once we go on to the examples so let's look into our first operation example where we are adding two different vectors this a basic example we got just two dimensional two vectors we got Vector a that has entries two three and Vector B that has entries one form and what we are doing is that we are adding Vector a to Vector B we just learned that a we need to have the same size of vectors so you can see that Vector a has a dimension 2 by one vector B has a dimension of 2 by one so their sizes is the same both they got two entries only two elements and at the same time we just learned that what we need to do is to take their corresponding elements and add them to each other now what does this mean it means that we take from a the first element two and then we take the first element of the second Vector which is the B so we take the two from here and one from here the first element of a and the first element of B and then we are adding them to each other 2 + 1 is equal to three and then the same holds for the second and Tre so three which is the second element of vector a and then four which is the second element of vector B we are saying 3 + 4 is = to 7 so let me write it down even in a simpler manner such that it will make much more sense so Vector a has elements 2 three in the first element we got two in the second element we got three so a then we want to add B which has in the first element element equal to 1 and the second element is equal to 4 this means that if we want to add these vectors 2 3+ 1 4 this is equal to we need to take two we need to add one so this element and this element and then we need to take three we need to add to four so this one and this one which is equal to 2 + 1 is equal to 3 3 + 4 is equal to 7 so we got uh Vector 37 do you note that this Vector the result Vector contains again two elements and just one column so 2 by 1 so you notice that the sign that the size is the same of this result Vector now let's actually generalize this concept before moving on to the next example so if we got let's say Vector a that contains n elements A1 A2 all the way down to a n and it is from n dimensional space and we got Vector B that also has n elements so remember that they both need to have the same size so B1 B2 all the way to b n so they come also so B comes also from n dimensional space so then when we add a to B this is equal to A1 A2 all the way to a n plus B1 B2 all the way to BN so n by one n by one the sizes this is equal to let me actually use this color to make it even more visible so I for the first entry for my result factor I will get A1 + B1 then A2 + B2 so all the way down onto the end element which is a n plus then me use a different color A1 B1 B2 b n so you can notice is now in general terms what we are doing here so we are taking the A1 coming from the vector a we are adding in the same uh position the value that comes from Vector B which is B1 we are saying take the A1 Plus B1 this is the uh first element so the position stays the same and then in the result Vector so we take all the corresponding values that are have the same position in the corresponding Vector first from Vector a and then Vector B we are adding them and this forms our new vector and this new Vector will again have a size n by one so you can see that a the sizes of the two vectors are the same both have n elements and then we are using their corresponding elements to add them to each other element wise and then we are getting the result that has the same size so n by 1 so this is a more General description of how you can add two vectors let's now look into this specific example so we have a vector with the entry 073 so this comes from R three you can see so three dimensional vectors the second Vector is 1 2 0 and then the final result is 1 193 so how we got this we took zero we added 1 7 we added two and then three we added zero so you can see all these elements element Y and then this is equal to 0 + 1 is 1 7 + 2 is 9 and then 3 + 0 is 3 exactly what we got here so again the same sizes and the result is from the same size so quite straightforward now when it comes to the vector substruction what are we doing that um so what are we doing here so we are doing kind of very similar thing we are taking this element one we are subtracting the other one in this first element then we are taking the nine in the second position and subtracting this again from the second position of the second vector and we are putting in here 1 and then 1 - 1 is = to 0 9 y - 1 is = to 8 so we get result Factor 08 like in here and you can see that the sizes stay the same so also in this case let's write more General um this idea of subtraction if we got a vector a from RN so n dimensional space and it can be represented by A1 A2 all the way down to a n so it has n elements n by one and then we got B also from RN so coming from the n dimensional space which means that it got n elements so B1 B2 all the way down to BN again with the same size n by one then a minus B is simply equal to a A1 let me actually use the same colors to make it easier to follow so let me first draw my Square races and then here I will use blue for a and then red for the uh color for second Vector which is B here I will use black minus then given that the same size should be for the result Vector I I already know that I expect n different elements for this and then here I'm taking this first element that comes from Vector a I substracting from this the first element that comes from Vector B so element wise subtraction B1 and I'm already getting the result for the first element in my result Vector so you can see A1 minus B1 I'm taking this element and this element and subtracting them from each other to get A1 minus B1 and then the same holds for all the other values only coming from different elements from Vector a subtracting from this the corresponding values element Wise from the vector B so B2 B3 all the way to a n so you can see that in my result Vector a vector minus B Vector in the first element I get A1 minus B1 then A2 - B2 then A3 - B3 in the third element all the way down to the end element which is equal to oh this should be b a n minus BN so um this already should makes uh much more sense so every time we take the element in the same position from one vector than the other we subtract from each other in order to get the corresponding element in the final Vector all right so let's now uh before moving on to the properties um I want to to show you um this only in a coordinate space so what this means in terms of visualization in a coordinate space so uh let's say we have a coordinate space this is my Y axis this is my x axis so this is X and the Y and this is my Center so 0 0 and what I'm doing here is simply I want to have Vector a let's say this is just um Vector a simple one with the coordinates um let's say four and minus 2 and I got Vector B let me use a different color Vector B that has coordinates let's say minus 4 and four so let's actually visualize them let's first start with D Vector a uh which has a x value of four three four one 2 three and four and the Y value minus 2 so this is my Vector a and let's now visualize the vector B so minus 4 and 4 which means that let me actually extend this this is minus 4 so the x coordinate is min - 4 so it should be here and then the y coordinate is four so 1 2 3 and 4 it's this one which means that my Vector B is this one all right so you can see now that the vector a is in here and the vector B is in here now what I want to do is to add these two vectors to each other so what I want to do is to take this Vector a and add to this the vector B which is is equal to 4 - 4 = 0 and then - 2 + 4 is = 2 so zero and then two it is z and two two so this is my result Vector so now when we are clear on how we can in vector s how we can perform these different operations and what it means in practice when it comes to looking at the vectors in a cordan space and adding them or subtracting them we are ready to look into the properties of vector additions so this is something that will definitely seem familiar to you uh from pre-algebra where we are basically using all these properties that we already know that holds for uh numeric values for the scalers that being transferred to this Vector space so we are going to talk about this four different properties that a vectors have the first one is the cumulative property which says that if we add a vector a to Vector B then this is the same as adding a vector B to Vector a so basically the order of the vectors doesn't really matter when it comes down to adding them so formally A + B is equal to B+ a for any vectors A and B of the same size then we have associative property which says A + B + C is equal to a + b + C we can write both As A + B + C now what does this mean we know from pre-algebra that this parenthesis means first do this addition and then do the the rest of operations in here it basically says if you add a to the B first and then you add the C is the same as first you add B to the C and then on the top of that you add D Vector a so then the third property is addition of zero vectors which says if we add a zero Vector to Vector a then this is equal to adding a vector zero to a and this is equal to Vector a so adding the zero Vector has basically no impact on the vector whatsoever then the final property is subtracting a vector from itself which means if we take the vector we substract the same Vector from itself so a minus a and we get a zero Vector so a minus a is equal to zero vector and this heals the zero Vector now let's look into each of those properties one by one and let's uh look into specific examples uh in some cases we will prove this on the example that we have to make this Concepts much more clear so let's start with this cumulative property of vector additions so we want to see whether A+ B is equal to B + a so let's say we have a vector a that has coordinates or magnitude and direction that is equal to one and two then we have a vector uh let's say B that has a magnitude and direction of - 2 and 3 so the first thing that we want to check is indeed whether the A + B is equal to B + a so therefore let's first calculate this part and then we will calculate this part that I will Define by one and two and we will see whether we are indeed having the same value the same vector or not so let's see so we have here a so A + B which is the first value that we want to calculate a plus b is = to 1 2+ - 23 and we learned before that this is simply equal 2 take this value and then add this one so 1 + - 2 and then 2 + 3 so this gives us a vector 1 - 2 is = to - 1 and 2 + 3 is = 5 so we get that A + B is = to -1 5 this Vector now let's look at the second quantity so B Vector B plus Vector a this is equal to - 2 3 + 1 2 and this is equal to - 2 + 1 and then 3 + 2 this gives us - 2 + 1 is = to- 1 and 3 + 2 is equal to 5 so we can already see from here that the quantity one is indeed equal to quantity 2 which proves that indeed the A + B is equal to B+ a what this basically means is that adding two different vectors the direction or the order is not important whether you add a on the top of the b or B to a it doesn't matter at the end is the same and actually you can also see it if you uh combine this or if you do this in more general terms so let's say if we have a vector a which is equal to in an N dimensional space A1 A2 up to a n so it has n by one dimension and you have a vector B with the same size from the same RN Dimension and it has element B1 B2 up to BN and the dimension is equal to M by1 then if we calculate first A+ B and this is equal to Simply A1 + B1 A2 + B2 up to a n + BN and if you calculate the second uh amount which is B+ a this is equal to B1 + A1 B2 + A2 up to BN + a n you can see that A1 + B1 is equal to B1 + A1 simply from prealgebra you know that if those are all constants for instance 2 + 3 is equal 3 + 2 in the same way A2 + B2 is = to B2 + A2 and then here up to a n + BN is equal to BN + a n what this means is that all these elements they are basically the same which means that we already have a proof so we get this proof and we can see that even for the general term independent what this Vector a is what this Vector B is that a + b is equal to B + a this is exactly what we saw before in the first property which is called commutative property of the vectors that a plus b is equal to B+ a now let's move on to the other property which is called associative property of the vectors now what this property does and says is that a plus b so first we do this plus C is equal to a + b + C and this is then equal to a + b + C now let's then see um this specific property on an actual example so what this basically says is that if we have this example where a is equal to actually I had this before let me simply just remove this part let's then add our third Vector which is C and let's call it let's say it has a representation of four and five then the IDE behind this property is that what we need to prove here that A + B within the parenthesis plus C is equal to a plus B+ C and then this is equal to a plus b plus C so let's see actually whether this is indeed true for this specific case now this should come very intuitive so I'm going to do it very quickly so first we have this quantity this one then we have this one and the third one let's do it very quickly so A + B plus C is equal to one Tu plus and then we had C so it is simply 4 five and then this is equal to we saw before when doing this that we were getting 1 - 2 2 + 3 and then we add this four five this is simply equal to 1 - 2 is -1 and 2 + 3 is 5 + 4 5 now given that it doesn't really matter no longer that we have uh here parenthesis or not this basically means that this volue is simply equal to -1 + 4 so here -1 + 4 here 5 + 5 so this is then equal to three and then 10 all right let's then now quickly do the second amount which says first add the vector B to Vector C and only then add the vector a on the top what this means is that we need to take one two this is Vector a and we will only add this once we have added D minus 23 the vector B plus to the Vector 45 okay so we can see that we are just leaving this in here let's first add this two minus 2 + 4 3 + 5 so this gives us 1 2+ - 2 + 4 is uh 2 and then 3 + 5 is 8 so this gives us let me remove this calculations so this gives us 1 + 2 is = to 3 and then 2 + 8 is equal to 10 okay great so now we got already the quantity 1 being equal to quantity 2 let's check whether this is all equal to this one it should already be um something that you see now given that um we know just from mathematics that parentheses doesn't really matter when it comes to the scalers and adding two vectors is basically very close to this idea of addited property um of the edited property of the scalers but just let's quickly do it to be 100% sure so when we take this Vector a to the B and to the C we had all this this is equal to one want to added to minus 2 three and then added this to four and five now what this is equal to let me actually write this in bit shorter way such that it can be all fit in in the small place so 1 2 + - 2 3 + 4 5 this is equal to basically 1 - 2 + + 4 and then 2 + 3 + 5 now what is this number 1 - 2 + 4 is simply equal to 1 - 2 is = to -1 and then + 4 is equal to 3 so first element is three 2 + 3 + 5 is equal to 5 + 5 which is equal to 10 perfect so now we get the confirmation that indeed a + b plus c C is = to A + B + C is = to A + B + C so let's quickly also look into this addition of zero vector and the subtracting a vector from itself properties and uh the detailed explanation of this or example of this I will leave it to you so when it comes to this A+ um 0 is equal to 0 + a is equal to a so this property let's say if a is equal to this 23 and then we are adding on this a plus some zero Vector which basically means take two three and then added the same size of zero Vector you can see that this is the same as adding this zeros on these values now what do we get we get that this is equal to 2 + 0 is 2 and then 3 + 0 is three there we go so we already see very quickly that it doesn't really matter whether we add a zero Vector to this original a vector or not we in all cases it just adding a zero Vector has no effect and seeing from the commutative property that a plus b is equal to B+ a we already know that if um a + 0 is equal to uh a and is equal to this then also 0 + a will be the same and we can see indeed that we just saw that a a + 0 is simply equal to a so we basically have quickly proven all this now when it comes to the subtracting Vector from itself I think this is a very nice one just to see how we um uh take the same vector and subtract from that value and we get zero and this is very similar to working with just real numbers in the same way as 3 minus 3 is equal to Z also when we have a vector consisting of the scalers like a is equal 2 23 in the same manner if we take this a and we subtract it from itself so A Min - A then what we will get is 23 - 23 and this will give us 2 - 2 is 0 and then 3 - 3 is zero so we get a vector zero so zero Vector so now when we are clear on how we can perform different operations on our vectors and also we know uh what are the prop properties of uh adding and subtracting uh different vectors we are ready to move on to a bit more advanced topics so uh in this module we are going to discuss this idea of scalar multiplication we're going to look into the example how uh what happens and how we can do the uh Vector multiplication with the scalar then we are going to uh look into the span of vectors what it means to have a sp of vectors uh what is this IDE of linear combination and the relationship between the span and linear combination and the unit vectors then we are going to look into the application of scalar Vector multiplication in audio scaling uh example and then finally we are going to finish off this module by looking into the length of a vector and a DOT product and we are going to uh go back to this idea of distance understanding vector magnitude and understanding Vector l so let's get started now before we look into this idea of span and linear combination I quickly wanted to look into this idea of scalar multiplication and the um specific definition of it so formally the scalar multiplication involves multiplying each component of a vector by scalar value effectively scaling the vector's magnitude so what do I mean here let's say we have a vector and I will write it in the general terms to keep everything General so let's see we have a vector a let me pick up my pen a and this Vector a is from n dimensional space so it is from RN and it can be represented by A1 A2 up to a n and I have this magnitude um of a vector and now I want to scale this uh Vector for which I know the magnitude and the direction I want to scale it with a scaler and we learned before that the scaler is just a number so um scaler in this case I will be uh referring it to uh by C so c will be my scaler and uh this comes from R which means that it's a real number let me actually use a different color to make it easier to follow okay so my scaler will be with the color uh red so C and C comes from R so what do I mean by scalar multiplication I mean that I want to find what is this c times a this is what we mean by scalar multiplying with Vector now what does this definition say it says when we are multiplying scalar we Vector so the scalar multiplication meaning multiplying Vector with the scalar it involves multiplying each component of a vector by a scalar volume so if we translate it to this specific example it means that this amount so this amount is equal to taking C and multiply find it with each element of this Vector so each component of vector and what are the components of my Vector the A1 A2 a up to the point of a n so all these components so that means that the first element of this new Vector the scalar multiplication result will be C * A1 C * A2 dot dot dot so all this middle elements and at the end again c times and then a n and then in both cases of course the number of elements doesn't change so the so the number of rows of my Vector doesn't change it's n so here also n and then number of columns is the same so it's just a column Vector so one column so what we see here is that we go from a 1 to C * A1 we go from A2 to C C * A2 up to the a n transforms into C * a n so we see very easily that I keep all the elements from this Vector I take them in here and instead what I'm doing is that I'm multiplying every element from this vector by the scaler C so this is exactly what this definition says and let's actually go ahead and do a Hands-On example with some real numbers to have this um method and to have this uh definition very clear in our mind because we are going to make use of this fundamental operation scalar multiplication on and on in the upcoming lectures and just in general in your journey in any applied sciences so this is an example of scalar multiplication uh here what we are doing is that we want to multiply this Vector C so in this case the vector is defined by a letter C and then on the top we can see the arrow indicating that this is the vector now and here we refer the scalar by a letter K we are saying we want to perform scalar multiplication which means that we want to multiply the uh a vector C by the scaler K so how we can do that so what we want is to multiply K by C and we just learned that for that what we need to do let me write this over so this equal to minus 2 multiply it by 4 - 3 this is my Vector so this is the K and this is the C this is equal to so I take my scaler and I multiply it with the each of the ele element of the C so - 2 * 4 and then - 2 * -3 so - 2 * 4 is = to - 8 and then - 2 * - 3 so- minus it goes away it becomes a plus and 2 * 3 is 6 so my end result the K * C is equal to - 8 6 this is my final result so let's quickly also do yet another example and this one is a unique one because it's relating to this idea of U multiplying something with a zero uh which is something that we also uh know from a high school that when we multiply number let's say seven by zero we are getting zero and here in this example the uh problem is describe the effect of a scalar multiplication by zero on any Vector which means what we are doing is that in this example is we want to know what is this result of any Vector let's say Vector uh C so we will use the same example C only this time instead of multiplying it with scalar k equal to minus 2 our scalar will be zero which means that c is equal to 4 - 3 and then K is now equal to zero and we want to find out what is this K * C let me actually write down the K with a different color k is equal to zero so what we want to find out is K * and then C and this is that equal to0 so I'm taking the k0 times then I'm taking each of the elements of C which is four and then minus 3 and I know that when multiplying the number with is 0 it gives me 0o which means that I end up with 0 here 0 * 4 is 0 0 * - 3 is also 0 so I end up with a zero Vector now this gives me an idea already that I can make a general conclusion that independent of the type of vector that I have independ and what are this values in my C uh if I have any Vector C and I'm multiplying it with zero then this will always give me a vector of zero because all the members of this final Vector will be just zeros so if for instance the C comes from uh let's say r n so it has n different elements it comes from n dimensional space then my final result of 0 * C so this zero Vector this one so zero that this one will come also from RN so you will be having a vector so 0 * c will then be equal to z0 blah blah blah blah zero so n time zeros so this is then the idea of multiplying so scaling a vector with zero and this is our example two all right so let's now move on on to our application of scalar vectal multiplication and then after this we will go back to this idea of linear combinations and dispense so in this specific application we have a scalar Vector multiplication and we are looking into application of audio scaling so the scalar Vector multiplication audio processing uh this can change the volume for instance of an audio signal without altering its content so um you might have noticed that um when uh when you are listening to video you can simply increase the volume of that video or decrease it but you will notice that the content doesn't change you are just increasing the volume or decreasing it even on the TV when you are watching a show you are increasing The Voice or decreasing now what you're basically doing behind and this is super interesting is that behind the scenes what is happening is that there is simply um audio that um contains that show and the audio of that show is being multiplied with a scaler and that scale is simply the volume scale if you scale it in such way that you want to decrease the volume so the audio will then have a lower volume then you are simply multiplying it uh your vector containing the audio information in such way that those newer volume indications they will be they will be containing lower numbers hope this makes sense let's look into the example this make uh this will definitely clear this out so um let's assume we have an a vector a that represents the audio signal and we want to multiply Vector a a by scalar B to adjust the volume so B is some sort of number it can be so B comes from R so is a real number while a is simply a vector given that it doesn't mentioning here I'm assuming that a comes from RN so it comes from r n dimensional space so imagine of a as this Vector A1 A2 blah blah blah blah to a n and each of these values it basically describes uh an uh the audio signal so it represents um uh an amount so it contains an amount that represents the audio signal of your uh video or uh your uh show and then the b in this case for instance in this example you can see that the B is then uh equal to for instance 1.2 1 / 2 or B is equal to Min - 1 / 2 so you can see that b is equal to 1 / 2 which basically is a fensive of saying that b is equal to 0.5 or B can be equal to minus1 / 2 which is minus 0.5 now then it says then the B * a which basically means multiplying our um scalar beta by the Vector containing the audio signal a so this B * a is perceived as the same audio signal but at the lower volume now why lower because you can see that b is equal to 0.5 or minus 0.5 it means that once you take all these elements of your a and you multiply it with a number that is smaller than one in this case 0.5 then all these numbers will decrease which means that also your audio volume will decrease so let me actually uh show you an example so let's say our talk show is very short and you know the audio variation is very low you have a vector a that is quite small it comes from a three dimensional space so R Tre and it has numbers like three uh six and then five so 3x1 vector and then we have our audio adjustment scalar beta which is equal to 0.5 now when we take the beta we're multiply it by our audio signal then what we do times is clear so times what we are doing is that we are simply taking all the elements of our a so Three 6 and five and what we are doing is that we are multiplying it by 0.5 0.5 and 0.5 or you can also say 1 / 2 so what this is equal is that 3 * 0.5 is 1.5 6 * 0.5 is 3 and then 5 * 0.5 is 2.5 and you can see that all this numbers 1.5 3 and 2.5 they are smaller and specifically two times times less than all the original values in the um original audio so original audio is a which was 3 6 and 5 and the new audio the the scaled one is so audio scaled so B * a is equal to 1.5 3 and 2.5 so you can clearly see this transformation where this element three is larger than 1.5 6 is larger than three and then the last element five is larger than 2.5 which means that this audio audio is much at a higher volume so the volume two times higher than this audio so this is basically the idea of uh applying scalar multiplication to our audio pre-processing I will leave the other example to you that will show that when your scaler is equal to minus 0.5 you again will end up with the lower volume only that time the volume will be much much lower than the original one so now that we know how we can perform scale multiplication in theory as well as we have looked into an example how we can do it in terms of the numbers and multiply apping them and we have also seen uh applying SK multiplication in practice uh so we have seen in this audio processing stage the uh multiplication process we are ready to look into the visualization of it this will help us to get a better understanding on uh what exactly happens when we are scaling different vectors let's look actually in the following example so let's assume we have a vector oh let me remove that so let's usum we have a vector and the vector is let me get a color this one for instance a vector a and this Vector a consists of elements one and two so where does this Vector Li the vector is with um one so here in our coord system this is our xaxis this is our y AIS and here we got uh let me actually pick another color let's say black one and then we got one and then two right this is two this is one so it is this one so the line that we get here it is this one so this is our Vector a now let's assume I want to multiply my Vector a so I want to scale my Vector a by a constant Tree by scalar tree so I have a scaler let's say I call K and this k a different number let's say k is equal to three so what I wanted to do is to perform a scale of multiplication so I want to obtain K multiplied by a and we learned that this is simply equal to three times and then one two and then this is equal to 3 * 1 3 * 2 which is equal to 3 and then six so let's also visualize this scaled uh Vector so let me pick this yellow color this will be our scaled Vector so we have done scale multiplication and we are going to visualize that so we have three and six so this is three 1 2 three and this is six so we have this point so you should already see what is going on here so we got 3A here so you can see that this part is our Vector a and this longer one is 3 a and even visually you can see that this longer Vector is simply the three times of the shorter Vector so we got this and then if you add on the top of this the same three times you will then end up with the original so scaled version of that so basically this is a this is a this is a we combine three different so we scale a three times and we simply get a three times longer version with the same direction so you can see that when we are scaling even visually it makes sense so we are scaling our Vector a three times and we are just getting that Vector so we are transforming oh let me remove this so basically we are taking this vector and we are scaling it up to this point if I would do it only two times then it would be something like this or one and a half times it would be something like this so only half of it so now this should make much more sense let us actually do yet another example to uh make sure that we are clear on this visualizations because we are going to make use of it when uh looking into this idea of linear combination in a span so let's say we have a vector B and this Vector B has elements zero and three so let's visualize and uh plot this Vector so it contains elements Z zero and three so zero and three so this is the X element and the Y element on the Y AIS we can see this is three which means that our Vector B is this Vector all right perfect so this is our B let's now multiply so scale our Vector B by scaler t Q so let's say we want to get 2 * B so what is this amount this is equal to 2 * 2 times and I'm simply taking each of those elements zero and then three so this is then equal to 2 * 0 is 0 and then 2 * 3 is equal to 6 so this is my new scaled Vector 2 * time B Vector this one so let's visualize this the xais value is zero so we are still here and then the y- axis value is six so what is sixth this thing all right so you already should see that this is very similar what we had before so this is 2 B all right so this all uh should make sense uh also we learned as part of the um High School when visualizing different plots so this is quite similar to this idea of having Y is equal to X and then scaling it getting like Y is equal to 2x so in this case only we know exactly where the vector starts and ends uh so we have a much more specific definition instead of having all this infinite number of points on the line but the idea stays the same so we are taking this vector and we are then scaling it two times so we get 2 B vector and I could do the same only instead what I could also do is I could do like uh 0.5 or 1 / 2 * B so I take the half of it which means I would get this vector or I could multiply it with minus one so minus - 1 * B so I was scale with minus1 and and then I will simply get the negative version of my original Vector so this thing this would be minus b or min-1 * B so this is basically the idea of uh scaling multiplication when visualizing it in our coordinate system cartisian coordinate system and now when we know all this we are ready to move on on this idea of linear combin and now when we know all this we are ready to move on on this idea of linear combination so let's now formally Define this ideal linear combinations a linear combination of vectors A1 up to a m using scalers B1 up to BM or what we also refer as beta 1 to Beta m is the vector beta 1 * A1 Plus up to Beta M * a m and the scalers are called the coefficient of linear combination and any Vector B in N Dimensions can be expressed as a linear combination of the standard unit vectors E1 up to n the coefficients in this combination are then the entries of B itself well this is whole bunch of information uh let's unpack them one by one firstly um I want to mention about this m so far we have seen this idea of n so I just wanted P to experiment with a different one just to ensure that we are clear that you can use any source of identifier to describe the size of your um uh number of vectors that you got and uh in this case we got M different vectors because so far we were using this n in order to describe the size of a vector and now we are no longer talking about the size of a vector but the number of vectors therefore I specifically didn't use use the letter N so here m is simply the number of vectors so don't confuse this with this thing where we were plotting this and we were saying this A1 A2 up to a n because in here we basically mean that we are dealing with some Vector a and this has n different elements whereas in here we are already moving from this idea of one vector and now we are talking about mve multiple vectors so we have M different vectors they all look like kind of this only with bit more complex indexing that we also saw before all right but we will learn this um that's not an issue I just wanted to mention this to ensure we are at the same page so then let's move on to this idea of using scalers beta 1 till beta M so it's a common uh practice in linear algebra in just in general in mathematics but also definitely in data science statistics and in artificial intelligence to use beta 1 as a way to describe the coefficient so what do you mean by coefficient it is just a scalar so it's just a constant or a number so in this case for instance this beta 1 can be 0.5 beta 1 can be uh two bet one can be let's say 100 it just describes how much we are multiplying scaling this Vector A1 so so far we have done a lot of scal and multiplication already lot of details there and we have seen different times different scalers that we use we can use um zero as a scaler we can use any other number as long as it's a real number so this beta 1 should belong uh in the a real number space so it's a real number and of course the same holds for uh all the betas so we have M different vectors which means we are going to have M different scalers because each of those vectors we are going to multiply with their corresponding or respective scalers so beta one is basically the scaler uh or the um uh coefficient that we are using to scale a one maybe I can actually write this down on a new page such that we can save this as a SL Light page for you let's write it down so what do we have as this idea of linear combination so a linear combination simply involves taking several vectors uh to go from this uh formal definition to more practical uh terms so we got this A1 A2 up to a and what we want to do is to take the linear combination of this m different vectors so we got m is the number of vectors and to get a linear combination we need to uh scale each of those vectors which means that we need to have this different scalers let's say beta 1 for A1 and then plus beta 2 for A2 so each time we are scaling each of those vectors where beta 1 is the uh scaler or the coefficient of the vector A1 and we are multiplying we are performing scalar multiplication of our scalar beta 1 with the vector A1 and then we are adding to this our beta 2 which is the coefficient corresponding to the vector A2 and then adding beta 3 * A3 and then dot dot dot so all these different uh vectors up to the point of beta M time a m and all this so A1 A2 up to a those are all vectors belonging to the m space so those are all vectors coming from the um M dimensional space so um in here this is the linear combination of our M different vectors and the uh beta 1 beta 2 up to Beta M those are all constants so those are scalers or real numbers that belong to R so those are real numbers all right so now when we are clear on that let's also unpack this idea of coefficients so the scalers are called the coefficients of linear combination so basically all this members so beta 1 beta 2 of two beta M that belong to real number space they are called coefficients this is what we are referring as coefficients and this coefficients this IDE and name is super important because you will see this time and time again appearing in your uh very basic machine learning models or some other applications of linear algebra because the end goal is always to find these coefficients so these coefficients those are numbers that we are using to scale these different vectors and uh the idea of coefficients is very Central because those are numbers that Define how exactly we are combin ining all these different vectors because this beta 1 beta 2 Beta 3 they can be different numbers real numbers and every time when we are choosing these coefficients or these betas we will then end up with a different combination of these vectors so we are basically mixing all these different vectors and the way we mix it and how we will mix it it will depend on the values of this beta 1 beta 2 Beta 3 up to Beta m so these coefficients so therefore coefficients are super important and they Define the end results from our linear combination so any Vector B in N Dimensions can be expressed as a linear combination of standard unit vectors E1 up to n so when looking into this um idea of unit vectors uh we saw already what this E1 is what is E2 is up to e n and we saw that E1 is for instance if it's from an N dimensional space and it says from n dimensions then E1 simply means 1 0 0 dot dot dot dot Z then E2 means 0 1 Z dot dot dot dot zero so we already saw this this is not something new that we are seeing so 0 Z blah blah blah and then one at the end and what this definition basically says is that any Vector b as long as the B comes from n dimensional space we can represent this by using this uh unit vectors and by linearly combining them so this is yet another part of this definition and we are going to by the way um go through each of the parts of this definition one by one going to each of the examp examples as well as visualizing them so now I just want to quickly unpack all the parts in this definition before moving on to step by step examples and explanation so this is about this linear combination of any n dimensional uh Vector B that we can uh create by using a linear combination of these unit vectors I will come to this in a bit so then the final part of this definition is that the coefficient in this combination are the entries of B itself so it says that the coefficients so beta 1 up to Beta m in this linear combination that we can create are the entries of B itself so we will come to this section once we are done with the first part so first let's have a good understanding of what this linear combination is and also touch base and we will also formally Define the idea of span and after that we will move on on uh representing and expressing any Vector B in N Dimension Space by using standard unit vectors E1 up to e n and this idea of coefficients and then entries of B so let's start with the first one so let's assume we have two different vectors we have Vector a and this Vector a is equal to one 2 so let's plot this one and two in our coordinate space that is this one which means that our Vector a is this one and let's assume that we have a vector B and this Vector B is equal to Z 03 so 0 is here and then three is here which means that our Vector B is this one this is Vector B now I want to create a linear combination of this Vector a and Vector B so we just learned from the formal definition that in order to do so I need a beta 1 to multiply the vector a and then I need beta 2 which is the coefficient corresponding to to my second Vector in order to multiply the second Vector which is B Vector B okay so I'm getting the linear combination of A and B by taking any beta 1 and beta 2 which are real numbers so beta 1 and beta 2 belong to R so they are real numbers and then I'm getting a linear combination of the two so let's look into a few examples of a linear combination of vector A and B depending on the different choice of the coefficients like beta 1 and beta 2 so example one is that beta 1 is equal to zero and then beta beta 2 is equal to Z Now what is the linear combination of A and B when my coefficients beta 1 and beta 2 both are zero it just means that I'm getting 0 time A Plus 0 times B which is of course 0 * 1 0 * 2 plus and then multiplying Vector B with a scaler zero which is 0 * 0 0 * 3 so let's quickly do this what this value is this is equal to 0 * 1 is 0 0 * 2 is 0 0 * 0 is equal to 0 0 * 3 is equal to 0 and this is then equal to 0 + 0 0 0 + 0 is0 so I'm basically getting a vector zero all right so this is then equal to zero so this equal to vector is zero so I can also say that this vector or it's actually a point so this point is simply a linear combination of these two vectors now this is a super basic case let's look at another case when our in our second example the beta 1 and beta 2 so our coefficients they are actually not zero there are some other nonzero real numbers so in this example I will then take beta 1 = to 3 and then beta 2 is = to 2 and then what I will do is that I will take actually I will take the um minus two then I can also get rid of one of the elements and I can get actually a zero for one of the elements I will show you in a bit so then the linear combination of A and B using these coefficients beta 1 and beta 2 where beta 1 isal to 3 and beta 2 is equal to minus 2 is then equal to so this amount this amount is equal = 2 3 * 1 2 and then plus we got - 2 * 0 and three now what does this give us 3 * 1 is = 3 3 * 2 is = 6 plus and then - 2 * 0 is = to 0 and then - 2 * 3 is = - 6 so you might have already noticed why I picked the beta 2 equal to minus 2 I wanted these two numbers to actually cancel each other so you see because 6 + - 6 is equal to Z so what do I get in my final result as a linear combination of these two vectors I get 3 + 0 so 3 + 0 so 3 + 0 and then 6 + - 6 and this gives me 3 + 0 is 3 6 + - 6 is zero there we go so this is my linear combination of the vector A and B when using the coefficients equal to 3 and minus 2 respectively so this value is actually = to three and zero in this case all right so let me actually clean this up because I also want to visualize this idea and then we will go uh back to this uh linear combination let just summarize uh what we got before moving on to the plotting part so if we simply take a and we add to this B so this is the first case so this is as you might have already guessed this is also l your combination here we are saying take 1 * a and take a 1 * B and this is yet in our linear combination here the beta 1 is equal to 1 and then beta 2 is equal to 1 so this linear combination gives us a vector that is 1 + 0 is = to 1 and then 2 + 3 is = 5 this is our first linear combination when the beta 1 and beta 2 is equal to one this is a basic case so doesn't require too much explanation here we have seen already this let's now look into the other example that we saw when we use uh the zeros as our coefficient so that is 0 * A+ 0 * B then this gave us 0 0 this was our second linear combination when beta 1 and beta 2 were both equal to zero and then the third linear combination that we saw was that 3 * A+ - 2 * B this gave us 3 and zero this was our third linear combination where beta 1 was three and then beta 2 was minus 2 so so then the linear combination of these two vectors is basically all the possible combinations of these two vectors that I can get when scaling or when multiplying these two different vectors by different sorts of uh vector by different sorts of scalars so in all these different cases what I'm simply doing is I'm taking different sorts of coefficients beta 1 and beta 2 and then I'm getting the linear combination of these two vectors we saw that in the simple case when we take a and we had to B so basically the coefficients are equal to 1 so 1 * a + 1 * B then the corresponding linear combination is equal to one and five it means that we are getting this vectors so one and five is in here which means that we are getting this one this Vector if we get if we take the zero as a scal so beta 1 and beta 2 are both equal to zero then the linear combination of these two vectors is simply the vector zero which means that it is this point then if if we take the linear combination using three and minus 2 as coefficients then we are getting this so 0o and three so one two and three this is three then this is our linear combination I can also take any other uh like scaled version of my B and of my a and then I will get entirely different sort of vector so let me actually show you a few more times um a couple of other examples so let's say I keep my a so I just take the beta 1 equal to 1 but instead I scale my Vector B two times so this was at three I'm taking two times of my Beta which means that I'm here then I can take this I can add this to my a so this is 2 B this will give me another linear combination of these two different vectors I can also you might recall that we said that the starting point and the end point doesn't really matter for for us what matters is that we uh have the same magnitude and the same direction for our vectors so this means that for me the vector being here and the vector being here doesn't matter when I scale it with two I can be here with three I can be here so this is the same as my B only 3 * B this is basically basically scaling B with three and this in here means that my Beta 2 is simply equal to tree and then this means that I can combine this with my a which was in here you remember so this here this means that I get yet another linear combination of these vectors which means that I'm taking 3B and I'm on this my a so 1 * a my beta 1 is equal to 1 my Beta 2 is equal to 3 which means that the linear combination of this is equal to one two plus and then 3 * B is equal to 0 and then 3 * 3 is 9 this is then the new linear combination which is 1 and 11 so the new linear combination is equal to 1 and 11 so this thing which is the same as this thing and then you can go on and on you can also calculate the same with a negative B so you can take B and then you can scale it with minus one so this is minus b or you can go in here in here the same holds for a so you can scale it all the way to here or in the negative side so so this already uh give us the idea that we will go into to the next point which is the span so when it comes to the linear combination and in this specific case when we have these two vectors we can combine these two vectors in anyway and uh we can mix them up by using different sorts of coefficients of beta 1 and beta 2 and we will can we can get any Vector in our R2 so this means that any vector in our R2 we can represent by using only these two vectors and this is not always the case for this specific case we are dealing with two vectors that we can use to represent any Vector in our r two so what I mean here is that let me clean this up so independent what kind of vector you will give me in the R2 so it has two different elements it is 2x one I can use a linear combination of A and B so a linear combination of A and B is beta 1 * a plus beta 2 * B in order to represent this Vector X1 and X2 therefore we are saying and we will come to this um in the next slide too that the spend of the vectors A and B so this is the set of all possible combinations of these two vectors is equal to R2 because any Vector in R2 can be represented as a linear combination of these two vectors so we have a linear combination of A and B and I'm saying that I can represent any Vector so here vector X and this Vector X I'm representing by X1 and X2 and X1 and X2 can be any real numbers so X1 and X2 they belong to R and X is simply a two-dimensional Vector so X1 and X2 those can be any numbers 0 1 2us 100 anything and I'm saying any number in this two dimensional space so whether it is this one any Vector this one this one or this vector or this one any Vector that you give me in two dimensional space I can find a linear combination of this A and B that is equal to that Vector so I can represent that Vector as a linear combination of vector A and B that we saw before so let's actually prove that so I'm going to represent this uh X1 and X2 by a linear combination of this Vector A and B and how we can do that so we have beta 1 * a plus beta 2 * B it is equal to X1 and X2 where beta 1 and beta 2 so beta 1 and beta 2 they are constants so they are also real numbers so let's unpack this which is beta 1 * 1 2 + beta 2 * 03 and this should be equal to X1 and X2 now this is equivalent of so beta 1 * 1 beta 1 * 2 plus beta 2 * 0 and then beta 2 * 3 and this should be equal to X1 X2 so this is my beta 1 a this is my Beta 2 B and this is my X all right so now what we get is that and this is equivalent beta 1 * 1 is equal to beta 1 beta 1 * 2 is 2 beta 1 plus then here beta 2 * 0 is 0 and then beta 2 * 3 is 3 beta 2 so we have learned U from the uh operations on the vectors that beta one uh so in this case when we are adding two vectors so beta 1 +0 is the uh amount that we need to put as our first element so when we are adding two vectors we just need to take their corresponding elements we need to add them up so equal to beta 1 + 0 and then 2 beta 1 + 3 beta 2 this is the result and this should be equal to X1 and X2 at least this is what I'm claiming so this zero doesn't matter so we what we are getting from here is that beta 1 is = to X1 and then 2 beta 1 + 3 b. 2 is equal to X2 this is the two expressions that we are getting based on all these different calculations so let me actually remove all this so we have beta 1 is equal to X1 and 2 beta 1 + 3 beta 2 is equal to X2 so here given that we have already that beta 1 is equal to X1 and here we have two unknowns I'm going to fill in the value for beta 1 in here so I'm going to take this and I'm going to fill in it in here so for this volue so remember that beta 1 and beta 2 are two unknowns and each one and next to are just uh numbers that we will get when we uh know exactly the vector and we just want to represent the vector as a linear combination of two vectors so when I take this uh value for beta 1 which is equal to X1 and I'm going to fill that in in here it means that I'm going to get from here that beta 1 is equal to X1 and 2 * X1 because beta 1 is equal to X1 and here I got beta 1 I'm just filling in that value for beta 1 which is equal to X1 so 2 * X1 and then the rest I'm just taking over 3 beta 2 is equal to X2 let me remove this and from here what we are getting is that beta 1 is equal to X1 and I will solve this equation for the unknown which is equal beta 2 so I will just take the three beta 2 from left hand side I will leave it there and I will take this and I will take it over to the right so I'm taking X2 over and this two X1 so this part I'm just taking to the right two of the equation so Min - 2 X1 which then on its turn is equal to so it goes to B 1 is = to X1 and then beta 2 is = to X2 - 2 X1 / 2 three perfect so what do we get here what is our end result and why is it significant so what we are getting here is that based on all this information without knowing beta 1 and beta 2 we got that beta 1 should be equal to X1 and beta 2 should be equal to X2 - 2x1 / to three this means that independent what kind of x's you will give me so what kind of vector we have in our R2 so this X1 and X2 they are just real numbers we can always find beta 1 and beta 2 that we can use to represent that X1 X2 so our X Vector as a linear combination of these two vectors let me actually give you an example so let's remove this so let's assume we have a vector X and this x is is equal to four and let's say three so if we got this Vector X and we are saying we can use this Vector A and B to represent X as a linear combination of vector A and B which means that I can find I can find real number beta 1 and beta 2 that I can use to multiply the vector A and B respectively combine them together so their linear combination that will be equal to this Vector X so this is my X1 this is my X2 so this is equal to 4 and three Now using this let's actually see whether that is true so based on this example my beta 1 should be equal to X1 which is four my Beta 2 should be equal to X2 which is 3 so beta 2 should be equal to X2 which is 3 - 2 * X1 which is 4 / to three and what's this number this means that my beta 1 should be equal to 4 and my Beta 2 should be equal to 3 - 8 so 3 - 8 / to 3 and this is equal to Minus 5 / to 3 so this means that I use a coefficients beta 1 is equal to 4 and beta 2 = to - 5 / to 3 to represent my Vector X as a linear combination of vector a and Vector B so let's actually prove that too as a final step so let's see where the four times Vector a which is 1 2 + - 5 / to 3 whether this is indeed equal to Vector X so my Vector B is 03 so this is the first part and I want to prove that this is indeed equal to X and we already know what x is so this is equal to 4 * 1 4 * 2 plus and then here we got - 5 / 3 * 0 and then - 5 / to 3 * 3 this is equal to 4 * 1 is = to 4 4 * 2 is = to 8 and then here we need to subtract minus 53 5 5 / 3 * 0 is equal to 0 so this one is zero and then minus 5 / to 3 so 5/3 * 3 this ones are canceling out and we got 8 + - 5 so here the plus and here minus just to make sure we got everything right and this is equal to 4 and then 8 + - 5 is equal to 3 so you can see already that this amount that we got here is equal to X which was equal to 4 / to3 and this helps us to uh verify and to know for sure that indeed while given any Vector in a two dimensional space in R2 X independent what this X1 is or X2 is we can always find a pair of beta 1 and beta 2 that will ensure that the beta 1 a plus beta 2 B is actually equal to this x where X A and B they are part of R2 and a is equal to 1 2 and then B is equal to 03 so we can represent any Vector in our two-dimensional space as a linear comp combination of this Vector a with elements 1 2 and um Vector B with elements 03 and that's exactly what we saw here because we could find any vector and we can represent this Vector as a linear combination of this Vector A and B this Vector as a linear combination of this A and B this Vector as a linear combination in any vector or a point in this plane we can represent as a linear combination of this Vector a and Vector B and in this specific case with this Vector a and Vector B we are saying that Vector a and Vector B they spin R2 so Vector A and B span R 2 now we will come to these definitions of the span and uh just in general for different sorts of vectors we will see what this IDE of span is but for now given that we just proved that we can represent any Vector in R2 as a linear combination of these two vectors A and B therefore we can say and we usually say it in linear algebra that the vector a and Vector B they spend R2 before moving on onto this concept of Spence that we just touched upon in our example I wanted to quickly go back to this example that I promised to discuss uh which was part of the definition of the linear combinations and unit vectors because we saw in our definition and let me just show you that uh the uh definition was providing these two highlights these two bullet points and was saying any Vector B in N Dimensions can be expressed as a linear combination of the standard unit vectors E1 to up to e n and the coefficients in this combination are the entries of B itself so let's look into the example and see what we mean by that in this specific example we have this Vector B it is coming from the three dimensional space which we can see given that we have three different uh three entries so three uh elements in our Vector so it's 3 by one and this means that b belongs to R3 and in here we can see that B can be written as a linear combination of these three vectors so you can see that b e is equal to -1 * this Vector 1 0 0 so this one then we have + 3 * 0 1 0 Vector so this one and plus 5 * this third Vector which is 0 01 now we already know from the unit vectors that E1 is equal to 1 0 0 assuming that we are in three dimensional space E2 is equal to 0 1 0 and E3 is equal to 0 01 you can notice that that's exactly what we got here this Vector is E1 this Vector is E2 and this Vector is E3 where E1 E2 and E3 belong to three-dimensional space okay so another thing that we can see is that here we got coefficients minus one here three and here five so this is basically how beta 1 beta 2 and beta 3 using the common conventions that we saw before when describing the linear combination so let's actually check that and then we will comment on these values so let's check whether -1 * E1 + 3 * E2 + 5 * E3 is indeed equal to this B so this is equal to -1 * this Vector gives us -1 0 0 three times this E2 gives us 0 3 and zero and then 5 * E3 gives us 0 0 5 and this is equal to -1 + 0 + 0 is = -1 0 + 3 + 0 is = 3 and then 0 + 0 + 5 is equal to 5 now what do we get here we see that this which is equal to this it is equal to this Vector B indeed okay so now when we have indeed checked that B can be represented as a linear combination of this three vectors this unit vectors E1 E2 E3 another thing that we can notice and I'm sure you already did is that those coefficients they are not just randomly picked coefficients those are the entries of this Vector B so this is exactly what that definition was about it was saying that any Vector B including this example in in this case threedimensional space can be Express as a linear combination of the standard unit vectors E1 A2 E3 Etc so this coefficients in this combination so you can see that the beta 1 beta 2 and beta 3 which are our coefficients in our linear combination they are the entries so this values of the B itself so the same will hold for four-dimensional case five dimensional case n dimensional case so this means that if we write this down for General case just to ensure that we are clear on this part of the definition so if we got B Vector in N dimensional space so it got B1 B2 up to BN as the elements of it comes from RN then we can represent this B as a linear combination of unit vectors coming from the N dimensional space so we got E1 E2 up to e n that belong to n dimensional space and we can represent this B as a linear combination of these unit vectors so by using beta 1 time so this this is a common Convention of the coefficient as you Rec called time C1 then B beta 2 * E2 blah blah blah plus beta n * e n and what is important here is that this beta 1 beta 2 and beta n those are not just some coefficients but we already know what these coefficients are because we can then represent this beta by taking the values so those are the entries the elements of the vector B itself so it is B1 * B1 plus b2 time E2 dot dot dot plus BN times e n where B1 B2 up Q BN they are all real numbers so basically knowing what these Vector is these elements of this Vector we can always describe and express it as a linear combination of the standard unit vectors and if you're wondering why is this important in some cases when performing different operations or working on different algorithms it just becomes handy to represent your vector as a linear combination of multiple vectors and in those cases exactly you can make use of this property of linear combinations to express your n dimensional Vector b as a linear combination of the standard unit vectors because everything is then down to you by having this Vector B you will already know what are the entries that you can use as your coefficients in this case beta 1 beta 2 so those are all these values coming from your vector itself and then the remaining is also none because you know exactly what these unit vectors are and how they are represented so here for instance the E1 is basically one 0 0 blah blah blah blah 0 and then this is n by1 Vector here the E2 is equal to 0 1 Z blah blah blah and then zero here so n * 1 again up to the point where you have the N where you have all the zeros only the last element is one again n by one vector so this is the idea behind this second part of this definition which says that any Vector being in N dimensional space can be expressed as a linear combination of the standard unit vectors E1 up to e n let's now talk about other concept which is also super important which is the span of vectors so by definition the span of a set of vectors is a set of all possible linear combinations of these vectors so if V is equal to V1 V2 up to V K and is a set of vectors then the span of V is written as a span V and it includes any vectors that can be expressed as C1 V1 up to C2 V2 up to CK VK so basically it is a common uh notation uh to say that if we got for instance vectors V1 V2 up to VN so we have n different vectors then we say that the span of V1 V2 up to VN that this is simply the notation that we use in order to describe the span of these vectors and we briefly spoke about this concept of span when we were looking into our example that we saw before so you might recall vectors A and B that we had and we saw and we said that the span of a and b is the entire space in the two dimensional uh real number space so we said that span of a and b is equal to R2 where our Vector a was simply equal to one 2 and B was equal to 03 so we proved that the span of one two and 03 was the entire R2 and how we knew that because we proved that any Vector in R2 could be represented as a linear combination of these uh two vectors so you might recall that we solved this equations we saw that in depend what kind of X1 and X2 uh one will give us we can always use the uh um we we found this amount let me see where I can find it back I no longer have this so we saw that for uh specific values of um beta 1 and beta 2 we can always get a linear combination of this A and B in order to get our desired factor x so beta 1 * a plus beta 2 * e will always then be equal to X1 and X2 if our Vector a and Vector B are those but of course this doesn't hold for all the vectors so not for all two-dimensional A and B uh we can say that the span of these vectors is the entire R2 therefore to better understand this concept of span and this concept of span of vectors I wanted to distinguish five different cases one of which we already spoke about and that is the case when we had this Vector a and Vector B and we said that the span of a and b is the entire R2 but we will also look into the case when we for instance have a span of the zero Vector the span of a single vector and the span of perpendicular vectors we might also look if there is time left we will also look into the span of parallel vectors so let's now look into this cases one by one so let's say we have a vector of zero so we have a zero vector so this is a very simple case we will start with the simplest case and we will move on B to two more advanced cases if we have a vector a that is a zero Vector 0 0 then independent what kind of scaler we will use to scale this so let's say um we Define it by C so C * Z independent what kind of scale we will use this will always end up being equal to 0 0 so if C is equal to0 C * 0 will be equal to 0 if C is equal to 1 C * 0 will be equal to Z or C is equal to 100 C * 0 will still be 0 0 so independent what kind of scal we will be using what kind of lead linear combination we will create from our Vector a this will always stay in here so the point the vector will always stay in here in our two Dimension space so this is completely different from what we saw before when we could create and we could take any Vector in our R2 and we could represent it as a linear combination of these two vectors that we saw in the previous example so in this specific case um scaling the zero with independent of any scalers we use this will not change the magnitude nor it will change the direction of our Vector so no matter how we scale it we still get zero this means that the span of zero Vector is just the zero Vector itself so you can see that independent what I scale the zero Vector I always end up with the same zero Vector so therefore this span of the zero Vector is equal to zero because by definition this Spen of set of vectors is the collection of all possible vectors that I can reach by performing linear combination and in this case I will always Reach This Z 0 Vector so all possible collections of these vectors are the vector 0 0 which is single vector and the same as the input so this is the basic case now let's move on onto bit more uh Advanced case so bit more complicated than this one but itself also very easy which is when we got a single Vector a so let's say a is equal to one and two now I want to know what is the span of a in order to know what is the span of a we simply need to understand what are all these possible collections of vectors that I can get when I'm uh combining um a I'm multiplying a with different coefficients so what are the all possible linear combinations of this Vector because I got just single Vector a so a is one one two which means one in here and then two here my a is this vector and let's look into uh different uh scalar multiplications of this Vector so let's say I want to calculate C * a so the scalar multiplication of this where C is equal to C is equal to 2 C is equal to 3 C is equal to uh Min -1 C is = to - 3 and of course C is = to 1 so in all these cases when C is equal to 1 then the linear combination in this case just the scale multiplication of this single Vector a so 1 * a is simply equal to one and two so the same Vector a c is equal to 2 this will give me 2 and four C is equal three this will give me 3 and 6 C is = to min-1 will give me -1 - 2 for my a and then C is equal to -3 will give me -3 and then - 6 so let's plot each of those so if we got for instance C is equal to one case you can see that we already got that Vector in here so it is this Vector when C is equal to two then we got this one so two and four so where is that it is in here let me use another color it is in here when when we got C is equal to three then we got so this one three and six so this is three and this is six so it gives me this Vector in the next example so in the next linear combination we have C is equal to minus one so we got Min -1 and Min -2 so where is min -1 it is in here where is min-2 it is in here so I'm getting this vector and then finally when I have let me change the color when I have C is equal to minus 3 so this case then I got minus 3 and 6 which means that here is my minus 3 here is my minus 6 so we got this thing so you already should see what is going on here when we got just the single Vector for which we need to know what is a linear combination and that Vector is not a zero Vector it has nonzero elements but um it's still it is just a single Vector then all its linear combinations given that it is simply a scaled multiplication of it we are all getting them on the same line so you can see all the linear combinations of this single Vector is just a scaled version of it and it lies on the same line so what this tells us is that essentially you can move along the line defined by this Vector a but you cannot leave it so you cannot get a vector that is in here that is in here that is in here in here so you cannot leave this uh line you will always stay on this line so this line essentially spend of a so when we got a single vector and that Vector is not equal to Z Vector then the span of a is equal to and this can be expressed as C * a given that the C is a real number so we already saw this independent of what kind of scalar we will take any linear combination of it will end up simply the C * a so therefore we are generally izing this and we are seeing that the span of a so to set of all possible linear combinations of this a is simply equal to C * a given that the C is a real number this is basically the spend of a real uh uh Vector in a two dimensional space let's now look into the next case the next example when we will calculate or we will Define the span of a perpendicular vectors so let's look in into another example when we are looking for a case when the um when we want to find out the span of perpendicular vectors so imagine we have these two vectors Vector a and Vector B where a is equal to 1 0 and then B is equal to 0 1 so we are still in our lovely uh two-dimensional space so let's first visualize the vector a it's quite basic it is this one and then Vector B it is simply this one so we can already see why they are perpendicular so you can see that they are forming this um 90° angle so right angle here and then we know that the span so that's exactly what we want to find out so the span of a and b and this is what we want to find out and we know that the span of two vectors is the set of all possible linear combination of these vectors so we want to see what are these all possible inar combinations of C1 so all the possible outcomes that we will get when we get a linear combinations of these two vectors so basically C1 * a plus C2 * B because those are all the linear combinations of these two vectors C1 * a + C2 * B nothing thing that we can see here quickly is that C1 * a so this part those are all the scaled versions of a so scaling multiplications of a and this second term in the linear combination those are all the scal variations so scalar multiplications of vector B which means that and we already have seen this time and time uh again that when it comes to Vector a all its linear combinations they will lie on the same line so let me take this color so if I do 2 a so C1 is equal C2 then I will be in here if C1 is equal to three then I will be here C1 is equal to 4 I will be here C1 is equal to 10 I will be in here and then the opposite holds as well if C1 is equal to for instance minus uh 2 then I will be in here if it's equal to Minus 5 I will be uh my Vector will look like this and so on so this means that all the scaled multiplications of vector a will lie on this line so I can also say that the span of see uh the span of a so span of a is simply equal to C1 a so you can see in here so on this line basically so this is C1 a so this basically means independent what kind of C1 I will take with is 1 2 3 0 - 5 - 100 I will always end up on this line so this line so this about the uh scaled multiplication of a but of course to create this linear combination of A and B we also have the second element which is the all possible scaled multiplications with a vector B so C2 B2 so let's see what that looks like so if I for instance take C2 is equal to Z I will be in here if I take C2 is equal to uh 2 I will be in here C2 is equal to 5 I will be in here C2 is equal to Minus 5 I will be here so you are already seeing what is happening here so all the possible scaled multiplications with Vector B will be on this line so now we are then getting that the span of B will then be equal to C2 and then B and here I'm not uh using formal notation I'm just trying to um I'm just trying to uh draft the idea of the spin of vector a and uh span of vector B because we are not uh done yet we still need to combine the two in order to find the span of vectors A and B when they are perpendicular so when this angle is simply 90° okay so let's also add this on our plot so this is C2 and then B so this already gives us an idea that all the possible combination of the two so when we add these two elements to each other the outcome will always lie on these two lines but there is no way that we can find any other coefficient for C1 or C2 that can help us to get a value that will be so a vector that will be in here or in here or in here or in here that's just not possible so just you can try to go ahead and solve that equations like we did before before and you will see that there there is no way that you can pick here a line and you can represent it as a linear combination of these two vectors it just not possible and later on we will see why but just keep in mind for now that once we have this this type of vectors when two vectors are perpendicular then um we cannot find a line a vector that is outside of the two lines so here you can see the xaxis and the Y AIS but it can also be like this it can also be like this but then you cannot find any other line that lies outside of this area that you can uh create a linear combination of these two different vectors and then you say then you cannot say that you can create a linear combination of these two vectors A and B so therefore when it comes to defining the span of the two perpendicular line we say that the span of a and b given that A and B are perpendicular but also given that these values in this case you know a is equal to 1 is z b is equal to 0 and one then their spend you might have already guessed is equal to C1 a plus C2 B given that C1 and C2 are of course real numbers so in this case C1 and and C2 as expected are just scalar so they are just some real numbers coming from R and uh this A and B those are vectors that are being spent and in this case specifically the vector a is equal to this one zero and Vector B is equal to 0 1 and this expression that we see here this pen this simply describes the set of all possible vectors that can be formed by adding the scaled versions of this A and B so C1 a plus C2 B in order to form this linear combination so this set this set of C1 a plus c2b which is a linear combination all possible linear combinations of the two vectors so um it effectively covers the entire plane illustrating that any point in 2B space can be reached by some combination of A and B let's now move towards our final example that we saw also as part of our definition for the span of vectors in order to check and to learn how we can usually check uh whether the two vectors they really spend the entire space so in this case we got two vectors we got Vector uh V1 which is equal to one two and Vector V2 which is it's equal to three4 so in here and also um we uh have in our example that it says the span of V1 and V2 is all over the R2 because any Vector in R2 can be expressed as a linear combination of V1 and V2 so the example basically is saying that if we know that um we can express any Vector in R2 as a linear combination of V1 and V2 2 then we say that the span of V1 and V2 is the entire R2 so let's actually go ahead and prove that from our example so we have X which we can represent as X1 and X2 and X1 and X2 are just real numbers and we got V1 which is 1 2 V2 which is 3 4 and we got in our example that uh we need to prove that the span of V1 and V2 is the entire R2 so for that what we need to do is we need to prove that we can express our coefficients C1 and C2 in such way using X1 and X2 that independent of what these X1 and X2 are so what kind of X Vector we have whether this is like one two or this is 04 or this is th000 and uh 5,000 independent what kind of vector we get uh we give here so X1 and X2 values as long as those are real uh numbers we can always find a set of C1 and C2 that we can use as coefficients in order to create a linear combination from vectors V1 and V2 and in that case we say then the span of V1 and V2 is the in par R2 okay so let's go ahead and actually prove that using our previous knowledge that we already gained so keeping in mind that C1 and C2 are unknown numbers for us whereas X1 and X2 are just a way to describe those elements in our Vector X that will be provided to us so X1 and X2 will be basically know and C1 and C2 are the unknowns that we are chasing so for that the first thing that I'm going to do is to describe this linear combination that we got here C1 V1 plus C2 V2 with actual equations unknown equations and the way I'm going to do it is by simply filling in this Vector V1 and Vector V2 um values so we have C1 and then C2 here and then here I got one two plus and then three and four here and what is this amount so this is equal to let me actually go on to the next row so we can create um set of equations for this so this is equal to X and we get C1 * 1+ C2 * 3 is = 2 and remember that this is X and we said that the x is equal to X1 and X2 so this basically equal to we can B right here is equal to X1 and X2 so this is then equal 2 let's not skip all the steps X1 and X2 so here then the second elements need to be added so C1 * 2 and C2 * 4 which is then the same as C1 + 3 C2 and then 2 C1 + 4 C2 and then this we are saying this Vector is equal to X1 and X2 so this is what we have here and let's move from the vectors to equations so given that we have this we are allowed to say that this gives us actually two equations this means that this element this element from this part should be equal to this and this element should be equal to this now let's write it down we see that c1+ 3 C2 should be equal to X1 2 c1+ 4 C2 is equal to X2 this is all that we see in here let's remove this to keep the space clean now what this means is that we have two equations with two unknowns C1 and C2 and X1 and X2 are the numbers that will be provided to us as part of our Vector so what we want to prove is that we can describe and we can express C1 and C2 which are our unknowns using X1 and X2 so you see here this is C1 C2 those are our nouns and we want to describe them by using X1 x 1 and X2 and very soon we will also see why so for now let's try to express those two unknowns using our nouns like X1 and X2 so here I already see that C1 is alone so there is no scaler so I will make use of that opportunity to keep the C1 on the left hand side and I will take this this amount to the right so I will say C1 is equal to X1 minus 3 C2 two okay slightly better so I have C1 at the left I do have X1 in the right but I also have three C2 in here but another thing that you will notice is that in my second expression here I got 2 C1 + 4 C2 plus X2 I want to have the C2 only because then I will have an expression of my C2 only using X1 and X2 um so numbers that are that will be provided to me that are n so for that what I'm going to do is basically trying to solve uh two equations with two unknowns exactly the same um process so I'm going to take this C1 from the first equation and I'm going to fill in in the second equation so I am going to say two times and here I'm going to fill in that C1 expression from here so X1 - 3 C2 2 so this is my C1 plus just taking over this part so 4 C2 is equal to X2 okay perfect so now what I end up with is C1 is = to X1 - 3 C2 just taking it over and then here I'm opening parenthesis which is 2 X1 - 6 c2+ 4 C2 is equal to here I forgot an X2 is equal to X2 okay one step closer why because I in my second equation I no longer have a C1 I only have a C2 which is great which means that this gives me an indication that I can rewrite the C2 which is unknown with nouns with X1 and X2 so let's make use of that opportunity the first equation I would just take over so C1 is equal to and then X1 - 3 C2 and then here I will do 2 X1 and then here we got 2 * c2s which means we can combine them so Min - c - 6 * C2 + 4 C2 it gives me - 2 * C2 and this is equal to X2 all right let's now solve that part so what I want to have is just the C2 in the left hand side which means I need to bring all this to the right and I need to get rid of them such that I can leave the C2 in the left entirely alone so this is what I'm basically chasing for that I'm going to once again rewrite C1 is equal to X1 - 3 C2 and this time I'm going to take them minus 2 C2 here I'm going to leave that in the left but then this one I'm going to bring to the right so X2 - 2 X1 here I need to be very careful to not make a mistake CU that will mess up my entire calculation all right so now we are one step closer just taking over the first equation again so C1 is equal to X1 - 3 C2 and here what I need to do to get rid of this minus two is to divide the two sides so both 2 minus 2 CU that will help me to keep the C2 only in the left alone without any scaler so the C2 is then equal to X2 minus 2 X1 / 2 - 2 so this is what I end up with perfect so we are very close stay with me so uh here what we are getting is that C2 is equal to this amount we see that now we no longer have any other C in here which is great and remember that X1 and X2 will be numbers that it will be provided to us I just wanted to give everything General and then uh another thing that I want to fix is this C2 because this C2 is an unknown and I want to fill in uh this value of C2 in here such that for the C1 I will have a similar picture so in the left hand side I will have C1 in the right hand side I will Express the C1 with no number so X1 and X2 but not the C2 or others all right so let's then go ahead and do that first I will write C2 in a simpler way so C2 is equal to here I got a min I will just write here minus so I will take the minus over here and then I will write X2 - 2 X1 to U be super careful with this minus therefore I'm using parenthesis so now I'm going to use this C2 and I'm going to fill that in in here so C1 is equal to X1 minus 3 * I can also make it plus because minus of here so minus of here and minus of here will cancel out therefore I will do plus three times and then X2 - 3 X1 / 22 and this then gives me C1 is equal to X1 + 3 / 2 * X2 - 3 sorry 2 almost made a mistake 2 X1 and then C2 is = to X2 - 2 X1 / 2 2 and here we got minus Perfect all right awesome so now we have expressed X1 and X2 well careful with this X1 and X2 we only noun numbers now what I'm going to do is that I'm going to prove that independent what kind of X we will be taking here we will end up getting the C1 and C2 using this what we just found here that will give us a linear combination of these two vectors that will be equal to that eight so for that so to prove that this pen of V1 and vs2 is the entire R2 I need to prove that independent what kind of X I will take so X1 and X2 I can always find the C1 and C2 that I um just calculate in here using that X1 and X2 that I can then use to combine with my V1 and V2 to find the linear combination of these two vectors with that C1 and C2 which will be equal to this x so for that what I need to do first is to take such a uh random X so let's say my X is equal to 0 and 4 this means that my X1 is equal to 0 and X2 is equal to 4 what this means is that this gives me C1 which is equal to and here X1 so I'm basically filling these two values for here to obtain my C1 so C1 corresponding to this specific Vector X so X1 is equal to 0 which means I end up C1 is = 0 + 3 / 2 * X2 is = 4 so 4 minus and then 2 * X1 is equal to - 2 * 0 which is 0 and then C2 is = to minus and then X2 is equal to 4 so 4 and then minus 2 * XY is = to 0 0 and then this divided to two now what are those numbers so C1 is equal to 3 / 2 * 4 which is 3 * 2 so 6 and then C2 is equal to- 4 and then minus so this is zero this cancels out which means 4 / 2 is 2 and then C2 is equal to minus 2 so basically I have calculated the coefficients C1 and C2 by just knowing what is this Vector so knowing X the provide X1 and X2 I have calculated my C1 and C2 using my derivations in here so let's now get rid of this this calculations to clear some space and to do the final part which is compute the linear combination of vector V1 and V2 for this specific coefficients well knowing what this given Vector now is example random Vector so the C1 is equal to 6 which means 6 * and then Vector V1 is one 2 so this is first part of my linear combination plus and then C2 is equal to - 2 times then here 3 4 what is this this is equal to 6 and then 6 * 2 is 12 plus now let's calculate the second part - 2 * 3 is - 6 and - 2 * 4 is -8 so what does this give us 6 - 6 and 12 - 8 this gives us zero and four nice so this confirms that we have done everything also correctly which is great because we have seen that using this C1 and C2 that we have just calculated we have successfully uh computed the 6 V1 so linear combination of this uh two vectors V1 plus - 2 minus 2 and then V2 and we have seen that this linear combination is equal to 04 which is exactly our 8 so in this way we have proven that independent what kind of vector we will pick what kind of X we will pick here we can always find and calculate the corresponding coefficients C1 and C2 in the same way as I just did and then by using those when we calculate the linear combination of these two vectors with this specific coefficient this will be exactly equal to X and this proves that independent what kind of vector we have in our R2 we can always express that as a linear combination of the vector V1 and V2 and this proves and this concludes our proof that span of V1 and V2 is the entire R2 all right so we are very close to finishing up this unit so the next topic we are going to talk about is a linear Independence and all this important stuff that we learned as part of the previous modules are going to become super handy as part of this specific concept so we just spoke about the idea of span we have plotted a lot of vectors we have seen the linear combination of that and how we can find out whether the span of multiple vectors is the entire space uh for instance the R2 or it is just the line or it's maybe the zero Vector we have seen many examples and many operations we have we have also seen this idea of unit vectors and we are finally ready to come to this very important concept which is a concept of linear Independence so by definition linear Independence says that the set of vectors is linearly independent if no Vector in a set can be written as a linear combination of the others otherwise they are linearly dependent so vectors V1 V2 up to VN are linearly independent if and only if the only solution to the equation C1 V1 + C2 V2 plus CN VN is equal to zero is C1 is equal to C2 up to CN is equal to Zer in other words in a l linearly independent set the equation C1 V1 + C2 V2 plus CN VN is put zero has only the trial solution where all CIS are zeros so now what do we mean here there is a ton of information in this definition so let's unpack them firstly it's really important to uh keep in mind this idea of Independence and dependence Independence and dependence there are things that we commonly use in data science in artificial intelligence in statistics so those are really important so we basically have linear independent condition so there is a certain condition that our vectors should satisfy vectors in our set in our Vector space for them to be named as linearly independent and otherwise we are calling them linearly dependent and you can see that here there are a couple of Parts as part of this definition first it talks about um being unable to create a vector in the vector set while using the remaining vectors in our set so it says if you can use the remaining vectors in your vector space and linear create a linear combination of them so linearly combine them and we have already seen the definition of linear combination so if we cannot create such linear combination from the remaining vectors to get our Target vector then we are saying that we have a linearly independent vectors so if we want to say that all our vectors in our Vector set they are linearly independent it means that each of those vectors we should not be able to recreate out of the remaining vectors so we should not be able to find coefficients to create linear combination using the remaining vectors in order to get our Target vector now what do I mean by this target Vector what do I mean by this linear combination uh I will come to this in a bit for now let's just try to unpack this definition cuz uh with examples uh we will definitely go through this step by step in detail such that this ideal linear Independence and dependence is super clear so in the second part of the definition it says vectors V1 V2 up to VN are linearly independent if and only if the only solution to the equation and we have here in the left hand side you might recognize the linear combination of our vectors V1 up to VN so in in the right hand side you have zero so you are saying our linear combination of vectors is equal to zero if and only if C1 C2 up to CN is equal to zero so linear Independence basically claims that we will have linearly independent vectors only if and only in the condition when um the only way we can create linear combination of these vectors equal to zero only if those coefficients are zero there is no other way that we can get a linear combination that is equal to zero while those coefficients are not zero so the only way that we can get a linear combination out of all our vectors equal zero is only when all of the coefficients C1 1 C2 up to CN is equal to zero that's something that we will come later to this again this something also that we are going to come back in our next module and the next one so um this one will be also super clear once we go through those modules but for now keep in mind that the uh linear combination of all these vectors can only be zero in case when all these coefficients are equal to zero so and then we have the third part in our definition which says that in other words in a linearly independent set the equation C1 V1 plus C2 V2 up to CN VN is equal to zero has only the trivial solution where all CIS are zero so this explanation is basically what we just spoke about as part of this second part where we said that only in case the coefficients are all zero we can have a linear combination of our vector V1 V2 up to VN which is equal to Zer and why we would like this linear combination to be equal to zero because it's a common way to find solution to our linear system so this is something that we will also see as part of the next module when we'll be discussing the idea of solving linear systems we will go into more uh Advanced topics but for now in order to understand this idea of linear Independence we should just keep in mind that we cannot find any CIS so C1 C2 so any coefficients that is not equal to zero and then expect that the linear combination of these uh linearly independent vectors is equal to zero so that's the uh if and only uh if and only uh if part which means that this holds from both sides on one hand we have V1 V2 up to VN which are linearly independent only if the linear equation so the linear combination of all these vectors is equal to zero if all these coefficients are zero but also the other way around holds as well so if we have a linear combination that is equal to zero only if those coefficients are zero that means that we are dealing with a linearly independent vectors this is the if and only if part which means that we have this uh conditions from both sides if one holds the other one holds but also the other way around all right so let's now look into specific examples that will make our journey in understanding linear dependence much more convenient so let's say we have our coordinate system and we have these two different vectors so we have Vector let's say 2 and three which is our Vector a and we have a vector B that is equal 2 6 and N so those two are our vectors and what we want to understand is where those two vectors are linearly independent or linearly dependent so one thing that you can quickly notice is that b looks quite similar to a in terms of its scal so there is a way that we can recreate Vector B by using Vector a now you can see that if I take Vector a which is equal to 23 if I take Vector a and I multiply it by three so three * Vector a this is a scale multiplication then what I can get is three times and then I have here two three and this is then equal to 3 * 2 is 6 3 * 3 is 9 this gives me 6 and 9 which is our uh Vector now another thing that you can notice that that is exactly my B so you can see that those two are similar which means that three * a is equal to B now what this means is that I can recreate Vector B by using Vector a so in our definition we saw that a set of vectors is linearly independent if no Vector in the set can be written as a linear combination of the others so here I can take this 3A as a way to write down a linear combination so 3A + 0 * B is then equal to B which is basically saying 3 a is equal to B so by using these two vectors in a set I can then create a linear combination of the two and actually even basic way of writing this is saying I can use the vector a to write a linear combination from this so 3A is a linear combination so just a scaled multiplication in this case of course but if we have just two vectors our Target Vector is B and I want to write this uh I want to see whether I can rewrite the vector b as a linear combination of the remaining vectors which is Vector a so I can then write Vector B as a linear combination of vector a because I can say that 3 * a is equal to Vector B so this means that Vector a and Vector B they are linearly dependent this means that I can use Vector a to recreate Vector B and of course I can also do the other way around right what I can do is that I can just take Vector B so I can take Vector B I can multiply it by one ided to three 1 / 3 is real number so I'm just performing a linear combination using B and this will give me 6 / to 3 is 2 9 / to 3 is Tre and I'm getting exactly what I have under a so I can then also rewrite Vector a by using Vector B so I created a linear combination using Vector B in order to get a vector a and that's exactly the opposite what we have learned here because we should not be able to write this a vectors using the other ones in our set cuz otherwise we have a linearly dependent set therefore we are saying that Vector a and Vector B they are not a set that is linearly independent but they are linearly dependent before moving on to another example I also wanted to visualize these vectors just to see what is going on with this pan and uh how the two linearly dependent vectors look like in R2 so this is our r t we have a vector a which has two tree elements so we know already the magnitude and the direction this is two this is three which means here let me actually use another color so 2 three which means this is my Vector a and then my Vector B is simply six and N so it is this one so you can already see what is going on so this is Vector a and this entire thing is Vector B and you can see that those two vectors no matter how I combine them I can I will always get the combination so linear combination of the two on this line if I want to get um Vector that is for instance in here I can never find a scalers of C1 and C2 in such way that these vectors so A and B they can form a linear combination that will give me this Vector there is no way that I can do that and that's why uh we say that this span of this two vectors so span of A and B with this A and B is this line and we cannot express any of these other vectors like this one or this one using a linear combination of these vectors A and B the only linear combinations that we can recreate using these vectors A and B are on this line so you can see that even if I have two different vectors I actually just got um single Vector because I have two Tre and both of these vectors they are actually um uh scaled multiplication of the other one so B is equal to I'm missing here something 1 / 3 so B is simply equal to 3 * a and then a is equal to 1 / to 3 * B so in both cases they are simply a version of scaled multiplication of this Vector Q3 so a is simply equal to B * 13 and then B is equal to 3 * a and both of them they are actually based on this Vector 2 Tre on this Vector a so therefore they both actually form and they span or round this single line and they are both linear we also call it collinear and they are linearly dependent okay so let's now move on to the next uh example where we will have bit more interesting case and we will look into this example when we have linear Independence look into another example bit more interesting one as we want to see whether those two are linearly independent or not so the first Vector that we got is the vector a the vector a is equal to 6 and Z so it is this Vector this is Vector a the vector B it is this one and it contains element of Z 0 and 7 so it is this Vector this is the vector B now in our definition of linearly independent vectors we saw that the idea of linear Independence is that the two vectors can only be linear independent if we cannot rewrite one of them by using the other so this means that we cannot rewrite a in terms of B and we cannot rewrite B in terms of a so there is no way that we can scale the vector a to get Vector B and there is no way that we can scale Vector B with vect with some uh scaler in order to get the vector a so there is no way that we can create a linear combination of this one vector to get the other one and the other way around so let's see whether this is the case just from uh trial and error we have a vector a which contains elements 6 and zero for us to go from A to B that has elements from so we need to go from 6 to zero in this case and we need to go from 0 to 7 now we can automatically already see from the second element that there is no way that we can go from 0 to 7 you cannot find any scaler C that you can multiply with zero in order to get seven there is no way that you can do that because any number any real number that is a real number if you multiply it with zero it will never become seven and of course another thing that you can notice here also very quickly is the other way around right so here if you go from this zero to six there is no way you can go from this Z to six because there is no such C that you can take this zero and multiplying it with that so here our scaler and you get this equal to six this is just not possible so what we are seeing here is that there is no way that we can somehow change this vector so there is no way that we can scale them in such way so this is minus B so all the scales scaled version of this or all the um scaled multiplications of vector B they will always be on this line and then the same holds for a as well so all the scaled multiplications of a will be on this line so then one thing we can quickly see here is that given that those two are perpendicular this pen of A and B is the entire R2 so we can see that by using those two lines we can recreate any other line in this R2 and this is highly related to this idea of linear Independence and given that we cannot come up with a linear combination using the a vectors to recreate the other one in this case given that we cannot recreate a using B and we cannot recreate B using a so no linear combination that exist that we can use to recreate Bay using a and the other way around we are saying that Vector a and Vector B are are linearly independent let's now look into another example that will uh clarify this linear Independence concept so we have three different vectors and the first Vector is Vector a 1 0 0 Vector B uh 0 1 0 our second vector and the third Vector 0 0 1 you can notice that we are in R Tree and then the example goes on and it says that those three vectors are linearly independent and as an explanation we have that there is no way to add these vectors together with any scalar multiples to equal the zero Vector unless all scalers are zero now before even going on to next part it's actually very quickly um uh provable that those three vectors are linearly independent and you cannot create a linear combination of one using the remaining of the two let's look into this example in more detail so we have three vectors A1 A2 sorry B so we got a B and C which are 1 0 0 0 1 0 and 0 01 now you can quickly see that if we are in R3 and this is actually our unit Vector E1 this is our unit Vector E2 and this is our unit Vector E3 because in that positions we got our ones and the remaining they are all zero and this is actually very similar to the previous example because we can quickly see how we are we will not be able to recreate one vector using the other ones by even looking at the positions of the zeros so for us to recreate Vector a which is equal to 1 0 0 it means that we should be able to find a linear combination C1 C2 and then using these vectors this is the vector B 0 1 0 plus C2 * Vector C which is 0 0 1 so in here basically we are already seeing a problem because we have here here an element one and we somehow need to be able to find C1 and C2 in such way that 1 is equal to C1 * 0 + C2 time Z but we know that there is no C1 and C2 that we can find such this uh expression actually is true because C1 and C2 they should be real numbers and there are no real numbers that we can find to multiply with zero such that this will end up to one because this is always equal to zero and we basically get 1 is equal to Z which is not true and of course the same holds the other way around you can prove that B can never be um recreated by using the linear combination of a and c and also the C can never be recreated by using a linear combination of A and B therefore we are saying given that a can't be written as linear combination of B and C B can be written so the same only this time A and C and then C hunt B written as linear combination of A and B those vectors A B and C they are linearly independent and if stronger you can actually go ahead and prove that this Spen of these three vectors is the r Tre but that's outside of the scope of this example so we will just pass but I will leave that um to you to prove all right so now when we are done with that let's actually move on to the last module which is the dot product and its applications so uh the length of a vector and Dot product is a con cep that um we um are familiar from the high school so the length of a vector is deeply related to this do product idea the dotproduct of a vector v WID itself gives this a square of the length of V what basically um it means is that this dotproduct of vector v so this thing which means take the vector v and multiplying it with the with the other vector v is simply equal to the square of a length of B so this is way to express the length of the uh of the vector B and once we square that that is the dot product so that's basically this definition what is about so we know what this definition of the distance is and we Define it by this and then we take the square of that distance and there is our DOT product and we are going to see this IDE of dot product a lot especially when it comes to uh matrix multiplication Vector multiplications also in many applications of linear algebra you will see this idea of that product coming again uh and coming back to us so uh this is a concept that we really need to understand so in the two dimensional space let's say we have a vector B which is um consisting of the two elements X and Y then the dot product and the link are related by V by V this is the way we denote the dot product so we just simply use the dot and the name also makes sense because we are saying we are using the dot to perform dot product so we are multiplying to two we are creating the product of this Vector with itself and this is equal to x² + Y 2 which is equal to the uh um squared of the distance of this Vector now you might recall from the high school that we have learned this idea of distance so if we have x-axis Y axis then we basically use this uh x² + y sare to uh get the uh you know the formula for from our Circle and then uh we have the x square + y Square we take the square root of it and then this is our distance so once we take the square root of that square of that from this uh square root of x square + y Square then we are simply getting this two cancel out which is equal to x² + y^2 So This is highly related to this idea because we are again talking about distances and we are simply taking the distance we are squaring them up and then we are getting the dot product so this the double uh straight line this is just a notation that we use and we spoke about this also before this comes um from the pre-algebra and this um this is highly important related to this idea of pythagore theorem and how we compute the distances so for instance when we have this uh Square triangular so we have this um uh rectangle here and we have here the 90 uh uh great so here we have the right uh right um angle and here we have our C which is uh the side right in front of this uh 90° angle and here we have the A and the B and we say that the c² is equal to a sare + b sare and if I were to actually write this in terms of X and Y so if this side is X and this side is y and this is my Z let's say then z s would be equal to x² + y² and this is something that we can see here too and the two terms are highly related so the Z squ is equal to x² + Y 2 and this is simply equal to Z * Z right and this is something that we know from High School welcome to the module one of this new unit when we are going to talk about about matrices as well as linear systems so those are all fundamental concepts that you will see time and time again when applying linear algebra not only in mathematics techs but also in applied sciences like data science artificial intelligence when training different machine learning models and trying to see what is this mathematics behind machine learning models different optimization techniques when you want to solve different problems using linear algebra so in this first module as part of foundations of linear systems and matrices we're going to introduce this concept of linear systems and then we are going to talk about the general linear systems we are going to uh see this common labeling of the coefficients this idea of indices that refer to the rows and the columns we are going to see what is this differentiation between homogeneous and nonhomogeneous systems so without further Ado let's get started so uh the linear systems form the uh bedr of linear algebra modeling this array of problems thanks to this advancements in these linear systems and Sol in it in Computing we can now solve a large amount of problems in a very efficient and a fast way so uh the general linear systems can be represented by this uh set of M equations with n unknown in the previous unit when we were looking into this uh linear combination of vectors we saw this notation which was A1 and then we we had C1 multiplied or rather let me keep me uh let me keep the same notation so we had this linear combination of vectors so we had beta 1 and then we had A1 Plus beta 2 and then A2 and those are all vectors plus A3 so beta 3 * A3 dot dot dot and then beta m times a m this is the notation that we saw before and we said we want to come up we wanted to come up with the linear combination of these different vectors A1 A2 A3 up to a and then we use that in order to get a sense of whether we are dealing with linearly independent variables vectors or linearly dependent vectors and then we also commented on the span that these vectors take now when it comes to um the uh vectors and just in general linear systems we can represent what we had before now in terms of with a bigger system so in terms of M equations and with n unknowns so here what you can see here is that we have M different equations so we have beta B1 B2 up to BM so you can see it in here and then each of these equations it contains n unknowns so you can see that the unknowns stays the same so the unknowns are those X1 X2 up to xn so X1 X2 up to xn are the set of all n unknowns and then M equations that you can see in here are all these equations so a11 X1 + a12 X2 dot dot dot and then a1n and then xn is equal to B1 and here one thing that is really important to keep in mind is that the indexing is what we need to focus on so we need to keep this one in mind this a i j and this XI so this is something that we also spoke about when uh discussing the linear combination of vectors we slightly uh touched upon on this topic so let's now dive into this this indexing and how do we indexes a i j what are this A's what are this JS and here you can see that we have a11 and then a12 and then up to the a1n and this is in our equation one and then we have in our equation two A1 two let may actually write this with different color so in our equation two we got a 21 a 23 up to a2n and this A's that you see here those are just real numbers so a11 can be 1 A1 2 can be three A1 n can be 100 and then the same also holds for this B1 for this B2 and for this BM and all these values A's and B's they are just real numbers the only unknowns that we got here are those so the X1 X2 up to xn all right so what about the indexing now so we got a i j and as you can see in this case the first thing that we can see here it stays everywhere the same which is the one so we got here one we got here one and up to the point we got here one whereas the second Index this one it does change it grows gradually with one and it becomes it goes from 1 to two and up to n so you can see here that the first index first index or index I it goes from one it doesn't change it's just one so it is one one and one so here in all cases for this equation I is equal to 1 but another thing that you can notice here is that the index 2 unlike index I so the second index which is the J so you see here that the second index is referred as J this is a general way of defining the indexes so here J is equal to 1 2 dot dot dot and then n so basically the I doesn't change in the same row but the G changes and then of course we have slightly different in terms of I but then the same for J for our second equation so here I is equal to 2 and then J is again equal to one and then two dot dot dot and then n and then here up to for the last equation our I is equal to M and then our J is again equal to one till two dot dot dot so you might notice that I was looking at this from the row perspective so I was saying pair equation or pair Row the I doesn't change but then the J stays the same and then it is either one two up to n but the set is the same so it is it contains all these different elements here so one one two and then n but it contains all these different real numbers going from one till n because we are combining and we are creating this combination the sum of all these values a11 and then X1 a12 X2 A1 n xn and another thing that you can also notice here is that here with the second index so with this J J is equal to one then here the X's corresponding index is also one when the J is equal to two then the ex's corresponding index is also two and then here the same story and you will notice that while the coefficient contains two indices 1 one one 2 or 1 n which are the two indices for the coefficients for the unknowns we got but just single index which goes from one till n so basically 4 a for the coefficients so I let me write with the right color so I can be one 2 all the way to M whereas in case of J it can be one to all the way to n and the indices are basically used to help us to keep track of in which row we are and what is the um variable that the coefficient belongs to because knowing this second Index this helps us to understand that we are dealing with a coefficient that corresponds to this first unknown the first variable X1 and then the same holds in here as you can see in here and in here we are dealing with the same variable X1 therefore the second index the index J is then the same both in the first equation and in the second one in both cases it's equal to one okay so now when we are clear on that let's also understand this high level concept because you will see this system of linear systems this m equations and N unknowns appearing a lot not only in terms of calculating and finding the solution to this linear system but this actually has a very common application when it comes to um running regression linear regression specifically and one thing that you can notice here is that here we got also this B1 B2 up to BM and you will notice that here the index also uh goes from one but then this time to M so when it comes to the rows we have M rows or M equations therefore we also expect when it comes to Counting from the top that at the bottom we will see an M whereas if we count from this side so kind of like imagine it like a column then we see that it goes from one till n so those are common observations and reference to um number of observations and number of uh features that you will see in your data when dealing with data analysis or modeling data so just this uh just keep those things in mind this uh abbrevation of M and then n m equations and unknowns because this will become very handy and the same also holds for this indexing just to keep in mind that this I and this J what those indices are and how for instance the first you know the I the first index changes when we go from up to the bottom and how the second index J goes and changes when we go from left to the right when we go through the columns but we are going to it is also in the uh upcoming slides so uh we can we will have time to practice it so um this is what we are calling a coefficient labeling the coefficient uh a i j so this thing in a linear system they are labeled where the first index represents the row and the second index denotes the column so when we see a i j we know that this refers to the row and the J refers to the column so this is something that we use in order to understand where exactly in our metric something that we can we will see very soon where exactly our unit or our uh member that is part of our Matrix where exactly is that located in which row and in which column the systematic labeling is super important because this helps us to keep the structure and this helps us to understand uh what does this uh coefficient represent what what is this row that it belongs and what is the column it belongs so for which equation and for which unknown we have already solved the problem such that we can know what this uh coefficient represents so before moving on onto the actual linear systems and the definition of metrices let's quickly understand this distinction between homogeneous and non-homogeneous because this will help us to also get an understanding how we can solve a system of linear systems so a system is homogeneous if all the constant terms b i are zero otherwise it's non homogeneous so identifying this helps us to really understand the nature of the solution set that we need to get and to understand what kind of strategy we need to use in order to solve this problem now what do I mean by bi we is so that we had this system of M equations with n unknowns and we saw that that we have in the right hand side this B1 B2 up to BM which means that we had this m different equations with n different unknowns and to find a solution to the system it means finding this value values corresponding to X1 X1 here X2 X2 xn so basically finding the set of X1 X2 up to xn that solves this problem and for us to know how to solve this problem we need to know whether this B1 is equal to zero or not this B2 is equal to zero or not and then this BM is equal to zero or not this is very similar to this idea of solving any sorts of um problems that contain unknowns for instance if we have three x is equal to let's say five solving this is entirely different than if we know that the tree exal to Z so this is a simplified version of course but the IDE is the same knowing that this B1 B2 up to BM this R zero this gives us an idea how we can solve this problem and later on we will see this distinction between non-homogeneous and homogeneous system and whenever these BS so whenever this B1 B2 up to BM whenever these BS are zero then we are saying that the system is homogeneous and we need to solve a homogeneous system otherwi wise we are dealing with nonhomogeneous system so this means that the bis are not all zero let's now move on to the second module which is about the matrices so we are going to define the Matrix we are going to see the definition of it as well as the notation this idea of rows columns Dimensions uh some of which we have already touched upon but we are going to uh go into the depth of it we are going to learn properly as well as we are going to see many examples then we are going to talk about Matrix types so here we will talk about identity Matrix diagonal matrices and also special type of matrices like matrices containing only zeros and only ones so by definition add a matrix is a rectangular array of real numbers that are arranged in rows and in columns for example an M byn Matrix a can be represented as follows so let's look into this definition and this reference to Matrix we call this Matrix or Matrix a and every Matrix it can be described by this rows and columns where we always have this uh way of describing this Matrix always should be defined by the number of rows and number of columns so this is super important and let's look into this specific Matrix so we have a matrix a and all these values they are members of this Matrix they form the Matrix and we already saw this labeling of a i j where we said that I is referred to the row so you might recall that those were all these equations that we got so this horizontal lines where I was equal to 1 I I was equal to two I was equal to three up to the point of I was equal to M and then we had this J so this thing and then J was referred to the columns and we had J was here one and then two and then three up to the point of n so one 2 3 and N this is exactly what you can see here so in this Matrix we got all these elements a11 is a number a12 is a number up to the a1n is a number those are all real numbers and one thing that you can notice here is that here we got a11 so this is our first row and First Column here we got A1 two this is our first row and second column and then we got up to the point of a1n actually let me just write this down even at a bigger scale such that I can make more noes so let's assume we have this Matrix a and this Matrix a a if I'm bigger and we got all these different elements so we start with our first row and here we have A1 1 so here the row that I will write with let's say with blue the r is equal to 1 and then the column is one so this is Row one this is Row one row one and this is column one let me write it with red this is column one this is column two this is column three dot dot dot and this is column n and this is row two this is Row three dot dot dot and this is row M so in total I got M rows and N columns I will come to this notation that I'm putting here later for now let's keep track of the rows and the columns to get a good understanding what this indices were about that we just learned so every time I will also mention this reference to a i j to keep track of this and also let me write it with the right colors so a i this is the row and J which is the column so all the elements I'm just defining by this a because it just a way to reference a part that comes from a matrix it's a just common way to write the higher matrix by capital letter A whereas its members we will write with the um with the lower case a so this is Matrix Matrix a all right so here in the second row But First Column we got a two and then one because it is still in the First Column and then when it comes to this element we have here a the row is the first one because we are in the first row but then we are in the second column so this one should be two then we go on to the next element in our first row so a one and then three and then dot dot dot the last element is an a as we are still in the first row it will be one the I but then given we are in the last column the column index or the J will be equal to n because we got in total n columns so we are now ready to go into the second row so here given that we already have our first element a21 this is in our second row and the First Column so the I is equal to here two and G is equal to 1 let's now write down the element in the second draw second column as you might have already guessed I is equal to here 1 I is equal to here two s and then uh the J is equal to 2 and then we go on to the next element which is in the second row and the third column so it's a the row index is 2 so I is equal to 2 and then the column index is three dot dot dot and then we got a as we are in the second row it is the I is equal to two and as we are in the last column the J is equal to n now you might have already guessed when I was writing this down that whenever you are in the row and you move on to all the elements in the same Row the I so the row index it stays the same only you need to uh update the column index so here for instance you got one one one here also one so all the way down in the same row or one which logically makes sense because we are in the same row so the row index should not change but instead you should change the column index like here column one column two column three all the way to column n so those are our columns dot dot dot so let me make this distinction and those are our rows as you can see so this kind of mentally helps us to understand why we are writing all these indices over time once you practice more with this this will become more natural very quickly remove this so now our ride rest very quickly so as you might have already guessed we are in the third row so we have a tree so everywhere I will just write down the ace so first write down the A's and then the rows the row index will stay the same as I in the same row but then I will increase the columns gradually so we are in the column one and the column two column three up to the column n so now the remaining stuff you can actually write down yourself to just practice let's now move on on to the last row and last column so in the last row we got a a a up to here and in the last show the uh row index is M which means that here I need to have M M M everywhere I need to have M and then the column index is 1 2 3 all the way to n so this last column is very interesting too you can see here that we have the opposite of what we have here because in the last column we see that the uh column index is the same so it is everywhere n Only the first index the index of the row it changes it goes from 1 2 3 up to M which is of course logical because we said that in the last column if we are looking it from the perspective of column so all these values this A's so the all the ends they are logical because they we are in the last column we are in the same column but then the row changes here we are in the row one here we are in the row two Row three of two row M therefore we have also at the end a m n now let's talk about this idea of MN we said that our Matrix a has M as a number of rows and n as a number of columns which you can see by the way also here so we always refer the dimension of a matrix so the dimension dimension of Matrix a by these two numbers so first we always write down the number of rows in this case M then as the second element we are writing the number of columns in this case n we are always putting this small X in between two kind of emphasize M by n Matrix and we most of the time use the square braces to Showcase that we are dealing with Dimension and in this case we are saying the dimension of Matrix a is equal to M byn so we are dealing with M byn Matrix this is a common convention used in linear algebra in mathematics General but also used in data science uh in machine learning artificial intelligence so whenever you are dealing with matrices a it is a common convention to talk about this idea of dimensions and the idea of Dimensions is super important when it comes to the idea of multiplication multiplying Vector with Matrix Matrix with Matrix so this dot product Dimensions play a central role in here so keep this one in mind once we uh get to the point of that products this one will become very handy so let's now look into a specific example where we see simple Matrix a so in this case you can see that we are dealing with a matrix that has a 2x3 Dimensions so like we just learned 2x3 means that we got two rows and three columns that's something that you can also see here very quickly so you have a small Matrix on the small matrix it's really easy to actually count so you can see that we got Row one and row two and we got column 1 column two and column three so this basically confirms this Dimensions therefore we are also saying that we have a 2 by three Matrix and like usual we first write down the number of rows and then the number of columns you can see here that here we have this elements for our Matrix so a is equal to 1 2 3 for the first row and then uh 4 5 6 for the second row so from this actually I think it's a good exercise to just uh very our understanding of indices and from this um we can write down that for instance all these different elements uh like a 1 1 is equal to 1 A1 2 which means that we are in the first row and in the second column so we have this element is equal to two and then we got a and then one Tre so we are in the third column so this one 1 is equal to 3 and then a 21 is equal to 4 a 22 is equal to 5 and then a 23 is equal to 6 so this is actually a good way to practice our understanding of indices our understanding of this Matrix structure and the understanding of dimension of the Matrix which in this case is 2x3 so this is yet another different definition of a matrix structure when it comes to the rows coms and dimensions so this is exactly what we just spoke about on our example and let's just quickly look at the formal definition so the rows of a matrix are the horizontal lines of the of the entries while the comms are the vertical lines so basically it's saying those are let me remove this so the rows are are the horizontal line and the columns are those vertical lines those are the columns this helps us to form these columns so column one column two and column three whereas this horizontal lines it helps us to create the rows so Row one and row two so then we have the dimensions of Matrix are given by the number of rows and columns it has so an M by n Matrix has M rows and N columns that's something that that we already saw so let's look into some special type of matrices one Matrix type is the identity Matrix so we saw before we had this Identity or unit Vector now we have identity Matrix so the two are quite similar so like before when we had our unit vectors we had this for instance E1 in three dimension we had 1 0 0 then we had our E2 which had 0 1 0 and then we had our E3 which was 0 01 so you might recall this about our identity vectors or we were calling it unit factors you might notice very quickly that we have formed an identity Matrix i n which is a square Matrix with one on the diagonal and zeros elsewhere is basically a matrix that is built using those unit vectors so here we have E1 here we have E2 and here we have E3 so you can also see that this 3x3 Matrix because we got three rows and three columns so you can see that here we have on the diagonal so we call this diagonal on this diagonal we have all ones and in here outside of the diagonal they are all zeros and this is the definition of identity Matrix it is this i n Matrix where n is the dimension of a matrix and given that it's a square Matrix it means that the dimension of it is n by n so all the rows so the number of rows is equal to the number of columns on the diagonal we have all these ones and every where else we got zeros and do note that we are forming this identity Matrix simply by combining these different uh unit vectors so like here E1 E2 and E3 so let me actually uh give you yet another example but of much higher Dimension so of this identity Matrix so let's say we have I and then this I uh let us actually use this notation i n so let's say we got i n what this means is that we got actually this large matrix it's a square Matrix which means that it is n by n so it has n as the number of rows and n as number of columns so the dimension is n by n you got n as number of columns too because it's a square and let us actually write down that how that Matrix looks like it's a large Matrix the N is the size of that Matrix so here we got on the diagonal we got one here we got one here we got one dot dot dot up to the last point one and the index of this one here so this is the first row this the First Column basically and everything else is simply zero so here we got z0 0 dot dot dot zero here we got 0 0 all the way down to zero here also zero all the way down to Z and then here also zero so everywhere here and here we all got zeros only on this diagonal we actually got once so basically by using our common notation we can say that in the D in the identity Matrix we got a 1 1 = to a 22 = to a 33 equal to all the way to a NN equal to 1 and then when it comes down to the rest of the cases so all the other observations let's say a 21 a 31 or a 41 anything so anything that is not um a11 or a22 anything that is not on the diagonal it is simply equal to zero we also say in those cases that a i j is equal to 1 if I is equal to J because then it means that we are talking about item that is on diagonal because both the row index is equal to the column index otherwise the a i j is equal to Zer if I is not equal to J so this is in the nutshell how a large identity Matrix in general can be defined so let's now move on to another type of Matrix which is the diagonal matrix so by definition a diagonal matrix is a matrix where all of diagonal elements are zero so what does this mean we is saw um example of a diagonal matrix which was our identity Matrix because identity Matrix is an example of a diagonal matrix and what do I mean by that in our just seen example we saw that only on the diagonal we had all these nonzero elements but the rest were all zeros so all the off diagonal elements were zeros like in here and in here exactly the same holdes for the diagonal matrices only unlike in the identity Matrix we no longer need to have this diagonal elements equal to one those can be any other numbers so as long as we have this um elements D1 D2 D3 that are not zeros but then of the diagonal numbers so all these elements they are zero then we are dealing with the diagonal matrix so in this case we got a 3X3 diagonal matrix because we have uh three rows and three columns and here we can see that the um the first so the a11 the first element from the first draw and First Column is equal to D1 so a 22 is equal to D2 and then a33 is equal to D3 so D1 D2 and D3 those are all so D1 D2 and D3 those are all real numbers now when it comes to the uh this numbers for example it can be that D is let's say 2 five 6 on diagonal then we have those zeros this is a diagonal matrix it can also be that D is equal to minus 3 and then 0 0 and then 5 8 and then here we have zeros so again we have on the diagonal all these elements and the off diagonal elements so if all the off diagonal elements are zero then we are dealing with diagonal matrix and if you wondering well what happens if on the diagonal we got zero do we still have a diagonal matrix it's actually a great question but yes indeed we are dealing with the diagonal matrix as long as all the off diagonal elements are zero so for instance if we got D is equal here we have zero here we have 0 0 0 and then 7 and then 0o and then 8 and then 0 0 so we got this of diagonal elements so here are the diagonal elements and all the of diagonal elements are those given that all the of diagonal elements are zero which is the definition of the diagonal matrix then we can say that our D Matrix in here is indeed a diagonal matrix let's now look into yet another type of Matrix which is a special type of Matrix and it's called one's Matrix so by definition one's Matrix is denoted by 1 M1 so you can see here and here it mens the dimension of it so the number of rows and number of columns and it's a matrix in which all the elements are one so this is a very unique Matrix we often use it during the programming so in data science data analytics but also in um uh when creating like data structures when designing algorithms this becomes very very handy and this idea of one's Matrix is that all the elements are just one it means that if we want to create a placeholder in such way that we can then multiply any number in here with some other number and get that number then it can be done very easily because we know that when we multiply a number with one then we get that number so a * 1 is = to a x * 1 is = to X now this is a exactly this property exactly is what motivates us to create and to have this type of ones matrices it means that we are defining matrix by its Dimension so it is M by n and here the m is equal to two and then n is equal to three because we got two rows and three columns but you can see that all the elements are the same and they are equal to one so a11 is equal to A1 2 is equal to a13 is equal to a uh 21 and is equal to a 22 and a 23 and they are all equal to one and this is the definition of one's Matrix you can have um on Matrix of the size 4 by 10 On's Matrix of the size th let say 10,000 by 100 Etc so any number any real number so M and then n are real numbers you can use in order to create this large M by n1's matrix let's now look into our final special type of Matrix before moving on onto the next module which is about zero matrices so similar to this one Matrix a zero Matrix denoted by 0 m by N is a matrix in which all the elements are the same with the one difference that this time all the elements are equal to zero so in the on Matrix all the elements were ones but in the zero Matrix all the elements are zero this type of matrices become very handy also during the programming creating um different algorithms during design encoding um but for slightly different purposes usually we create the zero matrices as a placeholder such that in the beginning we can have this uh tups or we can have this um uh arrays or nested Loops um that we want to perform and then gradually add these values to the existing Mt array so if we create this Zer Matrix and um this is a placeholder then next time we can always add on this this new data that we get and then we know that zero plus a number is always equal to number which means that once we have this updated information of a we can add this to the zero and we will then have this new updated information in our system therefore the zero Matrix is often used as a way to uh have this placeholder with the provided Dimension where we can always add new information and the information can be updated so in this specific case we got um a zero Matrix that has two rows and three columns so you can see two rows and three columns so m is equal to 2 and then n is equal to three three perfect so we are done with module 2 and now we are ready to go on to our next module which is the core Matrix operations so when it comes to matrices we often perform Matrix additions Matrix subtraction but also Matrix um scalar multiplication of this Matrix so multiplying Matrix with a scaler and then Matrix um multiplication just in general so taking two matrices and multiplying them we are going to look into this concept in detail we are going to see many examples like before we are going to dive deeper into this such that we lay the ground on uh to the next module which is solving a system of M equations with an unknown so solving this General um linear system so for the beginning uh we will be looking into this Matrix operations where we are adding or subtracting matrices so by definition the sum of two matrices A and B of the same dimensions is obtained by adding their corresponding elements so by taking the element i j from both matrices and adding them to each other so in this case you can see that Matrix A and B are here and uh the uh definition says we just simply need to take the corresponding elements corresponding elements from the row I and the column J take them add them and this will become an element in our final um Matrix because when we are adding two matrices of the same size the result is yet another Matrix so we will use the Matrix a to add to Matrix B and this will give us a matrix A + B and this i j simply refers to the indices corresponding to the row and the column we will look into an example in a bit and this will make much more sense and the same holds also for the difference so by definition the difference of the two matrices A and B of the same dimensions is obtained by subtracting their corresponding Elements which means that in order to obtain this Matrix a minus B this is a new Matrix we simply need to look for each element so we are going to index them for a row I and J we are going to do this pairwise element wise subtractions we are going to see what is that element corresponding to the row I and column G in The Matrix a which we say is a i j we are going to subtract from this the element in the row I and column G that comes from Matrix B and this will give us our new Matrix which is a minus B so let's now look into an example in this Matrix Matrix um uh a and Matrix B are used and Matrix a is of the size 3x3 Matrix 3 Matrix B is of the size 3x 3 in order to obtain a plus b what we are doing is that we are performing element wise additions now let's verify this so what we are doing here is that we are saying a plus b let me actually get a larger area here so let's say we have the two matrices I want to add the two in such way that we do everything one by one such that this idea of a plus b and addition of the matrices will make sense so we want to find out a plus b for that what we are going to do is that we are going to make use of this definition that A + B and then I J is equal to a i j+ b i j which is a fancy way or mathematical way or describing that for each element we need to go and look for the row I and column J and take that element from the um column from that uh Matrix a and from the Matrix B so this means that for a + b this is going to be a matrix that will have the same number of rows and the same number of columns as two matrices because both A and B are 3x3 which means also their sum is going to be 3x3 so this going to be 3x3 and here we are going to do so we are going to take for the first row in the First Column so for A+ b 1 1 so first row and First Column we need to go to the first row and First Column of Matrix a and the first row and First Column of Matrix B and we need to add these two elements so we need to do 1 + 1 and then we need to go on to the second column so the first row and the second column which means that we need to be here in both matrices so here we have 0 + 2 and then we got 2 + 3 and then we got 0 + 0 so you can see it in here and then we have 1 + 0 and then we have 3 + 1 0 + 1 and then 0 + 2 and then 1 + three which gives us so 1 + 1 is = to 2 0 + 2 is = 2 and then 2 + 3 is = to 5 0 + 0 is equal to 0 0 + 1 is = to 1 1 + 0 is = to 1 0 + 2 is = to 2 and then 3 + 1 is = 4 1 + 3 is = to 4 which means that our A + B is equal to this Matrix that we got in here so you can see that we are getting exactly what we uh what we have here only we have done it manually one by one so the same idea holds exactly when we have a minus B only instead of adding you will have to do here minuses so minus minus so everywhere minus so 1 - 1 0 - 2 2 - 3 Etc so let's look into another addition so in this case by definition it is defined as this element wise uh of the adding of these two matrices here the only difference in this definition is that it's saying it's calling this a plus b as C so this new Matrix that we are getting as a result of adding a to B it's calling C so basically it's the same as calling this Matrix as C you will see also this type of definitions so in this case The Matrix C is equal to a plus b which basically means that for each row with index I and with each column with index J go and look for row I and index J take the corresponding elements from Matrix a and Matrix B add them in order to get that corresponding element in our new Matrix C and you can see that in this example that's exactly what we are doing we have a we have B we are taking this element and this one so 1 + 1 we are getting here two and then 0 + 2 we are getting two here 2 + 3 is 5 and then 0 + 0 is = 0 1 + 0 is = to 1 and then 3 + 1 is equal to 4 so now we already go to the next topic which is about scalar multiplication of a matrix so by definition scalar multiplication of a matrix a by scalar Alpha results in new Matrix where each entry of a is multiplied by Alpha the idea of scalar multiplication matrices is actually quite similar to this idea of scaled multiplication in vectors so uh we have already seen in the lecture of the vector multiplication that when we were having this scaler C and we had this Vector a then uh when we are multiplying C which is a real number with Vector a then we simply need to take all the elements of vector a so A1 A2 all the way down to a n and we need to multiply them by this same scaler so see this is what we were doing with vectors and that's exactly the idea behind matrices and when uh doing the scalar multiplication of matrices only instead of multiplying only just one vector with this scaler C now we need to apply this to all the rows and all the columns so here we got this one column and Matrix is simply a combination of multiple vectors which means that we need to multiply all these elements of all the vectors of all the columns in this Matrix so let's actually look into a specific example so in this case we have a matrix a and this Matrix a is this thing and we have a scaler which is three so in here our Alpha is equal to three or you can call it C or anything so you can see that when we are scaling The Matrix with a scaler in this case Tre with this Matrix what we are doing is that we are simply taking each of these elements and multiplying it with this scum so 1 by 3 is 3 2x 3 is 6 3x 3 is 9 and 4x 3 is 12 this is the idea behind this entire scal multiplication ofation Matrix in more general terms if we for instance have a matrix a so let's actually look into a high level General example where we have a DA Matrix M by n so we got M rows and N columns and we want to get a scal multiplication of this Matrix and um scaler that we have here as in our definition it is defined by this alpha alpha is just a number you can qu C you can qu B anything so in this case our scaler alpha alpha is coming from R so it's a real number so Alpha time a is then simply equal to to this new Matrix where all of these elements are simply multiplied by this scal so I will just take over all these values H1 up to a M1 and then A1 2 a22 all the way down to a M2 and then let me also add the last column just for fun here a 2 N and then here a m n so here this new scaled M multiplies so so scaled uh Matrix a so Alpha * a is simply equal to Alpha time all these elements are simply multiplied by the scale it is as simple as that so that's the simple idea behind um Matrix as scaling so when you are doing scalar multiplication of this Matrix you simp take all the values and you multiply them element by element per row and per column by that single scalar Alpha do note that you are multiplying them all without exclusion with exactly the same number which is that Alpha so let's now look into the definition of matrix multiplication so here we are no longer multiplying a matrix with a scalar but we are multiplying Matrix with Matrix so the product of an M by n Matrix a and an N by P Matrix B results in an M by P Matrix C where each entry cig is computed as the dotproduct of the e Road of a and the J column of B now what does this mean firstly let's look and unpack this part of the definition so we got Matrix a that is M by n and then we got Matrix B which is n by P what this means is that in this case Matrix a has M rows and N columns and Matrix B has n rows and P cups so this is then simply the dimension dimension of the two matrices so then it's saying that by definition the product of these two matrices so the product of A and B the product of the two B is equal to to this Matrix C and each entry cig J so c i j is computed as the dotproduct of the each row of a and the Jade column of B now this part might seem bit difficult but once we look into the actual example and we illustrate this on our common high level General expressions of Matrix am and their multiplication this will make much more sense for now before coming to this one I just wanted to refresh our memory on one thing I said before when discussing also this idea of improving uh this uh different properties of vectors that when we want to multiply a vector with a matrix or Matrix with Matrix or vector with a vector we need to ensure that from the first element the number number of comms is equal to the number of rows of the second element this is also very important for this specific case and just in general for matrix multiplication so you can notice here that the number of coms here is equal to the number of rows in here and the order is very important so in case of matrix multiplication the order is really important which means that if you have a matrix a and you want to multiply with the Matrix B then the number of columns of a should be equal to the number of rows of B otherwise you cannot multiply those two matrices with each other so in case you got a matrix a that doesn't have the same number of columns as the rows of number of the Matrix B then there are some alternative things that you can do including this idea of the transpose that we saw also doing when Computing the dot product between this Vector a and Vector B that's something that we also do in programming when we are dealing with this Matrix and we want to compute this relationship between two matrices but the number of columns of one of the first one is not equal to the number of rows of the second one we are simply uh manipul ating this matrices or removing some data if that's not hurting our problem maybe uh flipping so transposing our Matrix or applying any other source of operation to it to ensure that the two matrices that we are multiplying with each other the first one's number of columns is equal to the second one's number of rows that's just the low and that's something that you should follow if you want to multiply these two matrices all right so now let's move on onto this idea of multiplying and Dot product let's look into a specific example and this will uh help us to understand this process better so before doing that I just want to quickly show you this general idea so if we have a matrix a that is M by n which means that it looks something like this like A1 1 a 2 one up to the point of a M1 and then here we got let's say a 1 2 a 22 up to the point of a M2 and then at the end we got a MN and here we got A1 n so let me also add this one 2 N and we got a matrix B this Matrix B is n by P so it has n rows and P columns so we are fine in terms of Dimension here and we got here b11 B21 up to the point of b m sorry b n in this case let's not confuse the letters so b n 1 B1 2 B 22 up to the point of b n 2 because n now is the number of rows for Matrix B unlike for the Matrix a up to B1 p and here b 2 p and here after the point of B and then n p this is the last element in order to perform um multiplication between these two matrices so to obtain a matrix C which is a equal to a * B what we need to do is we simply need to take pair case or pair Row for the row I for instance we need to take this element so this row and we need to multiply it with this so we need to find the dot product between this row and this column then we need to move on on to the next one and then for the second element we will then take this row and we will multiply it with this one so this is then something that we need to do in order to obtain these elements and you might have already noticed that we got this m by n and n by P so you might have already guessed what will be the dimension of the C if we got that the dimension of a is equal to M by n and the dimension of B is equal 2 N by P then the results Matrix after M multiplying the two so Matrix c will be will be having a number of rows equal to this and the number of columns equal to this so This middle part basically disappears and the number of rows of the first Matrix will be then the number of rows of this result Matrix C and the number of columns or the second Matrix so Matrix B will then be our final number of columns so we will then have a matrix C that will have a dimension so Dimension so dimension of C will then be equal to M by P so we will have M rows and P columns so how we are going to compute this so for c i j which means row I and column J let's look into the definition of it it's saying c i j is computed as a dotproduct of the each row and the Jade column so each row from a and Jade column of B what where is the each Road of a the each Road of a is somewhere here so each Road of a it is uh the A and then I one then a and then I 2 and then a and then I Tre dot dot dot and then a i and then then we got in total n columns n and we always do the transpose right when Computing this um dot product so we then take the transpose so we take this row row I and we multiply it so we do the dot product between this one this is the a i and the B J this is column J it is somewhere here so it is B and then we got the first element which is one and then J and then b 2 J B 3j dot dot dot up to B and then in total we got n rows in B so n and then the J is the column so it stays the same so this is then the dot product between e row that comes from Matrix a and the J column that comes from Matrix B so it's always like that actually so we always take row by row so we take this different so every time we take just a row and we multiply with the corresponding column and then we get the dot product between this row that comes from the first Matrix and then the column that comes from the second Matrix in that specific order in order to get our DOT product and that specific valum and what is this amount actually so when we calculate this do product you can quickly see that we have a i1 multiplied by B 1 J plus a I2 multiplied by b 2 J and then dot dot dot a i n multiplied by b n g and this new Matrix c will then have all these elements so C11 C 21 and then c31 dot dot dot and then C the last row as the number of rows of C is m c m see here M so C and then here it will be one 2 C 22 C3 and then 2 up to the point of cm and then two and then here the last col will be C1 and then p is the number of coms in C so C1 p and then C2 p and then c3p dot dot dot and then c m and then P okay so this is what we get this is our final Matrix C when multiplying Matrix a and Matrix B so let me clean this up C is to now if you want to find out what is C11 you can easily fill in this general formula that uh that we just calculated the I is equal to 1 and then J is equal to 1 and this will give you C11 by using this formula if you want to get the C and P then just fill in the I is equal to M and then J is equal to P in order to get this value C and P so you can already see the amount of calculations you need to do in order to get all these elements from this l large matrices A and B let's actually look into a simple example to clarify this so we have a matrix a here and Matrix B here and we want to do a multiplication of the two and we have just learned how to do it let's actually do it one by one so we got a matrix a which is equal to 1 2 3 4 with Dimensions 2 by 2 then we got a matrix B which has values two Z and then one two so it is 2 by two and I want to find what is c that is equal to a * B and I know already by looking at these Dimensions that c is going to be equal to 2 by 2 so you might recall that I said that when looking at this final result the number of rows or the final um Matrix will be this so the number of rows of the initial Matrix a and then the number of columns of this final Vector c will be the number of vectors number of columns of this second Matrix B so two therefore I know already before even doing calculations that the uh product Matrix c equal to a * B is going to have a dimension 2x two let's actually do a calculation to check this so C is then equal to a * B and it's equal to 1 2 3 4 4 multiplied by 2 0 1 2 okay so I expect to have four different elements here here here and here so to obtain the C11 so it is C11 in here what I need to do is that I need to look at the first row and in the first column in here so first row from a and the First Column of B and I'm doing the dot product which means 1 * 2 + 2 * 1 1 * 2 is 2 2 * 1 is 1 so here I'm getting 1 * 2 + 2 * 1 which basically gives me 2 + 2 and that's equal to 4 so here I'm just writing [Music] down 1 * 2 + 2 * 1 now when I want to get this value which is C12 this means that I want to get the first row and the second column and that's exactly what I'm doing so I'm going back and I'm saying let's look at the first row but this time will look at the second column coming from the uh from The Matrix B so 1 * 0 0 + 2 * 2 and then I do the same only this time for the second row which means I'm picking this row and then this column so it is three * 2 + 4 * 1 and for the final element c22 I'm taking the second row and the second column which gives me three * 0 plus 4 * 2 now what does this gives me this gives me this 4x4 Matrix where 1 * 2 + 2 * 1 is 4 1 * 0 + 2 * 2 is 4 3 * 2 + 4 is = to 6 + 4 which is 10 and then 3 * 0 + 4 * 2 is = to 8 so let's check 4 4108 that's exactly what we have here so as you could see here the idea is that every time to follow what element I'm looking for for the CI J and then I just go to the E rows from the first Matrix and the J column from the second Matrix and I do the dot product of the A and then I and then K let's say so I'm going to the E Row from the first Matrix and I'm taking all the elements which means I don't even need to mention this index it just means the entire each row coming from the Matrix a and then I'm doing the dot product between this row and the column that comes from the Matrix B which means B and then J which then will give me the cig so I'm looking at this and taking this multiplying this dot product and this gives me the first element then the first row and then the second column which gives me the uh second element in the first row in my Matrix so this one and so on so hope this makes sense uh if it doesn't make sure to reach out because it's a very important concept and uh let's also look into another example to make sure that we got this right so in this case as you can see we have another matrices so set of A and B matrices again 2 by two a simple one and we want to know what is a so let's say we call this C we already know C should be 2 by 2 and what we are doing is basically for C11 we are saying let's look at the first row so first row and the First Column coming from the second Matrix B and let's do the dot product so 2 * 1 2 * 1 2 * 1 4 * 5 4 * 5 we get this and then when we want to find what is C oh what is C and then one two so in the first row but in the second element in our final Matrix so I is equal to one and J is equal to 2 it means we need to look at the first row from The Matrix a but this time the second column from The Matrix B so it is 2 by 3 2 by 3 4 * 7 4 * 7 and this gives us a number 13 four even if you calculate you can see that 2 * 1 is equal to 2 4 * 5 is 5 so 2 4 * 5 is 20 so 2 + 20 is 22 in here and then you can do the rest of calculations and this will be a good practice to see how we can do a basic matrix multiplication the idea is actually quite straightforward when it comes to multiplying it it just it comes with a practice when we see all these uh much bigger matrices so um this is another example I will leave this one to you to complete it just uh to keep in mind we always do uh so we always look at the dimension first in here 2 * 2 and 2 * 2 which gives me an impression already what I can expect the result will be 2 by two and when it comes to the uh cross elements just ensure to always look to the E row and the J column this comes from Matrix a and this comes from Matrix B take them compute the dot product and then you will find your C uh your final result let's call it um kig because in this case we have a matrix C already welcome to the module 4 of this course when we are talking about matrices and linear systems so in this module we are going to dive deeper into this uh idea of linear systems with matrices and solve linear systems using different techniques and specifically we are going to learn the uh concept behind solving linear systems using matrices named gausian elimination and gaussian reduction welcome to the module one in this unit so in this uh case we are going to talk about algebraic lows for matrices we are going to discuss four different properties for matrices uh and the first one is the communative laow for Matrix addition the associative law for matrices the distributive laow for matrices both the left and the right one and then finally we're going to talk about the scalar multiplication laow for matrices so the algebraic lows or matrices they are like in case of real numbers like in case of vectors they help us to do different operations on these entities they are very similar to the real numbers and the vector cases where we for instance um so that if for instance A + B uh is equal to B+ C C or a * um b + C is equal to a b + a c those are all sorts of lows that we uh learn as part of high school prealgebra and we have applied it to real numbers we know how helpful those can be and similar type of lows we have also for the matrices and we got in this case four different laws that we will be discussing the first one is what we are referring as associative law the second one is the distributive low the SEC the third one is the scalar multiplication low and the fourth one is the communative low for addition so these laws help us to do different metrix operations they help us to manipulate algebraically this metrices and then uh this can help us to solve different sorts of problems including solving a system of linear equations so let's start with the commutative law for Matrix addition so the Matrix addition is cumulative um which means that A+ B is equal to B plus a so unlike the matrix multiplication that we have seen in the previous lessons uh where the order did matter and we said that we um had to uh ensure that the number of columns of the first Matrix is equal to the number of rows of the second Matrix in case of addition that's this is not the case so we should not care about the order whenever we want to add two matrices the other thing that we need to keep in mind though is that the two matrices needs to have the same size so I mean that both Matrix a and Matrix B need to have a dimension so dimension of Matrix a should be equal to dimension of Matrix B and let's say should be equal to M by n but for the rest we don't really need to care uh which one we will put first will we put first a and then add the b or we will do the other way around so we will then First Take B and then we will add a so this is the idea behind communative low for Matrix addition so first let's look into all this uh lows and then we will also look into the corresponding examples so for this specific case it might actually also be helpful to write down the general formula which will um make sense out of this um low for the uh uh which is a communative low for the Matrix additions so let's say we got a matrix a which is M by n and this Matrix can be represented as a11 dot dot dot a M1 so this is something that we saw time and time again so I'll just quickly write it down the common notation for this and then here we have the last column which is a MN and this is the Matrix a then we got Matrix B which is again M by n and can be represented as b11 and then dot dot dot and then B M1 dot dot dot b1n dot dot dot b MN so the communative lows says that A + B should be equal to B + a let's check that whether this is the case let's first compute this part and then we will do this one well the first one means that we get so A+ B is and we remember remember how we add matrices right so we know that we just need to pick their corresponding elements and add them to each other so we get a11 plus b11 then dot dot dot and then a M1 plus b M1 this is why also D is really important that they got um the same Dimension which means that they got exactly the same amount of elements the same uh amount of columns and the same amount of rows um in terms of the uh Matrix size so then here we have a 1 n and then plus B1 n then dot dot dot and then a M1 and then plus b m here I need to put n we are in the last element of the Matrix so BMN and that is it this is our Matrix A + B let's now look into the Matrix b + a so what that amount is so the Matrix b + a will then be equal to b11 plus a11 dot dot dot and then B M1 plus a M1 then dot dot dot and then b 1 n plus a 1 n then dot dot dot and then the last element will be B MN and then Plus a m n so in here if we remember from the real numbers we know that a + b is equal to B + a for instance if a is equal to 2 and then B is equal to 1 then a + b is = to 2 + 1 which is equal to 3 and then b + 1 is = to 1 + 2 and it's equal to 3 so we know that indeed for the real numbers a plus b is equal to B + a and making use of that property we can already state that b11 + a11 is equal to A1 1 + B1 1 and then the general case is that a i j plus b i j is equal to b i j plus a i j where I is the index of the rows and then J is the index of the columns from the coefficient labeling so using this property from the real numbers given that all these values in the m Matrix are real numbers we can quickly see that the Matrix B+ a that we just got in here is equal to this Matrix a plus b which means that 1 is equal to B and this proves that A + B is equal to B + a this is the communative property of the Matrix additions so the next law is the associative law for matrices which says that the in case of Matrix addition a + B+ C isal to A + B + C so basically this time we go from here to adding one more element which is the third Matrix Matrix C so we are saying A + B + C is equal to a + B+ C so it doesn't matter whether we will First Take The Matrix a and then B and then add them up and then we add Matrix C or if we first take the Matrix B and C add them up and then we add a to this sum it doesn't matter we will see an example of this in a bit and then the uh second part of this associative law for matrices says that for matrix multiplication a * B * C is equal to a * B * C so again in terms of the um order when it comes to this specific multiplication so it doesn't matter whether we will first multiply a by B and then by C or we will first multiply B by C and then we add the a at the end we will end up with the same amount so a * B and then * C is equal to B * C C and then in the left hand side we add the a so a * B * C so these properties help us to add or multiply matrices without really worrying about this idea of grouping of the terms so we can always group them and perform all sorts of operations so this is this first property that we see in here let's say we have this uh Matrix a matrix B and Matrix C so let's prove that in the order doesn't matter and this associative property holdes so let's prove that so for that the first thing we need to do is to compute a plus b so this part so A + B + C what is that first I need to compute this part and then I will add C which is the second part so A + B is equal to my a is equal to 1 2 3 4 plus and my B is equal to 5 6 7 8 this is then equal to so 1 + 5 is = to 6 2 + 6 is = 8 3 + 7 is = to 10 and then 4 + 8 is equal to 12 this is my A + B this is the first part now the second part is then to add to this A + B this C this I can by the way also call some Matrix D so I can say that this is equal to D+ C so let's find out what is this amount so a plus b or what we're referring as D is 6 8 10 12 we just calculated it in here I'm also adding now my Matrix C which is 91 10 12 so 9 10 11 12 what is this amount it is 6 + 9 is 15 8 + 10 is 18 10 + 11 is 21 12 + 12 is 24 this is my final Matrix so I have checked that D A + B + C is equal to 15 18 21 and 24 this is the first part let's now go ahead and check whether this is equal to the second part which is this part so this is one this is two so this is then A + B + C as you can see it in here let's now calculate that amount and like previously we will do it in an order so first we need to calculate this part and then the entire thing so B+ C is then equal to the B was 5 6 7 8 5 6 7 8 plus and the C was 9 10 11 12 9 10 11 12 what is the much 5 + 9 is 14 6 + 10 is 16 7 + 11 is 18 and 8 + 12 is 20 this is the first amount let's refer refer this as a d or we can even call it by some other letter let's say k this is Matrix K so B plus C is k then the second part is to take this B+ C so B+ C which we have referred as K say K and then we are adding to this the A and specifically just to ensure that we stay with the same order I'm saying I will add from the left side the a to this Matrix K and this obviously means this is equal to so A+ b + C this is what I'm referring by just uh in a more simpler note a I'm just using K in here so this is my B plus C or what I'm referring also as a k and this amount is equal to what is my a my a is 1 2 3 4 1 2 3 4 plus and what is B plus C we just calculated that that's the K so 14 16 18 20 T So 1 + 14 is 15 2 + 16 is 18 3 + 18 is 21 4 + 20 is 24 so we have learned that the A + B + C is this Vector now is this Vector equal to the a plus b and then plus C well here we got this 15 18 21 24 15 18 21 24 so we have just proved that the first part is equal to second part which means that we have proved that indeed the order doesn't matter and A + B + C is equal to a + B+ C so this calculation confirms that the both sides of this equations they are in indeed equal and this confirms the associative low for the Matrix addition so let's now look into the distributive law for matrices which says that Matrix addition and multiplication they satisfy the distributive property which means that if we have a left distribution a * b + C is equal to a + a c and then in the right distribution we basically have the Matrix multiplying from the right from hence the name right distribution A + B * C is equal to a C + BC you might very quickly see and recognize from here that we have very similar actually exactly uh the same rule only for real numbers we know that a * b + C is equal to and then we open the parentheses with say this is equal to this times this so AB plus this times this a c you can see that we have exactly the same here only in the capital letters so in the real numbers we have exactly the same low so the same we have also for our left distribution when it comes to Matrix additional multiplication and the same we have only with a different order here you can see the C so this one is basically uh with the different order instead of having the Matrix multiplied in the left here we have from the right and this is similar to the property that A + B * C is equal to C * a which is a c plus c * B which is BC an example uh where we will prove that the distributive law for matrices um is indeed true and I have skipped deliberately the uh example for this one because uh this a b * C is equal to a * b c so the associative law for matrix multiplication it's something that you can calculate for yourself using the same a b and c matrices only this includes multiplication of these two matrices and it's something that we are going to do as part of this example so instead of doing and redoing this multiplication I thought that it's great to leave that for you as a practice and instead focus on bit more complex problem like this one that one way or the other includes the same matrix multiplication so I need to calculate the a * B in this case which means that by providing you this example I'm also including what is needed to do the previous example only it would be a great way to practice the material for yourself so let's now move into proving the distributive low for matrices so we got this mat matrices a b and c and here I'm going to apply matrix multiplication the same as that is needed for the previous uh case and here what we need to prove is that a * b + C is equal to a * AC so this is the first part this is the second part so let's go and calculate them separately so for the first one we need to calculate a * b + C which is then something that we can calculate by first doing the addition so we will first do the addition of matrices B and C and then once we are done with that we will then do a * b + C so that's the second part so let's go ahead and do that calculation so first we got b + C what is B plus C B is 5 67 8 5 6 7 8 plus and the C is minus one 0 0 minus one so on the diagonal we got min-1 and minus one and then of diagonal lower and upper part we got zero and what is this Matrix this is equal to 5 - 1 so 5 + -1 is equal to 4 6 + 0 is = 6 7 + 0 is = 7 and then 8 - 1 is = 7 this is our B plus C which we can refer also as Matrix D so let's call this D which means that now we are interested in a * B so for this second part we need to take this Matrix a so a * D is then equals to we need to take the Matrix a which is 1 2 3 4 and we need to multiply it with this Matrix that we just got because this is the B plus C or the D that we were referring 46 77 okay so let me remove this part cuz we are going to need some space for this and let's do this calculation this is 2x two and this is 2x two I will do the calculations in here so we need to end up with the Matrix that is also 2 by 2 because we know 2x 2 Matrix times 2x two we will pick this part so the number of rows and the number of columns of the second one this will be our resulting Matrix which is 2 by two all right so for the matrix multiplication we know that for this element in the place of so one a or let's call this Matrix we don't even actually need to call this anything we we can keep it simple so let's say that we are in the first draw in the First Column so this is the first draw in the First Column for this what we need to do is we need to take the first row from the first Matrix so Matrix a and then the First Column of the Matrix D so this one and we need to do the dot product which means that we do basically 1 * 4 1 * 4 plus 2 * 7 so for this element which is in the second row and the First Column we need to take the second row and First Column in here so we end up with three times 4 so 3 * 4 and then 4 * 7 so Plus 4 * 7 The Dot product between this one and then this one so for this element which is in the first row and then the second column of the final Matrix so first row and second column we need to pick the first row and second column and do a DOT product which means one 1 * 6 + 2 * 7 2 * 7 and then in here in this element we got the second column and second row so second row second column which means that we need to pick the second row and the second column the dot product of the second row of Matrix a and the second column of Matrix D which is 3 * 6 Plus 4 * 7 4 * 7 so let's quickly calculate what this amount is so this is the a * B+ C basically and this Matrix is 1 + 4 is 4 2 * 7 is 14 4 + 14 is 18 1 * 6 X is 6 2 * 7 is 14 and 6 + 14 is 20 3 * 4 is 12 4 * 7 is 28 which means that we got here 40 3 * 6 is 18 4 * 7 is 28 which means we got here 36 and 46 this is our final a * b + C let's now go ahead and calculate the second part so the second part says that we got a * b + a * C so a * b + a * C which means that first we need to do this calculation and then this one and then we need to add them to each other so let's quickly then calculate what is a * B and then a * C and then add them to each other let me clean up some space in here we're going to KN that when we write this one in a smaller format so this is equal to 18 20 40 and 46 and let me take over the second element which we still need to calculate which is AB plus a c first we will do this and then this and then we will add them to each other so a * B is equal to 1 2 3 4 multiplied by 5 6 7 8 5 6 7 8 now following the same approach from the previous example when we calculate this Matrix I will then quickly calculate what is is a * B so in here we got first row and First Column so 1 * 5 + 2 * 7 so the dot product between the first row and the First Column from here now for this element here we got the second row and the First Column we need to take the second row in the First Column from here and we do the dot product which means 3 * 5 + 4 * 7 in here here we got the first draw and second column so the first draw and second column which means that we need to have 1 * 6 + 2 * 8 then here we got the second row and then second column which means 3 * 6 + 4 * 8 and then this is equal to 1 * 5 is 5 5 2 * 7 is 14 so this is 19 1 * 6 is 6 2 * 8 is 16 6 + 16 is 22 in here 3 * 5 is 15 4 * 7 is 28 so this is then 33 and then 43 so 43 and in here we got 3 * 6 6 is 18 4 * 8 is 32 so we end up with 50 so hope I haven't made any mistakes in the calculations so this is the a * B so a * B is then equal to 19 22 43 50 let's clean this pce and let's move ahead to the second part of the calculation which is a * C what is a * C well a * C is 1 2 3 4 1 2 3 3 4 multiplied by -1 0 0 -1 so here we are then getting -1 + 0 here we are getting -3 + 0 here we have -1 + 0 so no so the first row and second column which is 0us 2 and then in here we got the second row and the second column which is 0 - 4 which means that we end up with this Matrix and it's equal to -1 -3 then -2 and then -4 which means that we are getting as a final step AB plus a which means 19 202 43 and then 50 then plus -1 - 2 - 3 - 4 and what is this 19 - 1 is 18 43 - 3 is 40 22 - 2 is 20 50 - 4 is 46 okay so we got that this amount ab+ a c is equal to 18 20 40 46 and as you can see already here this Matrix that we got in the previous calculation from one is equal to this Matrix that we got as part of second calculation which means that now we have proved that for this specific example indeed 1 is equal to 2 which means that a * b + C is equal to AB plus AC there we go so let's now look into another law which is the scalar multiplication law for matrices so the scalar multiplication law for matrices says that if we got a scalar R and a matrix A and B then R * a * B is equal to R * a * B and is equal to a * R * B so here the r is just a scalar so it's a real number and then A and B are matrices and what this low basically says is is that it doesn't matter what in which stage you will do your matrix multiplication with the scaler if you have this external scaler you can first take the two matrices multiply them with each other so the A and then B and then multiply it with r or you can take the scalar R multiply with your first Matrix and then multiply with B or you can take your second Matrix multiply with the scaler and then multiply with a it doesn't matter they will all result in the same Matrix so let us actually prove this by making use of our skills from matrix multiplication and scalar multiplication here I've picked up bit more uh Advanced example where uh a is 2x3 and B is 3x3 in this way we will train our multiplication skills for matrices and at the same time we will also prove that the scalar multiplication law of matrices holds so let's go ahead and do the multiplications so first we have a matrix a what is that Matrix Matrix a is 1 - one 2 so 1 - one and then two then we got 0 2 and then 1 which is 2 by 3 and then we got B which is equal 2 it is 3x 3 with elements 1 Z 1 1 2 Z one one and then 3 1 0 2 so it is 3x 4 so it's 3x 4 not 3x 3 but 3x 4 Matrix now the final part that I need here is this which is R is equal to 2 the scalar value so R is equal to 2 so the first thing that I'm going to do is to calculate this amount which is R * a * B for that what I need to do is to First calculate this a * B so let me quickly go and calculate this for us so given that the a has Dimension 2x3 and then B has a dimension 3x4 I can see that quickly that my Dimension criteria is satisfied the number of columns of a is equal to number of rows of B so that's fine and then I know also know that the final dimension of a * B is going to be 2x4 so it's going to be a 2x4 Matrix and how do I know that well because I know that from our um all the problems that we have solved we have already seen that we always need to pick the number of rows of the First Column and the number of columns of the second uh Matrix in order to get the final Dimension which is 2x4 so let me then go ahead and do the calculation so we are going to have a 2x4 Matrix let me write it even bigger so it's going to be a 2x4 Matrix so for the first row and First Column I need to look in here the first row and the First Column which means I need to take one so it's equal to 1 * 1 so + 1 * 1 is 1 - 1 * 2 is - 2 2 * 3 is 6 + 6 this is my first value and what is this amount it is equal to 1 - 2 is - 1 and 6 - 1 is equal to 5 so this amount is five five so what is this amount well this is my second row in the First Column so I need to make use of second row and First Column which is equal to 0 * 1 is 0 2 * 2 is 4 and 1 * 3 is 3 4 + 3 is 7 so this value is 7 seven we are ready to go on to the next column so column number two so then this time I need to look at the first row and second column so we are going to use this one so first we will use this first Row 1 * 0 is 0 - 1 * 0 is 0 0 + 0 is 0 and then 2 * 1 is the only nonzero element 2 * one is two so I already know that for my second column I got here two and what is this element well for this I need to look at the second row and second column so this thing so 0 * 0 is 0 2 * 0 is 0 1 * 1 is one which means that here I get a one let's not move on to on uh towards the third column so in here first I need to look at the first row so 1 - one and two and then this time remove this I need to look at the third column because I'm here in the third column so 1 * 1 is 1 -1 * 1 is 1 so here I got 1 - 1 and then 2 * 0 is 0+ 0 1 - 1 + 0 is 0 because those two cancel out this means that here in this element I got a zero and what about this element where I need to look here in the second row and here I need to look at the third column 0 * 1 is 0 2 * 1 is 2 1 * 0 is 0 so 0 + 2 + 0 is equal to 2 so this is 2 and now we are left with the fourth column so for that I need to look in here so for the first row which means first row in here and then the fourth column in here so first row in here and fourth column here 1 * 1 is 1 - 1 * 1 is 1 and then 2 * 2 is 4 which means that I end up with 1 - one and then + 4 and what is this this two cancel out I end up with four which means that here I need to fill in four and what is this final element it is the second row in the fourth column so the second row in the fourth column 0 * 1 is 0 2 * 1 is 2 1 * 2 is 2 0 + 2 + 2 is = 4 so now we obtained that a * B is this 2 * 4 Matrix as we have expected so this is then equal to 5 2 04 and then 7 1 2 4 so then the next step would be to take the scaler R and multiply it with a * B let me actually keep the colors consistent so a * B this is a * B so the only thing that I need to do is to take that in here and multiply this two with each of those elements so I will end up with the same size Matrix so 2x 4 only all these elements need to be multiplied with the scaler which means that I will get 5 * 2 is 10 2 * 2 is 4 0 * 2 is 0 4 * 2 is 8 and then 7 * 2 is 14 1 * 2 is 2 2 * 2 is 4 and then 4 * 2 is 8 so this is the result of the multiplication so this is the first part this is what we are referring as one so we have then checked in here that the r times actually we have already in here so I won't be writing again so as part of the first section we have already seen that R * a * B is this Matrix let's now move on to the next one which is calculating the second part so this is the first part this is the second and this is the third we have this already let's now move on and calculate this one so for this second case so the second case what we want to calculate is R * a * B so it is R * a and then * B this is what we need to calculate so the first thing that we will do is to calculate this part and then to calculate the entire thing the second point so let's go ahead and do that first we will take the A and then we will multiply all its elements by scaler two to get the r and then a this amount is equal 2 1 * 2 is = 2 - 1 1 * 2 is - 2 2 * 2 is = 4 0 * 2 is = 0 2 * 2 is = 4 2 * 1 is equal to 2 this is R * a now in The Next Step so this was one the next step we need to take this amount this Matrix to minus 2 4 042 and multiply it with 1 2 3 0 0 1 and then 1 1 Zer and then 1 one 2 so basically the Matrix B let's now move and work our way out with that one actually let me remove this from here and keep the space bit more clean R time a and I will multiplying this with the Matrix 1 2 3 and then 0 0 1 and then 1 1 1 1 and then 0 2 well I know that this one is 2x3 and this one is 3x 4 which means that the result will be 2x 4 let's now go ahead and calculate that Matrix which is equal with a dimension of 2x 4 well for the first row and First Column let me actually go and quickly do those calculations let's now go ahead and do those calculations so we are going to have four columns as previously the dimension is going to be 2x4 so let's do it column by column in here it means that we are in the row one and then column 1 so 2 * 1 is equal to 2 - 2 * 2 is - 4 and then here we got 4 so 4 * 3 is 12 so we got 2 - 4 and then + 12 and what is this amount 2 - 4 is - 2 + 12 is 10 so here we got 10 let me remove this 10 this is the second row and the First Column which means we got 0 * 1 is 0 4 * 2 is 8 and 2 * 3 is 6 so 8 + 6 is equal to 14 so here we got 14 this is the first row and second column which means that we are looking at this row and second column this time so 2 * 0 is 0 - 2 * 0 is 0 the only thing that we care about is this one and this element which is 4 * 1 so this should be four let's now do the same for the second row 0 * 0 is 0 4 * 0 is 0 0 + 0 is 0 which means we are left with 2 * 1 so here it comes two let's now do the third column so for the third column we got First Row 2 * 2 2 * 1 is 2 - 2 * 1 is - 2 and then 4 * 0 is 0 which means that here we get 0 because 2 - 2 + 0 is 0 then we got the second row and third column which is this row and then third Comm so 0 * 1 is 0 4 * 1 is 4 2 * 0 is 0 0 + 4 + 0 is four so this is four and then for the first row and then fourth coln so it means that we need to look at this specific column the first row is 2 * 1 it is 2 - 2 * 1 is - 2 and then 4 * 2 is 8 so 2 - 2 + 8 is 8 and then finally for the second row and the fourth column 0 * 1 is 0 4 * 1 is 4 2 * 2 is 4 4 + 4 is 8 this is the final Matrix which means that this entire amount that we just calculated step by step this is equal to this Matrix in here okay so this is the second element let's check whether the first element is equal to the first one so we see here 10 48 142 248 as you can see we are dealing with exactly the same Matrix which proves that indeed R * a * B is equal to R * a * B so this part we have already proven because we have seen that 1 is equal to 2 perfect so the only thing that is remaining is to calculate this third part and to see whether this is equal to this matrices because we have seen that the two of those are equal so the remaining thing that is left to prove this theorem is to calculate this third part let's go ahead and do that that so the third element says let's first calculate the r * B and then multiply it by a so we need to calculate b r * B * a this is what we need to calculate which means first we need to calculate this and then we need to can calculate the entire thing all right so let's go ahead and do that so R * B is equal to so we need to multiply each of the elements of B by two so we end up with this Matrix 2 0 2 2 and then 2 * 2 is 4 2 * 0 is 0 and then 2 2 and then 2 * 3 is 6 2 * 1 is 2 2 * 0 is 0 2 * 2 is four this is that first Matrix let's now go ahead and calculate the second part which is a * Matrix a so it is 1 - 1 2 and then 0 2 and then 1 multiplied by this Matrix which is 2 4 6 0 0 2 and then 2 two 0 and then 2 2 4 okay perfect so this is then what we need to calculate well this is 3 * 4 this is 2 * 3 which means the result should be 2 * 4 let's go ahead and do those calcul ations this first amount will be the first row and the First Column dotproduct of those which means 1 * 2 is 2 -1 * 4 is - 4 and then 2 * 6 is 12 so here we got 2 - 4 + 12 and what is this amount well 2 - 4 is - 2 12 - 2 is = to 10 so this one this element is 10 then for the second show we need to look in here so 0 2 one and the dotproduct of the one with the First Column so this thing and that is 0 * 2 is 0 2 * 4 is 8 and then 1 * 6 is 6 so what is 8 + 6 that is 14 and then for the first row and then the second column we need to look to the first row in here and then the second column in here and the dotproduct of the two well 1 * 0 is 0 Min - 1 * 0 is 0 and then 2 * 2 is four so that's what we are left with four and then for the second row and then the second column so this element we got 0 * 0 is 0 2 * 0 is 0 1 * 2 is 2 for the first row and the third column so we need to look in here 1 * 2 is 2 - 1 * 2 is - 2 and then 2 * 0 is 0 so we are left with zero 0 and then once we do the calculation for the second row we will see that we end up with 0 * 2 is 0 2 * 2 is 4 and then 1 * 0 is 0 so we end up with four and then here for the final column 1 * 2 is 2 - 1 * 2 is - 2 and then 2 * 4 is 8 the first two cancel out and we end up with 8 and then for the second row and the fourth column we look into here again this time the second row 0 * 2 is 0 2 * 2 is 4 and 1 * 4 is 4 and then 4 + 4 is 8 so if we look in here this is our third amount we will quickly see that again we have the same Matrix with exactly the same elements so now we have also proved this third part and we have seen that in all cases the r * a * B is equal to R * a * B is equal to a * R * B now we are ready to move on towards the second module in this unit which is about the determinants and their properties we are going to look into the uh determinants at high level we are going to Define them and going to understand what why they matter and why they are important then we are going to see how we can calculate the determinants we are going to see the calculation for 2x two Matrix then 3x3 Matrix and then just in general how we can do it and then we are going to see the properties of determinants one by one and then finally we are going to see the determinants interpretation from the geometric perspective so when we visualize it using python so by definition the determinant is a scalar value that can be computed from the elements of a square Matrix so this important Square Matrix and encodes certain properties of the Matrix so the determinant provides a critical information about The Matrix such as whether it's invertible and the volume scaling factor for the linear transformation it represents so we see that the uh concept of theer detent is highly related to many other concept that we have seen before so first here it's talking about the square Matrix then it's talking about encoding certain properties so having the determinant it contains certain information that um is related to the properties of the system that that Matrix is representing and then it provides critical information about the underlying metrix because the determinant is calculated from Matrix we say the determinant of a matrix so it contains a critical information about that Matrix such as whether it's invertible or not and this goes back to the concept of inverse we will see this once we learn the concept of determinant because the inverse calculation is dependent on the determinant but keep this thing in mind that the determinant contains information whether we can get um inverse from a matrix or not we will see this concept over also in detail in the next section but for now we can remember that the determinant contains important information related to the invertibility of the Matrix so having an inverse or not and then it also contains information about the volume scaling factor for the linear transformation it represents so here we then go back to this concept of a x is equal to B and then knowing the determinant we can then comment on this volume scaling factor for this linear transformation that it represents so let's go uh on to the next slide to find out bit more about the determinants and specifically how we can calculate the determinant in the mathematical terms when it comes to the 2x two Matrix because the uh determinant of a 2X two Matrix is quite straightforward so for 2x2 matrix a with this elements where a b c and d they are all real numbers the determinant which we Define by this de a so that is a short way of saying determinant and then in here we always write the Matrix of which we are Computing the determinant is then equal to and then we are taking this a * D so we are taking this diagonal elements a * d so they are on the diagonal and then we are subtracting from this this other two the remaining two elements of the diagonal so B * C and this gives us the determinant of 2x2 matrix this is just a formula that you need to uh remember whenever you want to calculate the determinant of a matric by hand manually so the calculation for larger matrices it involves bit more uh difficult uh calculation we will see also in a bit the uh determinant of a 3X3 Matrix It relies on the determinant of a 2X two Matrix and the idea is that every time we uh increase the dimension of our problem so let's say we are in R4 then we will go back to the R3 and then given that R3 relies on the determinant of the underlying 2x two matrices anytime we increase the dimension we again go back to this IDE of using 2 by two matrices that form the entire Matrix in order to compute the determinant only when it is R4 R5 Etc so it becomes much more difficult to describe and to do it manually therefore there are other algorithms which we will see at the end of this course like uh the composition algorithms and factorization algorithms that can be used in order to calculate the determent of a matrix that has higher Dimension higher than the tree for instance but in this specific unit we are going to discuss both the calculation of the 2x two matrices determinant and the determinant of a 3X3 matrices and we will also see detailed examples of them so without further Ado let's then go ahead and calculate the determinant of this 2x2 matrix so let's now look into this specific example where we are calculating the determinant of this 2x2 matrix so this is the A and let's keep in mind that this is the um uh a this is the uh B in this not in this uh way of writing the Matrix a so the uh letters corresponding of the uh elements of this Matrix a so this is the a this is the B and then this is the C this is the D and we said that the determinant that of a is equal to the diagonal elements so 1 * 4 minus the of diagonal Elements which is 2x3 because we said that the definition of this determinant is that is equal to a * D and then minus B * C which is exactly what we are doing in here so if we calculate 1 * 4 is = to 4 and then 2 * 3 is = 6 4 - 6 is = to - 2 therefore we say that the determinant of Matrix a is equal to Min - 2 let's now go ahead and uh practice with calculation of determinants on two other matrices so in this case we are still in the two dimensional space so we have 2 by two matrices we'll first calculate the the determinant for Matrix a so we see that we got this element 5061 and we know that by definition the determinant of the 2x two Matrix so that of Matrix is equal to a * d - B * C where the Matrix has the following form so we got a and then D in here and then B and the C in here so we see that this is basic Bally our a this is our D this is our B and this our C the way you can also said is that those are the diagonal elements and those are the of diagonal elements so therefore it means that we can calculate the determinant of Matrix a by taking the five multiplying with one so it is 5 * 1 minus the off diagonal element which is 6 * 0 and this amount is equal to 5 - 0 and is equal to 5 let's go ahead and also calculate the determinant of Matrix B we see here that on the diagonal we have this two elements one one and of the diagonal elements are both zero those two therefore we can calculate the determinant of this 2x2 matrix which is also sometimes referred as I2 so it is the identity Matrix because we got here the E1 and then E2 in the two dimensional space and the determinant of the Matrix B using this definition is then equal to 1 * 1 - 0 * 0 so 1 * 1 - 0 * 0 and this is equal to 1 and this is actually a special case of determinant and later on we will see why it is so important to uh have this relationship of identity Matrix having a determinant and having it equal to one um and this relationship between determinant identity Matrix is something that we see so uh in the upcoming lesson so keep this one in mind so now when we are clear on how we can calculate the determinant for 2 by2 Matrix so this is quite simple and straightforward calculation by taking the diagonal elements a and then D and then subtracting from that from that product a * C we are subtracting the off diagonal elements products B * C we can then get our determinant and now when we are clear on that we are ready to go on to bit more Advanced calculations which is calculating the determinant this time for the 3X3 Matrix so now we increase the um the dimension size and we go from R2 to R3 because now we have a 3X3 Matrix and by definition given a 3X3 Matrix a which has the following elements so a11 a21 a31 and then a12 a32 a 32 so we have already seen this coefficient labeling this should look very familiar this is 3x3 Matrix 2 and the determinant of a matrix a denoted as that a is calculated using the formula and here we see the formula we are basically using the 2x two matrices that form this Matrix a in order to calculate the determinant of the 3X3 Matrix and how we are doing that well we are using this element and then this element and this element and every time we are hiding part of the Matrix so when we have for instance this a11 so for this first part we are saying well let's hide the row and the column corresponding to this element which means that we need to hide this this row and this column and what is left is this 2x two Matrix we will calculate the determinant of this 2x2 matrix and we will multiply this with this element that we use in order to remove the corresponding row and column this will form the first element in here so you can see a11 which is a simple value so this is the um uh entry volume which is in the first draw and First Column a11 multiplied by the determinant of this Matrix so this Matrix so once we have that and we already know how we can calculate a determinant of a 2X two Matrix because this 2X two so taking the diagonal elements and then multiplying them together subtracting from that the of diagonal elements product now we are ready to go on to the next part of the calculation which is this time adding a minus here so you can see here this here is plus and then here is minus so we do here minus and for this second step what we need to do is kind of similar only this time the element that we will be using to understand how we can remove the row and the column so we will then dark it out it is this one a12 so then we will need to remove this column and this row and then the remaining Matrix which is this one this 2x two and here I mean a 21 a 31 and then a a 23 and then a33 this is the Matrix that you can see in here remaining which means remove this one and then this one and then the remaining 2 by two Matrix is what you need to use in order to do your calculations so you can see that I got exactly the same in here and once again we are Computing the determinant of this Matrix we are multiplying this with this a want to element so this element and now we have also the second element in our calculation and then we go on to the next step which is a plus sign here let me use the same colors plus sign here and then we are using this time our final third element to understand which row and which car we need to dark out which is this element so we then remove the first row and the last column and this is then the Matrix the 2x two Matrix that we use in order to do our calculation so deter the determinant of this Matrix multiplied by the a13 so we could also use in the same manner this row or this row it really depends on the kind of values the the tip that um I will provide to you or the trick is that to always look for these z z values wherever I see Z zos or I see one one I'm thinking that hey those s u values that um give me the more straightforward and easy calculations because if I have zeros in my Cal in my entry so if I got a zero here for instance 0 times any determinant is zero I don't even then need to calculate the determinant right because then I know that I'm multiplying that determinant with zero therefore if I know that that entry for instance this row contains the majority uh of zero so it is 0 01 then of course it's a great uh row to pick to use these Target elements so in that way I will then know that this is the row that I need to Target but if it is like that that for instance I got a matrix 10 3 4 and here I got 0 1 Zer and here I have 100 three and four of of course the easiest thing would be to not use this row but instead use this one so in that case I will then have this zero and zero as my target values which means that I will only need to calculate the determinant of a 2x2 matrix this uh for this one for the two cases I don't need to do it because I know that the corresponding Target values the target elements from my Matrix will be zero so let me show you what I mean by that so if for instance I go for this second row and not the first one what I need to do is that I can calculate the determinant of a by taking the a21 this then will be my target I will then need to remove this uh column and this row then I will need to do the determinant of A1 2 A1 3 a 3 2 a33 this is what then I need to do then the next thing I need to do is of course here I have a plus here I need to do minus because we always need to Interchange the values so here is a plus here is a minus here is a plus so I do PL a minus in here then I do the next element in my row which is this one let me use red color so a22 so I'm then doing a 22 multiply the determinant of so I'm re removing this row and this column A1 1 a13 and then a31 a33 and then the final part is of course as you might have already guessed is to look into this element so it is plus a23 multiplied the determinant of let me actually write it down in here the determinant of a11 a12 and then a31 and then a32 so in this way basically independent of what row I will take as my leading row that I will do my calculations and I will just need to pick one row I can always get the same value for determinant of a but choosing intelligently which row to pick it will save you a lot of time and headache in terms of calculations because if you are dealing with a row that contains many zeros for instance you have 0 0 one or one 0 0 or even better 00 0 then you know know automatically that you will need to calculate your DET the determinant once here also once and here you don't even need to calculate it you know that you got zero here Z here zero here so it's automatically equal to zero so I hope this makes sense because this is a trick that usually you will not come across but this just helps you to save a lot of time uh when it comes to calculation of your determinants in a tree by3 settings so in this case uh we have this um we now we have this definition and we know the tricks that we can use but I think it's really uh helpful to go ahead and to solve a problem so basically this is the higher level summary of the steps that we just discussed um so the determinant of a 3X3 Matrix it simply involves multiplying the a11 by the determinant of the 2x2 matrix that that remains after excluding the row and column of a11 so what we did in here by doing this and subtracting the product of A1 2 and the determinant of its respective 2 two Matrix so this part and then adding the product of a13 and the determinant of its respective 2x two Matrix so this part and the signs alternate so it means first you always got the plus then you always is get the minus and then the plus so they interchange you start with plus then you do the minus and then the plus so let's go ahead and calculate the determinant of this Matrix so before even looking at the answer let's actually go ahead and do that on this page paper so we got a matrix a which is equal to 1 2 3 4 so basically from 1 till 9 1 2 3 4 6 4 5 6 and then 7 8 9 and for this 3x3 Matrix we need to calculate the determinant so the determinant of a the first thing that I'm seeing is that there are no no rows with zeros or columns which means that I cannot use my uh trick and instead I will just need to go with let's say the first dra and it's also convenient given that I got as scaler this values this much smaller values relatively to the other ones all right so first things first let's go ahead and write down that formula so the determinant of a is equal to first we are going to take this one so our one one times and then we got determinant of and then we have this Matrix which is 5 6 8 and 9 this is our remaining Matrix then the next thing we need to do is to Interchange the size uh the the sign which is minus and then we got so the remaining Matrix is then determinant of 4 6 79 and then finally Plus plus three times and then determinant of what do we have well this is the target so it is 4 5 7 8 right so let's go and do those calculations quickly this is equal to 1 * the determinant of this is the diagonal element so 5 * it is 5 * 9 - 8 * 6 - 2 * 4 * 6 - 7 * 6 Sorry 4 * 9 so the diagonal elements 4 * 9 - 7 * 6 and plus three * and then 4 * 8 - 5 * 7 this is equal to so 9 * 5 is = 45 8 * 6 is = 48 then - 2 * 4 * 9 is 36 7 * 6 is = 49 7 * 6 is = to 42 + 3 * 4 * 8 is = 32 - 5 * 7 is = 35 so this is equal to 1 * - 3 - 2 * and then here we got 36 - 42 so that's - 6 + 3 * - 3 which is that = to - 3 + 12 - 9 which which is equal to zero so let's check it indeed we got the right answer perfect so now when we are clear on how we can do this calculation let's now go ahead and calculate yet another determinant of a 3X3 Matrix and this time I want to show you this uh simplified version by you making use of this trick that I uh specified so instead of using this first dra as an indicator I will be using the uh second row as my indicator one thing to keep in mind when making use of this trick is that when you start from the second row so from the even rows even rows second fourth or sixth then in those cases you need to flip the signs that you will be using so while in here you had Des Sign Plus in the beginning then you got a minus and then a plus when doing all these calculations so you remember here we got plus minus plus when you start from the second row instead of first one you need to flip the order of this so you need to start with minus you have minus you got plus and then minus so knowing this trick it also means that you go One Step Beyond and you know how you need to intelligent ently uh reduce the time that you are spending on calculation calculation of the determinant but it also means that you need to be careful on knowing what kind of signs you need to use because if you start from the first row you start with plus and then you do minus plus minus plus so knowing how to start you already know how you can go on but when it comes to the second row so the even rows you need to start with a minus so you need to do minus plus minus plus dot dot dot all right so let's now go ahead and use that technique in here so here I see that my first row doesn't contain zeros but my second row does so this gives me indication that I can reduce the time that I spent on calculating the determinant at least one time because I didn't no longer need to calculate that determinant so the determinant of B is then equals you I will then start with minus given that I'm going to use this rope and then I have zero times so because this is my element the determinant the determinant of 2306 and then plus then this time the second element Target element is this one so it's four times and then we got determinant of 1 3 1 6 and then minus the five times so this five determinant of 1 2 1 0 and what is this amount it is equal to this I don't need to calculate because I got a zero in here this this trick is all about this to not calculate the determinant too often and then this equals you four times four times determinant of this is 6 - 3 so 1 * 6 - 1 * 3 which is equal to 3 and then minus 5 * determinant of 1 2 1 0 which is 0 - 2 so this equal to 4 * 32 - 5 * - 2 which is equal to 12 + 10 and this is equal to 22 let's go ahead and check this and this is the more detailed and formal derivation so one uh interesting thing is that I calculated with my second row and in here in this slides you can see calculation with the first dra this is just a nice way of seeing the difference that you can do and here uh in this solution what we have is that we have manually calculated this first determinant too so in total three determinants but we again in end up with the same determinant so independ what kind of row you will use in order to calculate your uh determinant of Matrix B you will all always end up with the um with the same similar volume unless you have made a mistake in your calculations so you just need to keep track of the uh rows that contain many zeros and you need to um be careful in terms of the signs that you need to use and the sign that you will need to start if you start with the first row then start with plus if you start with the second row then it is minus and then plus Etc so as you can see here it's a plus and then minus and then Plus in my case I did with my second row therefore I started with minus all right so let's now move on to the properties of determinants so the determinant of an identity Matrix is one that's something that we have also seen when doing our calculations because we are so that in one specific case when we had this example so this Matrix B and the Matrix B was the identity 2 in the two dimensional space we have calculated its determinant and we saw that it's equal to one and this was not a coincidence because the determinant of identity matrices is always equal to one then the second property is that swapping two rows or Columns of a matrix changes the sign of its determinant so if you swap rows or columns in your Matrix so if you end up with Matrix A and B they are exactly the same only one swaps the two columns or two rows then you are changing the determinant of that Matrix uh the sign of that determinant but not devalue itself it means that if you got a and you got B and your a is equal to let's say uh A1 and then uh A2 and then A3 so it contains these columns and then Matrix B is equal to um let's say A2 and then A1 A3 then the determinant determinant of a will be equal to minus of the determinant of B you can also say determinant of B will then be equal to the minus determinant of a this is basically the idea of this property let's now move on to the third property which says that if a matrix has a row or a column of zeros its determinant is zero so if you got a matrix a that contains this different values a11 H1 dot dot dot a and uh M1 and then here you got suddenly um column that contains all zeros and then the rest are nonzero even so in that case you know that your determinant is equal to zero so for a specific example if you got for instance Matrix 1 2 0 0 0 3 13 then the determinant of this Matrix is equal to zero and otherwise if you got a matrix B that has a column of zeros so column that is entirely of zeros so let's say here we have 1 one one and we have a zero Vector here so we got in here 0 0 0 and then 3 4 five then given that we have here this zero Vector then the determinant of Matrix B is equal to zero and this actually straightforward to be seen from this calculations that we saw because if you do the uh if you pick this specific row and then you do zero times the ter DET minant of the remaining Matrix 0 times determinant of the other Matrix and then plus so PL and then so minus and then plus and then minus 0 * determinant of the third Matrix it is obvious that 0 * a determinant is 0 0 * determinant is z 0 * determinant is zero which means that you got a whole bunch of zeros to be added to each other or subtracted from each other this means that if you have a row or a Col with zeros this already gives you an idea that your determinant is equal to zero you don't even need to do calculations so the final property of determinants is that if a determinant of a product of matrices equals the product of their determinants so the determinant determinant of a b is equal to determinant of a multip by determinant of B this is basically what this property is about so let's quickly go through examples to ensure that we are at the same page with all these properties and we can prove them so let's say we have an identity Matrix n by n which is we are dealing with I in now according to this first property when we calculate the determinant of this Matrix so determinant of i n is equal to 1 Let's actually look at a specific example so here we got um identity um Matrix in the two dimensional space in the R2 and we can quickly calculate the determinant of this I2 and we can see that it is equal to this diagonal element so 1 by one - 0 * U it's actually something that we did as part of my previous examples so this equal to 1 - Z and is equal to 1 one thing that I wanted to show you before moving on to the next example about the swapping rows is that when we are swapping some of the rows or some of the comms or two rows or two cars of Matrix a we are referring to this matrix by this notation so we add this nod in here and we say that that this is basically the manipulated version of Matrix a so if we have for instance Matrix a equals u a b and and c and those are vectors and then we are swapping two of The Columns let's say we are swapping this two we get B and then a and then C then this Matrix will are referring as a not this is just an a matter of notation and we just learned that as part of the properties that the determinant of this new Matrix is equal to minus the determinant of a so if a matrix a has a row or column of zeros then the determinant of it is zero so let's actually quickly look at this specific example example in here we got a which is uh having a column of zeros and another column of B and D where B and D are real numbers so let's prove that this determinant is actually equal to zero so the determinant of a 2X two Matrix we have already seen is equal to the diagonal elements so 0 * D minus the of diagonal Elements which is 0 * B 0 * B and what is number * 0 is equal to 0 0 - 0 * B is also Z it's equal to Z therefore the determinant of a is equal to Z so when it comes to the uh determinant of a product of a matrices let's prove that the determinant of a * B is equal to the determinant of a times the determinant of B so therefore the first thing we need to do is to calculate this a * B let's quickly go ahead and do that so let me add here this um blank file so a is equal to 1 2 3 and 4 B is equal to 5 6 78 and I I want to prove that the determinant of a b is equal to determinant of a Time determinant of B first I will be calculating this and then I will be calculating this so for the first one what I need to do is that first I need to calculate d a * B which is equal to 1 2 3 4 times 5 6 7 8 and then this is equal 2 should be 2 by two so first I take this 1 * 5 is 5 2 * 7 is 14 14 + 5 is 19 then for this one I need to pick this row so 3 * 5 is 15 and then 4 * 7 is 28 so 15 + 28 is so there we have 33 43 so I got here 43 then I'm going on to the next column which is in this case 1 * 6 is 6 6 + 6 16 is 22 and now the second column 3 * 6 is 18 4 * 8 is 32 and this gives me 50 all right so now I have the a * B then as the next step what I need to do is to calculate the determinant of this a * B which is equal to the determinant of this Matrix 19 22 4350 that I just calculated and what is this amount the diagonal elements 19 * 50 - 43 * 22 19 * 50 is then equal to 95 and 43 * 22 is 94 6 which means that we end up with four this means that the determinant of the a * B is equal to 4 let's quickly check what are the parts of the second amount so for that I need to calculate determinant of a which is equal to 1 * 4 - 2 * 3 1 * 4 is 4 3 * 2 is 6 so 4 - 6 is = - 2 determinant of B is equal to 5 * 8 which is equal to 4T and then 7 * 6 is equal to 42 and this is equal to - 2 and determinant of a * determinant of B is equal to - 2 * -2 which is equal to 4 so we can see that now we just provve that the determinant of a * B is equal to 4 so we have seen that determinant of a is equal to 4 and we see that that's exactly the same as determinant a * determinant of B which is equal to 4 so we have just proven that the this equation indeed holds so the determinants they are not just um some calculations or some amounts but they are actually uh important concept and their interpretation um is highly relevant from geometric perspective so the terminant have a geometric interpretation and the for example the terent of a 2X two uh Matrix or 3x3 Matrix they represent the area in case of 2x two or the volume in case of 3x3 Matrix uh of the parallelogram that they are forming so uh this is often referred as a parallel uh piped um I hope I'm pronouncing this correctly and it's formed by the con vectors of the Matrix so if we have for instance this uh Matrix a and then we have a b and then C and D we have this A and C which is the first vector and then B and D which is the second vector and the uh the shoe vectors they actually form a parallelogram um when it comes to the uh two dimensional space and the area that this uh parallelogram um is forming that is equal to the determinant of this Matrix so the determinant ofer this scalar value that summarizes this linear transformation that we describe by this Matrix because we saw that we had this a x is equal to B linear system that we were describing using this coefficient Matrix and this was our unknowns this was our variable and then this B was the um amount that we were uh putting this as equal to if B was equal to zero then we were solving the homogeneous system otherwise we had this non-homogeneous system and in the geometric terms the determinant of this Matrix a so the determinant of a um in case of 2x two space so in R2 um when we got two vectors basically in our Matrix a this is equal to the area that is spent by these vectors in the two dimensional space in a bit I will also show you specific example such that um we will be on the same page when it comes to this concept of parallelogram the deter determinant and those vectors that form the column um uh space of the uh Matrix a uh when it comes to the three dimensional space when we have R3 so we got 3x3 Matrix of a then the determinant of this Matrix a is the volume that is um formed by these uh threedimensional vectors because unlike the 2D in R3 we got the three vectors that form the a let's say this one this one and then this one and then here we can create this area covered by this Tre vectors and the area that is formed by the tree vectors from a it is equal to the determinant of that Matrix a so in terms of the 3D it's bit harder to uh visualize it but in uh case of the two-dimensional space I think this will help uh to improve our understanding of the determinants and make this interpretation uh from geometry uh from geometrical perspective so given the two vectors A and B in the two dimensional space the determinant of this Matrix uh is then equal to the um diagonal elements we already know minus the of diagonal elements right so we are also saying we have seen this notation already very often you will see this volume this is the absolute we already know this from high school this is the absolute volume because the determinant can also be a negative number we have seen minus 20 or minus 2 and we know that the area cannot be a negative number therefore we are adding this absolute term here so knowing for example that we have this m matx a which consists of the elements 3 2 and then 1 14 we know that the determinant of this a is equal to 3 * 4 12 - 1 * 2 it is 10 and the absolute value of it so absolute value of 10 is equal to 10 given that is positive and this is exactly what we have here and this is referred as the area of the parallelogram that the two vectors are forming and how does that look like in uh the uh coordinate space so this is the parallelogram that we were referring by and this area that is formed by this parallelogram is equal to the determinant of the a The Matrix a so so one thing that we need to keep in mind is the definition of parallelogram which means that those two are parallel and they are the same so this and this lines those two are the same and then of course the same holes for those two they are parallel and they have the same um length therefore this figure in here this is what we are referring as parallelogram and those two vectors that we can see in here this one and this one they form this parallelogram and they are the two vectors that are part of the Matrix a hence if we got two vectors that the uh that come from The Matrix a so Matrix a and we got here this two vectors in a 2X two Matrix then the determinant of this Matrix is then describing the area that these two vectors are using or are spanning when creating this parallelogram so the determinants they play an important Ro in understanding the geometric properties of the spaces that uh spent uh by these vectors they provide valuable insights when it comes to the scaling effect effect of linear transformation the or orientation and the um the locations of them in the cordan system as well as the Practical applications in calculating areas in calculating volumes welcome to another unit in our fundamentals to linear algebra course where we are going to talk about Advanced linear algebra Concepts so uh in the first module we are going to talk about Vector spaces and the projections we are going to define the bases in a couple of examples of them we have already touched upon this concept briefly when we are calculating the basis of a no space and the basis of a comp space we are going to do a similar example in this case and then we are going to uh look into this concept of the uh standard bases for uh different spaces including the R2 we're going to introduce the concept of projections what is the definition of projections what is a Formula how we can calculate it we are going to look into detailed examples of that then we are going to talk about the concept of uton normal basis in this module we are going to introduce this concept and we are going to understand the orog gonality normalization we are going to then discuss a very important topic in linear algebra which is a gramme process we're going to Define it we are going to see the overview the step-by-step process of applying grme uh algorithm then we are going to see an example of it and the calculations step by step and then we are going to talk about applications of auton normal bases the application of gram Smiths process and the importance of this auton normal basis this is the module one of this part so let's first Define the basis a basis of a vector space is a set of of linearly independent vectors that spend the entire Vector space every Vector in the space can be expressed as a unique linear combination of the basis vectors so there are a couple of parts in this definition they are really important and first thing that we need to uh mention here is this Vector space that says it is a set of linearly independent vectors that spend the entire Vector space this is very important because um here we are with the basis is simply this Vector space that is a set of linearly independent vectors which means that one of these vectors cannot be Rewritten as a linear combination of the other one so we have a linearly independent vectors and they span the entire Vector space so for instance if we are in R2 then the basis of a vector space is then a set of linearly independent vectors that span this entire R2 so we do we then need to have for vectors forming a basis so let's say we have a basis of vector space for us to say that this is the basis of this Vector space let's say in R2 we need to First say we need to First prove that these vectors this vectors are linearly independent and two they span the entire R2 which means that span of this vectors is equal to R2 we can actually be even more spefic specific in a example of let's say having a vectors A and B we can say that this set that we have here consisting of vectors A and B in R2 form the bases of a vector space if the first criteria is that a and b are linearly independent and the second criteria is that those two vectors together they spend the entire Vector space of R2 which means that span of a and b Vector space is equal to R2 on more specific example and then the second part of this definition says that every Vector in the space can be expressed as a unique linear combination of the basis factors which means in our specific example when we had this A and B forming the bases of a vector space this means that if we prove that this is indeed the basis of this Vector space then any combination every Vector let's say a vector C that consists of this C1 and C2 elements that this Vector this random Vector from R2 C can be represented as a linear combination of these vectors A and B so let's say we have a coefficient K1 * a plus K2 * B then here we are representing this random Vector c as a linear combination of these vectors A and B which form the bases of a vector space of this Vector space so we have previously spoken about the no space and Comm space so let's now go ahead and do one more example when we are calculating the new space and the Comm space and then we are again calculating this concept of basis of Vector space and a b basis of the com space and then we will be uh finding the basis of a vector space uh with um R2 example so given that we have already looked into this concept the basis of Comm space and based of no space I will try to uh go through this example bit more quickly to save time on more complex Concepts so let's say we have an example Le of a matrix and that Matrix is a is equal to 1 2 36 this is our 2x two Matrix a and the first thing that I want to do is to understand look into my Matrix and understand whether I'm dealing with unique vectors or not and by unique I mean whether I'm dealing with two vectors that are linearly dependent or linearly independent this kind of inspection always helps us to save time when we are doing our calculation for the no space and for the Comm space and for the basis of no space and base of Comm space now here we can see that this is our A1 the first Vector the first com Vector forming the Matrix a and this Vector is the A2 another thing that uh we can notice here is that we can easily take the First Column A1 multiply it by two and get the A2 because 1 * 2 is 2 3 * 2 is 6 that is that 2 * A1 is equal to A2 which means that we can say that A1 and H2 are linearly dependent okay so seeing this and knowing this this can help us to quickly go through our calculations of the bases of the co space and the bases of a no space so let's go ahead and first calculate what is the basis of no space of a so we have already learned that the um basis of a no space can be calculated when looking into the first no space so we C we need to calculate the no space and then we need to calculate the basis of that no space so this means that we need to get the na and we have learned that in order to get DNA we for that need to solve the a x is equal to zero problem and this x will give us the no space of a we have also learned that the no space of a is equal to the no space of r r EF of a which means that using gausian reduction or gausian elimination we can quickly find the solution to this problem of a x is equal to Z and find this x this is simply solving a similar problem only in this case the B so this is equal to zero because we are dealing with the homogeneous case I want do the calculation for this we have done a ton of examples when we were doing this step by-step calculation getting the uh argumented Matrix of a and then uh doing all these different draw operations normalizations and then eliminations in order to uh get this uh complex Matrix a to the point of uh basic representation from which either we can visibly see the solution to the problem or we can at least simplify it and describe it as a linear combination of vectors in this case if you go ahead and solve this problem you will find that the x that solves the a x is equal to Z problem is unique and this x is equal to minus 0.894 as the first element and then 0.447 as a second element this can be a good practice also to refresh um the memory when it comes to the gaion elimination and reduction the example itself is quite simple the a is just a 2x2 matrix um and um by performing couple of operations uh in terms of normalization and elimination you can find this X for your a is equal to zero given that now we know what is the solution to a is equal to Z problem now we know what no space is because in this case the all this help us to understand that the no space of a is then equal to the set the vector set where as part of this we got just single column which is - 0.894 and 0.447 this is the no space this is the first part I will say it 1.1 and then 1.2 will be to get the basis basis of this Na and we have just seen what is the definition of the bases so the basis of vector space is a set of linearly independent vectors that spend the entire Vector space therefore given that we got just this single Vector as a solution to our problem we can see then very quickly that the new space of a is based on this and then the basis of the no space is simply this entire set so knowing what the solution is to our homogeneous problem a is equal to zero so let me also write down in here then we know that the no space the N A is then equal 2D Vector minus 0.894 and 0.447 this is my Vector X that solve this ax isal to zero problem and this is simply the no space of a and given that we have calculated and we have got this unique solution to our problem we can say that any Vector in R2 can be represented as a linear combination of this Vector so 1.2 any Vector in R2 can be represented as linear combination combination of this Vector X therefore we are saying that the bases of of no space of a is this entire set consisting of the single Vector so this is about the basis of a no space let's Now quickly look into the concept of the basis of a calm space so the first thing we need to then uh get is the column space so to get the basis of Comm space we need to get the ca first which is the Comm space of a and what is the Comm c space of a the Comm space of a is the uh setle and the space of the vectors that we can see in here in this A1 and A2 is it's quite straightforward so this two vectors they form the Comm space of this Matrix a so then the ca is simply the set of one three and then two six vectors this is A1 this is A2 now we have just seen in the beginning before even starting our calculations that A1 and A2 are linearly dependent because A2 can be right written as 2 * A1 so one of these vectors can be written as a linear combination of the other one this means that we got just a single linearly independent vectors and why is this important because we have seen in the definition of the basis that for us to have a basis we need to have a linearly independent vectors so the basis of vector space in this case the Comm space is a set of linearly independent vectors that need to spend the entire Vector space in this case R2 so therefore we need to look into the ca that we got in here and select one of these two vectors that can be considered as linearly independent let's say we pick one three now we know that we can then write any Vector in R2 as a linear combination of this Vector 1 3 so we can scale this Vector one Tre and get a new Vector in R2 therefore or we are saying that the basis of Comm space basis of Comm space space of a is then the set of one Tre because one Tre so A1 is then linearly independent and the span of A1 is R2 now when it comes to the uh basis of the entire R2 one thing that we can notice is that this A1 so one three it's not forming it's not spanning the entire R2 because because we cannot uh write any random Vector in r two as a linear combination of this two therefore we are saying that this is the basis of Comm space but we are not saying that this is the basis of R2 and the final element in this definition that I want you to uh focus on is that every Vector in the space can be expressed as a unique linear combination of the basis vectors so in here we have looked into this idea of bases of a new space and the base of Comm space and we saw that we are talking about specifically the new space and comp space but when it comes to the entire space for instance the basis for R2 then the basis of Comm space for instance is no longer um helping us because the basis of Comm space it consists of this Vector one tree and this one tree alone is not satisfying the second criteria that says that this Vector needs to spend the entire Vector space because this one Tre vector it's a single vector and this Vector it is not forming the entire R2 it's not um the basis for R2 it's not spinning the entire uh R2 so given that the one tree is not spinning the entire R2 because of that we know that the one tree is not the set of one Tre is not the basis of R2 so this distinguishing of the basis of R2 basis of Comm space basis of no space is really important because basis for R2 it means that we need to find set of linearly independent vectors that they together form the entire R2 they span the R2 which means any random Vector that we can see in R2 we can represent as a linear combination of the vectors in this space so in here let me also prove that this one tree alone is actually not forming the R2 it's not spinning the R2 which then uh concludes that they are not the it is not the basis of R2 CU and after this I will then provide you an example where we have a set of vectors that span R2 and are linearly independent which means that they are the bases of the entire R2 so first I want to show you why this single Vector one Tre is not the basis of R2 so being the base of R2 we have the criteria that the vectors need to be linearly dependent so let me actually clear up some space here so I want to see and find the basis of R2 first I want to prove that this set which is the base of Comm space I want to prove that this is not the basis of R2 then I will also as part of the second part of this proof look in look into the case when we do have vectors and the set of vectors it forms the base of R2 so the first thing the first criteria of the basis of R2 says that quote 1.1 the first criteria says that this Vector in this Vector space it need to be they need to be linearly independent well that criteria is valid given that one3 is linearly independent this means that criteria one is satisfied so whenever you got just one vector this criteria is automatically satisfied so then you have the 1.2 which says that we need to have this spin of these vectors equal to R2 so is the span of one3 the R2 well no and how we can prove that because the idea is that any Vector including an example where I have for instance uh let's say four and five this Vector that I need to be able to find a scalar that will help me to create a linear combination let's say C linear combination using this Vector 13 which will then set this amount this to be equal to this which means that I need to be able to write my random Vector 45 as a linear combination of this Vector that forms my uh Vector space so let's see whether that is even possible well here I got four and five if I do this multiplication in the right hand side I get C and here I got 3 C because C * 1 is C and 3 * C is C and this means that I have an equation 4 is equal to C and 5 is equal to 3 * C from this I get that the C is equal to 4 and C is equal to 5 / to 3 but that is impossible because 4 is not equal to 5 / to 3 which means that I'm proving in here and I got to prove that the uh any random chosen Vector 45 cannot be written as a linear combination of this Vector that 4 forms this uh space therefore as random Vector from R2 can't be written as linear combination of one three criteria two is not satisfied because for that we had to say that this pen of One Tree is equal to R2 which we saw that it's not the case because then we would have been able to represent this four five as a linear combination of the one Tre Vector okay so now we have proven that the one Tre is not forming the bases of R2 let's now look into what then does form the basis of R2 an example of it so we are familiar with the unit vectors E1 and E2 into R2 which form the identity Matrix I and this is 1 0 and this is 0 1 also 1 0 0 1 in the form of a matrix so in this example we have a set consisting of E1 and E2 where this is this E1 this is the E2 and the set corresponding to this Vector space is then 1 Z and then 01 and now I will be proving that this space this Vector space does indeed equal to the bases of R2 so this is the basis of R2 so the first criteria of the bases is that these two vectors should be linearly independent now we can quickly uh remember from our previous theory that the two unit vectors one z01 are actually linearly independent that's something that we have proven and you can easily see it also from here there is no way that you can find um scalar C that you can multiply this Vector we and get a vector 0 one because for that for this one to become a zero you need to multiply this with zero but then 0 * 0 is not equal to 1 which means that there is no way that you can find a scaler C to multiply this E1 to get the E2 so let me write this down E1 and E2 are linearly independent because there is no scaler C which is a real number such that such that c * E1 is equal to Ich so this means you can't write hu as linear combination of A1 or vice versa this means that E1 and E2 are linearly independent and this satisfies our first criteria so criteria one is satisfied what we have also learned is that any Vector in R2 can be actually written as a linear combination of a unit vectors that form that um R2 in this case 1 0 and 01 so let's assume that this random Vector is C1 C2 so this is C vector and what we want to prove is that we can always write this C in terms of linear combination of these two vectors and how can we do that well let's say here we got a K1 K1 which is a real number and we multiply this by one Z and then we add K2 K2 and then here 01 so this is our E1 this is our E2 can we do this well what is this this is equal to K1 0 plus 0 K2 and and what does this give us well this means this amount let me write it over K1 * 1 which is the E1 plus K2 * 01 which are which is our second Vector E2 this is equal to K1 0+ 0 K2 and this is equal to K1 K2 so I got on one hand this Vector C1 C2 which I want to write as a linear combination of K1 E1 plus K2 E2 if I take D K1 equal to C K1 and K2 K2 equal to C2 well then in that case I can prove so this is basically equal to C1 and C2 which means if I take this K1 and K2 equal to C1 and C2 respectively and those numbers are given then I can represent this vector c as a linear combination of E1 and E2 which is what I had to prove in order to say that the Spen of one Z which is the E1 and 01 which is E2 is equal to R2 because any random Vector that will be provided to me with an element C1 and C2 and those are just real numbers can be written as a linear combination of these two vectors this means that the spend of these two vectors is equal to R2 and this is basically the second criteria so criteria to satisfied and if the criteria one and criteria 2 are both satisfied it means that this Vector space of 1 Z and 01 this is the basis of the entire R2 so let's now talk about the concept of projections by definition a projection of a vector a onto another Vector B is the orthogonal projection of a along B it's denoted by approach and then B underneath here we see the index and then a so projection of a onto B so here is the A and here is the B and represents the component of a in the direction of B so component of a in the direction of B all right so in order to properly understand this concept the intuition of it let's actually make use of the R2 space so let's first start by picturing in our flat world the R2 coordinate so the Cartesian coordinate system so let's say here we got our y AIS here we got our x-axis so this is the X this is the Y and uh here we of course we need to keep in mind this is just an example when it comes to projections we can always go beyond R2 but for keep it simple and truly understand this Concepts and this intuition behind the projection I want to simplify this and do the example in R2 so here uh imagine that we got this line and this is our a line that goes through the center that let's call this line B so B is line in R2 let's say this is that line and now that imagine that we have this Vector which is part of this line let's say this is this line and this line is the representing by uh on this line we got this Vector B and this Vector is basically part of that line as you can see this is the vector B on this line B so we know from this concept of the line spanning the R2 and then vectors we know that in this case independent what is the magnitude of this Vector what is the direction of this Vector we can represent this line B by this linear combination based on this Vector so linear combination of this Vector which is in this case D set set then here we got some C where C is a real number multiplied by this Vector B knowing that this C is just a real number so let's make it actually green so we can basically say that this entire line B can be represented as this set of this linear combinations of these vectors so for instance if this is one and we do the C is equal to two then we can get this part of so we can get this Vector otherwise this is equal to three we can get this vector or C is equal to for this vector and then and so on which means that we can always come up with a linear combination forming a part of this line therefore we are seeing that this line can be represented as all these linear combinations uh of this Vector B which is part of this line and here the C is just a scaler so a number which is a real number so this C * Vector B represents this uh entire line so we will knit this in a bit but for now imagine this line and part of this line which is this Vector B so imagine then that we got yet another Vector which is let's say in here again going from the center but this time in this different direction so in here this is Vector a we call this Vector an A so you can see that this Vector a is actually much longer than the vector B and we see that Vector a is not lying on the same line as B so B is lying on the line b and a is not lying on the line B now let's say we want to project this Vector a onto this Vector B which means that we want to project this a in this direction so we want to bring this Vector a onto this line let me actually use a different color and the word of the projection actually does make sense in here as you might notice because we're trying to cast the shadow of a onto this line of B and how can we do that we can only do that if we connect this Vector a like this with this orthogonal line let's Say by using a different color of this so with this perpendicular line we then will be connecting the vector a to the line B because we want to project our Vector a onto this direction so this perpendicular line that you see in here that goes from Vector a to the line B where on line B we have the vector B so here is the line a line B and this perpendicular line it goes from a to line B and on line V we have the vector B that is represented like this then the projection of a onto Line B is this Shadow Vector that you see in here and the word projection or the name projection actually does make sense because we are projecting this Vector a onto this line and it creates this Shadow so we are casting this Shadow on here and this Vector is what we are referring as projection of vector a onto l line B notice that we don't say projection of B on Vector B but instead we are saying projection of a on the line B then another thing we can notice is that we are getting this projection of a on B so this vector by taking the vector a so Vector a and subtracting from that projection of a on B that is the formula for this Vector that we refer as a perpendicular that goes from a to line B so when drawing this perpendicular line from a to line B we are referring this as a minus projection of a b because you can see that this Vector is simply this Vector minus this Vector that is the um mathematical expression for this perpendicular line so how we can then find out what is this C that we got in here because we understand that to get this exact formula for the projection of a on the line B we need to understand what is the scaler specifically what value are we using to multiply this Vector B to get to this point so what is that c what is C what is C such that c times a is then equal to projection of a on the line B because we can have different sorts of a linear combination of vector B on this line uh B and in fact B this line B is the set of all linear combinations of this Vector B and I want to know specifically what is the vector that we see in here what is the shadow Vector because this is the projection of a on the line B what we see in here now how can we do that well let's first formally Define on this specific case what is the projection of a on this line B so projection of a on line B is some Vector that is also on line B where a minus projection of a on B is per pendicular or ortogonal to this is basically the definition of the projection of a on line B under this specific example so in this case the way we can find this projection is by looking into this C so this is what we are interested this specific specific C * B vector and knowing C and knowing B we already know what is B what B is knowing C we can then describe this specific projection so one thing that we can know is the condition under which we say two vectors are autal that's something that we already have learned as part of the previous lessons so let's go ahead and find that amount so now what we need to do is to calculate this value of C because value of C calculation will then lead us to the exact uh Vector that we are interested in which is this projection so our end goal is to find out what is this projection of a on B this is what we want and for that we need to calculate this C because we already know the vector B so let me quickly remove this part cuz here we will then do our calculation so one thing that we need to make use of is this part when it says orthogonal because we know that if two vectors are orthogonal then they dot product is equal to zero so we know that this Vector is orthogonal to this target Vector which means that we can say that the vector a and then minus projection of a on B multiplied with Vector B that this is equal to zero this is something that we know by definition of orthogonality two vectors are orthogonal it means that their dotproduct is then equal to zero now let's make use of that part so this means that we need to describe this projection of A and B we need to make use of the fact that we know that this projection of a onto B is actually some linear combination of vector B so let me actually go ahead and remove this part we already know the definition so let us go ahead and calculate that c that we need in order to find out what is this entire projection so few things that we need to clear out is those formulas because then we can make use of them to find the C so we know that by definition the projection of a on the line B it is this Vector that we get where we draw this perpendicular line from Vector a onto Line B and we said that this line is equal to this amount this is simply the vector a minus this Vector the shadow Vector which we said it's defined by projection of A and B this thing so we can make use of that because we also see in here that this we are saying isogonal to this Vector so given that this uh Vector a minus projection a b is orthogonal to line B that is also orthogonal on this specific Vector which is the projection itself so from this we can make use of the fact that two vectors when they are autal their dotproduct is equal to zero in order to find this uh value of C so firstly we just set that the a minus projection of a on the line B that this multiplied by this Vector B is equal to zero because those two lines they should be perpendicular but at the same time we know that this is simply the linear combination of this Vector because this line is perpendicular to this one and this line is some linear combin a of this Vector B because if I have here a vector and then I have the longer version of that Vector on the same line which is then a linear combination of this original Vector let's say this is my Vector B then this second Vector that I have in here is then equal to some C * Vector B this is also exactly what we said in here here we said any Vector on line B can be represented as a linear combination of vector B and this is exactly what we are seeing in here so this projection is simply that c times Vector B this is something that we have already said so we are just making use of that to fill in that volum so this then results in a minus this C * B multiplied by this Vector B is equal to zero formula so here we are simply making use of the fact that the projection of a onto B is the shadow Vector which is then equal to some linear combination of this original Vector B which is on this line B then I can easily find the scaler C from here because we know how we can easily calculate this dot product so let us actually go ahead and do that let's first multiply this a by B and then my minus so I'm simply opening the parenthesis C * then I got B by B and this equal to zero so C * B time B is then equal to a and b which means that c is equal to a * B / to B * B now when we have the C we can easily derive the formula for the projection of a on the line B so this is the first part this is the second part so then the projection of a on B so projection of a on B is equal to this c c times the B and we just found out that this is equal to the C was equal to a * B / to B * B and now we need to take this C and then multiply by Vector B this is then the projection of a on B this Vector so projection of a on line B so you will notice that this is the same that we just got so whether you compute the projection of a on the entire line b or projection of a on the specific Vector b as we are using the vector b as a source for drawing our line this is the same as the project rection of vector a on Vector B and this is the same formula as we SE in here so this is the projection formula that we have just uh found out so projection of a onto B is given by this formula a * B so the dot product of the vector A and B divided to the dot product of the bay withd itself and multiply with the vector B and this is the in here this is something that we have calculated time and time again in our examples so if we go back to our example then here we can see that this is our Vector B this is our Vector a and we are saying if we take the vector a and we project it onto this Vector B then we can calculate this Pro projection which is in here the formula for this entire Vector which we are calling projection of a on b or projection of a on B this can be find out so the the length of that Vector we can find by using this formula so the dot product of vector A and B divided to the dotproduct of B with itself and then multiplied with Vector B so again a DOT product produ and this is of course something that we get as a vector so this is a vector something that is equal to this entire Vector in here this Vector so uh I know that this uh might look bit messy because it contains many moving Parts but I wanted to provide this detailed explanation and the step by-step process even if it is bit confusing and bit messy um in the beginning because this help us to understand what this uh formula is about and what is the intuition behind it because what we are doing is that we are making use of the fact that the line can be represented as a linear combination of all the um vectors that we use in here so this is Vector B and this entire line B is a linear combination of this vector B and we can make use of that in order to find that scalar that we are multiplying to create this single linear combination that will end up giving us this Vector that we see in here which is the projection the projection that we are interested which is this line This is the projection that we are defining by this projection a on to B and we can get that by making use of the fact that this this perpendicular line that we are creating in here which is simply the vector a minus this projection this is this Vector this projection Vector that this is perpendicular to this line B and if the vector B is part of this line B this means also that this line a minus projection a is also perpendicular to that vector vector B making use of that formula we can then uh make use use of the product of the two we know that the dot product of two perpendicular vectors is equal to zero making use of that we can then obtain this specific scaler C that we need in order to get the final formula for our projection we are interested in this C because knowing C we can then multiply with this Vector B to get our final projection and we have found that that projection a on B is is defined as the dotproduct of the A and B divided to the dotproduct of the B with the B and multiply with the vector B and this is again a vector now let's look into a couple of numeric examples to clarify this topic and practice with it so given vectors A and vectors B find the projection of a on to B so without looking into answer I will quickly go onto that example itself so Vector a is this Vector 3 4 can also represent this by our more common notation which is three and four and then Vector B is one and zero so let's quickly draw our coordinate system this our xaxis this our y axis and then what is the a the a is three and four three and four so this is our a and what is the B the B is one and zero which which means that our line B is then C times the vector B given that the C is a real number and one thing that you can notice is that the line B is actually our x-axis it is this line this is our line B this is our line l b so the projection is then this line this is our projection because we can know that by drawing a perpendicular line in here from a to the line B we can get then the connection between our Vector a and Vector B and create our projection so this is then the A minus projection of a on line B and this part is then this is then this projection a on B and how we can get this projection well we just learned that the projection of a on B is equal to dotproduct of a with B divide it to dotproduct of B with B itself and multiply it by B this is the formula that we can use and even if you don't remember the formula by heart you can make use of this visualization to figure out what that formula is because we know that if this line is perpendicular to this one then a minus projection of a on B multiplied by this projection a on B should be equal to zero and this projection of a on B is equal to some scalar C multiplied by Vector B that's something that we see in here the first thing we need to do to compute the dot product between a and b a * B is equal to 34 multiplied by 1 0 this is the dot product which is then equal to 3 * 1 + 0 * 4 and this is equal to 3 the next thing we need to to do is to compute the dotproduct between B itself so B * B and what's that that is 1 0 with 1 Z multiplied this is equal to 1 1 * 1 + 0 plus 0 * 0 is equal to 1 then the third thing that we can do then is to obtain the final value which is projection of a on B is then equal to three / 2 1 multiplied by the vector B which is 1 0 which is equal to 3 0 and this actually makes sense visually too as you can see in here this is the tree for the xaxis and here we have the center Z so this projection is then the vector 3 0 so even without calculation we could see just from plotting the uh on the coordinate system the vectors A and B that the projection of a on B will be this Vector 3 but we have followed the formula in order to do calculation step by step which is something that you can see in this answer too so the projection of this Vector a onto B is then this Vector of a length tree in the direction of B so you can see that it is of the length of three so this is the tree on the direction of B so on the line B let's now move ahead and look into a different example but this time we will do the calculation in a quicker way so we got two vectors 4 three and B is equal to 20 and we need to find this projection of a on to B so the first thing we need to do is to calculate the a * B which is equal to 43 multili 2 0 and that's equal to 4 * 2 + 3 * 0 and it's equal to 8 the second thing we need to calculate is the B do product with B which is equal to 2 0 2 0 this is then equal to four and the final part is to take and uh from this one and two this values and then bring them all together so then the projection of a on B is equal to H ided to 4 multiplied by the vector to0 and this is equal 2 8 / 2 4 is 2 2 * 2 is 2 2 * 0 is 0 so we are getting this two Vector so projection of a on B is then this Vector 20 which is actually on this xaxis similar to what we had before only with the length of t uh towards the direction of which is then equal to 4 and0 and this is again similar to what we had before uh where we got the projection of a on B on that end up on the x axis but now with the length of four so now our projection has the following Vector so the uh following magnitude and Direction so this is the step by-step process that I just followed if if you want to do it bit slowly and this is the final result so uh the interpretation of this projection is that this projection a onto B is simply this 4 zero this means that the A's component in the direction of B it spends uh four units along this x-axis that we saw in here because this is the value X this is the value of y so this projection shows us that A's influence in the direction of B is completely horizontal with this magnitude of four because we saw that we end up with the projection on the x-axis again so this was four this was our projection vector and if you plot this entire Vector a and Vector B on this x-axis and y axis then you can clearly see that the uh horizontal line that we end up with the uh projection of a n b is very similar to what we had before in here let's now talk about a concept of auton normal bases so let's now Define what the auton normal bases are so by definition auton normal basis for a vector space is a basis where all vector vors are orthogonal or perpendicular to each other and each Vector is of unit length so as you can notice here here we have a special type of basis it's called auton normal basis because in the beginning of this section of this module we defined formally this concept of bases we talked about the concept of colal uh space and then the uh basis of a comp space the no space the basis of a n space and then we talked about the concept of the bases of the entire space for instance the R2 and now we are defining a special type of bases which we are referring as auton normal bases and this auton normal basis as you can see from this definition it contains two criteria for it to be auton normal so an auton basis for a vectory space is a basis where a all vector are orthogonal or perpendicular to each other and B each Vector is of unit length we already have learned that when we have vectors let's say Vector A and B perpendicular it means that A and B their dot product is equal to zero that's the first criteria that we need for calling our basis an auton normal basis then the second criteria is that each of these vectors they need to have a length of one if we have this condition satisfied then we are saying that our vectors they help us to form this auton normal basis if we got three vectors forming this Vector space it means that we need to have the a * B = to 0 a * C = 0 and then B * C = 0 this is if we are in in case we are using three different vectors that Define our Vector space in this case let me make this part smaller so let's put the length of B in here in this case the second criteria becomes that the length of a is equal to the length of B and then is equal to the length of c and is equal to one so depending on the number of vectors that you use to form your vector space the proof that you are dealing with auton normal bases will be different here we got just two vectors here we got three vectors but in both case we first need to prove that we are dealing with uh vectors Each of which are set of orthogonal perpendicular vectors and all of them pairwise they need to be perpendicular and at the same time the second criteria says that they all need to have a unit length so their length should be equal to one we need this auton normal bases in order to simplify our calculations including the calculations of projections and Transformations that we just so before when we were discussing this concept of projecting a vector onto a line or projecting a vector onto not a vector because we were in this basic case when we had just two vectors in R2 and calculating projection in R2 is very easy because we can make use of this formula um a and then B uh the dot product of them and then divided two product of the B and then times the B this was quite straightforward right but when it came so this is the projection of a on B but when it comes to projection in higher dimensional space let's say you have R5 or you have R 100 or R th000 then it becomes much more difficult to do those projections and to calculate the projections and for those cases we can make use of this concept of auton normal basis to simplify our calculations and we will see that in a bit so let's first understand this orthogonality and the normalization part so orthogonality refers then to the part of uh when we are saying that the vectors should be orthogonal to each other and the normalization refers to the fact uh to the fact that the length should be one this is basically the set of two criteria that I just discussed this is uh the summary slide that will give you an indication what is meant by that so if we have two vectors V and W then we say that the first criteria is that those two vectors are orthogonal which means their dot product is equal to zero and we are saying that their length is equal to one which we are referring as a normalized vector so if the vector has a length of one then we are calling a vector v normalized so if both of this criteria of normalization and orthogonality is satisfied that we are saying that we are dealing with an uton normal basis so now where we have learned this idea of projections also this idea of autog colonization and the uh concept of auton normal basis we are ready to discuss the concept of the grade process so the grade process is this method for orthogonalizing a set of vectors in an inner product space and turning them into an auton normal set so let's say we have a set of vectors we want to uh bring and transform all these vectors onto this auton normal set of vectors which means that we want them to be aized so we want them to be perpendicular and we want them to be normalized because we know that the two criteria were specified right so the first criteria was that we need to have vectors ortogonal hence we are doing orthogonalization and the second criteria was that they need to be normalized because we want the vectors to have length one so we are doing normalization this process of turning this set of vectors onto this uton normal set by using this method of orthogonalization which is something that we are referring as a grme process this is something that we can use in order to simplify later this different sorts of Transformations which we need in order to perform bit more advanced uh Transformations like Matrix uh factorization different decomposition techniques so given this set of linearly independent vectors this process which we are referring as grme process produces this auton normal set that is spinning the same Subspace so we have the same Subspace it's just that we are turning the set of vectors into an auton normal set of vectors that is spanning the same Subspace so the gr Street process step by step looks like something like this so given the vectors A1 A2 up to a n the first thing we need to do is to start with the vector V1 which is equal to our first Vector A1 and first we need to normalize this vector and how we can normalize this Vector well we need to take this vector and we need to divide it to its length so the grme process step by step will look like like something like this so in the first step what we need to do when starting with these vectors of A1 A2 up to a n so in RN we need to First Take the first vector and we need to normalize it and how we can normalize the vector and ensure that its length is equal to this length of P1 well we need to take that vector and we need to divide it to this length because when we take the vector the 1 and we divide it to its length of V1 then we will ensure that the length of that Vector is equal to one we can actually prove that very easily but I won't do it in here uh feel free to go through the process assuming that the length of the vector what what you want to achieve at the end is that the length of a vector v is equal to one this is something that we want to achieve and this normalization process can be done if we find a way to ensure that we uh get this E1 because E1 means that we end up with this Vector y 000000 0 this will be for first Vector so V1 this is E1 so the one is really important here so we want to normalize this Vector View 1 by uh ensuring that we get the E1 so we go from V1 to E1 and the way we do that is that we take the V1 and we divide it to the length of V1 and in this way we get the E1 so the normalized version of P1 is E1 so then for each subsequent Vector a k which means A2 A3 A4 up to a n we need to subtract its projection on all the previously computed orthogonal vectors in this way by using this tab two we are ensuring that all these different each pair wise set of A1 A2 and then A2 A3 Etc they are all orthogonal to each other and we know that this projection is something that we got when we had this two perpendicular lines so we had this Vector we're projecting onto this Vector we got that by finding this perpendicular line and making use of that using this property we are then making use of that in order to see how we can ensure that the subsequent Vector that we have is always perpendicular to this one so let me actually write down what is in this formula so here VK is equal to a minus the sum of all the projections so then we need to normalize the VK to get the EK and then we need to repeat this step two and three for all vectors which means that first here we apply this normalization on the vector A1 so V1 is to A1 and then we get the normalization by getting this E1 so E1 is normalized version and then we need to apply a bit different tactique for our V2 V3 up to VN and then let me actually write down this for this General case so what this processed this the GR let me ensure that I'm not making a typo Schmid process step by step means step number one for vectors A1 A2 A3 dot dot dot a n so we are in the RN then step number one basically says take the V1 and set it equal to this first element V1 this is A1 then what we need to do is to normalize it to get the E1 so normalize normalized V1 to get E1 which is equal to 1 0 0 0 and then dot dot dot zero and the size of this n by one and how we can do that by taking this Vector V1 and divided it to the length of V1 which basically means in this specific case A1 divided to the length of A1 this will then give us our A1 this Vector this is basically what the step one entails then in the step number two we have for each subsequent a where K is just an index referring to whether we are dealing with K is equal to 2 so uh A2 A3 and then dot dot dot a n this is what basically the K is used for to refer to which Vector we are dealing with we need to subtract its projection on all previously computed orthogonal vectors by using this formula so let's actually do a couple of those case to see what is going on for instance for K is equal to 2 so K is equal to 2 and here is the formula by the way so VK VK is equal to AK minus some K is equal to K starts with one and then let me use a different index so I is = to 1 till K minus one and then projection of a k a k on E1 or eii so the EI that we have just computed because every time you are then normalizing and normalizing every time your vectors and then you are uh finding out what is the projection of your vector a onto that EI and then you are substract in that from your vector so what this means in Practical terms when for instance your K is equal to 2 it means that V 2 is equal to a 2 minus sum of I is = 1 and then K is = 2 K - 1 this means this is one projection of a a and then 2 on A1 given that this is one this is simply equal to A2 minus projection of A1 that's normalized version of E1 and then A2 so projection of A2 on E1 and then in the step number three we need to do we need to go from VK to get EK so basically we are ensuring with the step number two the orthogonally uh orthogonality condition and with step number three I me add some space in here so in the step number three step number three we then saying let's normalize normalized this VK that we have just computed in here because we remember that the second criteria after tonality is normalization that the unit or the length of the vector should be equal to one so then VK in this case for K is equal to 2 for K is equal to 2 means that we need to go from V2 to E2 and the way we can do that is by taking the V2 by V2 and then divide it to the length of V2 this will then give us the E2 this is the normalization part and the step number four basically means repeat repat step two entry for all case which means that if we go back so we are done with V2 so we have obtained V2 and then we have obtained normaliz normalized version of V2 by getting this E2 we are ready to come back and do the same for K is equal to three and for K is equal to three in Step number two we got V3 is equal to A3 minus making use of this formula sum overall I is = to 1 K - 1 is = to 2 and then projection of this time a Tre see three k is equal to three and then on E2 actually it says EI let me remove this this otherwise we would have made a mistake this should be I because an I will change per K this is the entire idea we need to re um subtract all the um projections what this basically means is that we need to take A3 and this time given that here we have two instead of one in here we need you have an extra step which means A3 minus and then what this formula basically says this is the sum of the projections of A3 on e i where I goes from one till two so projection of a Tre on a one when K so when I this is the I is equal to one case plus projection of a Tre on a 2 this is the I equal to 2 case this is basically what this summation says this is this element and we have seen this as part of the high school but also the pre-algebra course okay so now when we are clear on how we can calculate the V3 in the step number two for K is equal to 3 we are ready to go onto the step number three and what was step number three the step number three for K is equal to 3 was saying let's take the V3 and normalize it to go from V3 to E3 and how we can do that by taking the V3 and dividing it to the length of V tree to get on to E tree and this cycle goes on and on until we cover all the case so all the vectors so the idea is that we first for our initial step we set the V1 equal to A1 we normalize it then starting from the K is equal to two we don't go first on and on WE autog it by formula in here by using this we can ensure that each of these vectors is then orthogonal to all the other vectors so for K is equal to 2 we ensure that this uh Vector that we get is orthogonal to all the other ones and the case equal to treat that the third Vector isogonal to all the other ones and we are doing that in Step number two so for each case for each K we basically are ensuring that in this case we have an a vector that is orthogonal to all the other vectors in this set and for each Vector we are also normalizing it to satisfy the second criteria because we had these two criterias to create this auton normal set so we are doing this in subsequent uh way so first for K isal to 1 so basically for A1 and then we are doing this for K is equal to 2 so A2 and then until K is equal to n so a n what we are doing every time is that we are obtaining this V1 and then we go from V1 to E1 to normalize it and then here we are getting the V2 here to go to E2 by normalizing it so this basically the step two and step three and then we do this every time up until to the point of obtaining VN and then from VN we go to en n to normalize it so this is the idea of this entire process step by step to start with V1 as part of the step number one and then as part of Step number two for each subsequent Vector a k so K is equal to two obtain the VK and then normalize it for K is equal to 3 obtain the V3 and then normalize it to get E3 up to the point of the last Vector which is a n the vector a n we compute the VN and then we normalize it to get the and this is what this part is which is the St number two that says repeat steps 2 and three for all vectors it means that every time when you increase your K when you go to the next Vector we first compute the V so VK and then you normalize it you get the EK and then you go back to the step number two at three because you then again need to calculate the VK and then EK and then for the next case so this is something that you will see also a lot when you are writing the code for your uh algorithms because in many cases you need to do this reputation of the steps so uh you for one Vector you do something or for one iteration you do process and then you uh go back and do for the next one and for the next one this process is what we are referring by repeat step number two and three for all vectors so let's now look into an example let's apply this grumme process to vectors A1 and A2 where A1 is 1 1 0 and A2 is 101 so let's go ahead and do that so A1 is equal to 1 1 0 A2 is equal to 1 0 1 we want to apply this gret process to create this Aon normal basis for the Subspace that is Pinn by A1 and A2 so now we have this set 1 1 0 and one 1 and what we want is to create an auton noral basis so creating creating or to normal normal basis which CR meet process so here we got only two vectors so obviously it's this and it's a very simplified version of it what was the first step in our case uh in our algorithm it was to set the V1 equal to A1 what we need to do step number one we need to set the V1 equal to A1 and we need to normalize normalize the V1 to get E1 that's what our goal is so let's go ahead and do that V1 is equal to A1 and is equal to 1 1 0er that's our A1 so 1 1 Z and in order to normalize V1 and get the E1 we know that this is equal to V1 / to the length of V1 which is then equal to take the V1 so that is 1 1 0 and then divide it to the length of V1 and you can very quickly see that given V1 is equal to V1 * V1 that's something that we learned in the very beginning of our fundamentals to linear algebra course that the length of V1 is simply the dotproduct between uh V1 and V1 and it's equal to 1 1 0 * 1 1 0 which is equal to 1 + 1 so two so this is then equal to 1 1 0 / 2 which is equal to 1 / 2 1 / to 2 and then 0 this is our E1 so we are done with our step number one because now we have V1 and we got E1 so what was the step number two in the step number two we need to set the k equal to 2 this is the next K so for A2 what we need to do is we want to get V2 and normalize V2 by getting E2 and how can we get that well first let's find what is the V2 well V2 was and using that formula that we saw before which was this formula so it's equal to a k minus and the sum I is equal to 1 K minus one and then projection of a onto EI I so let's take this formula over this is equal to a 2 because K is equal to 2 a a k minus Su and then I is equal to one till K -1 and then K -1 which is equal to basically 1 given that K is equal to 2 and then projection of A2 onto e i and this is equal to A2 minus given that we got k - Y is equal to 1 so the limit for our summation is equal to 1 so this one this means that like before we got just one part as part of our summation so minus and then projection of let me actually keep the same color I want it to be consistent so projection of A2 on d e 1 so you see here the i i is equal to 1 and the limit of the I is K minus one which is equal to 1 so we got here just E1 so we got the V2 formula we can then now calculate it because we know A2 and the A2 is this so one one 1 0 1 but now we got a problem we don't know what this is so let's quickly go and calculate this part so projection of A2 on a 1 and we learned from the projection formula that this is equal to H 2 * E1 / 2 E1 * E1 so the dot product multip by E1 and what is this this is equal to 1 1 multiplied by and what is the E1 E1 we just calculate in here so it is 1 / to 2 1 / to 2 and then zero here / two and then 1 / 2 1 / 2 and then 0 0 multiplied by 1 / 2 1 / to 2 and then Z here multiplied by the same Vector so E1 so this two cancel out this two also cancel out and as you can see we are getting that the projection of A2 on E1 is equal to this vector we can also manually check that actually so let's let's do that so let's see we are not canceling out these vectors and instead we are manually calculating this so here we got 101 ultip by 0.5 and 0.50 this is equal to 1 * 1 / 2 is 1 / 2 0 * 1 / 2 is 0 1 * 0 is 1 so + 1 / 2 this amount is 1/4 + 1/4 this multiplied by the vector 1 / 2 1 / 2 and then 0 in here this is equal to 1 / 2 1 + 1 / 2 is = to 3 / to 2 and then 1 1/4 + 1/4 is equal to 1 / 2 2 multiplied by 1 2 and then 1 2 and then zero what is this amount well those two cancel out so we end up with three * and then 1 / 2 2 and then 1 / 2 and then zero this is then the projection 3 / 2 3 / to two and then zero so let me remove all these calculations and then we can take over the projection value which is 3 / to 2 3 / to 2 and then zero to get our Vector V2 which is equal to 1 - 3 / to 2 0 - 3 / to 2 and then 1 - 0 and this is equal to here it is 1 here it is - 3 / 2 and here is minus and then 1 / 2 2 because 3 / 2 is minus uh it is 1.5 and then 1 - 1.5 is simply minus 0.5 so this is then the vector V2 then what we need to do is to normalize this Vector to get D E2 which is then equal to V2 ided to V2 length which is simply equal to V2 / to V2 * V2 so the dot product and this is equal to let's take the V2 which is -1 / 2 and then - 3 / 2 and then 1 and then divided 2 and this amount let's quickly calculate that it is equal to so the length of V2 is equal to - 1 / 2^ 2 + - 3 / 2^ 2 + 1 this is equal to 1/4 + 9/ to 4 + 1 which is 4 / to 4 and then this is equal to 1 + 9 is 10 10 + 4 is 14 so 14 / 2 4 this is the length of it so 14 / 2 4 so then this is equal to this Vector to this threedimensional Vector min-1 * 14 - 1 / 2 * 14 / 4 is equal to this is 7 so minus 7 / 2 4 and then - 3 / 2 think I just made a mistake here actually so Min - 1 / to 2 so the first element and then ided 24 / 4 is actually actually equal to this multipli by four ided to 14 so you take this element then divide it to this one and we know that a / to B * C / 2 D is equal to a * D and then B * C so we are basically flipping this side this is from pre-algebra and then here this is equal to 2 and then is equal to -1 / 27 then let's do the second one twoo so we got minus 3 / to 2 / to 14 / to 4 is actually equal to - 3 / to 2 * 4 / to 14 and then if we remove this this is then 2 this is 7 this cancel out this equal to - 3 / to 7 - 3 / to 7 and then finally we got 1 / 2 14 / 2 4 which is equal to 4 / 2 14 this equal to 2 / to 7 so 2 / to 7 and this is our A2 and given that we got just two vectors so we have already reached the end of our solution so now when we have already the V1 and the V2 the E1 and the E2 We have basically completed the process of this grummet uh procedure because we have already uh only two vectors that means that we need to have V1 and V2 and then uh E1 and E2 and this is all that you need in case you got two vectors if you have three vectors of course the process will include um the same process of Step number two and three so the V2 and the normalization of it two times for your K is equal to 2 and K is equal to 3 and then if you have more vectors than every time you will have more of the steps but at the end what we want to have is the set of vectors that are orthogonal and at the same time they are normalized in this case we say that this vectors form this oron normal bases now why is this important the applications of orthonormal bases well firstly it simplifies a complex Vector operations and uh this is the basis of many uh more difficult mathematical Concepts uh like foror series or quantum mechanics it's used also um when when it comes to this auton normal basis uh also signal processing and it's a critical uh process in numerical methods especially in machine learning algorithms and in data compression so we will see this process to be used also as part of uh decomposition techniques which is really important when it comes to different algorithms uh whether it's optimization algorithms but also um algorithms that are used for recommender systems for example and those uh Concepts they all come together and we will see later on when we will be discussing the concepts of the compositions and metrics factorization so this Aon normal basis and this grum process they are really foundationally in linear algebra they provide tools for simplifying and also solving these higher dimensional problems efficiently their application include different fields of science engineering demonstrating their versatility and utility let's now talk about the special matrices and their properties so we are going to talk about special matrices like symmetric matrices and their example diagonal matrices and their corresponding example but also the ortogonal matrices with the corresponding example so when it comes to the special matrices special matrices have unique properties such as being symmetric or all nonzero elements on the diagonal like diagonal matrices or orthogonality uh in matrices which means that we have orthogonal matrices so when it comes to the symmetric Matrix it means that uh the a The Matrix a is equal to its transpose the a so a is equal to a in this case we can confirm and say that the Matrix a is symmetric so in this case we have Matrix a and we know that the way we need to to transpose this Matrix is to taking this rows and making them The Columns of our transpose Matrix so a is then equal to 2 - 1 and then zero then the second row which is min -1 and then 2 and then Min -1 and then the third row which is 0 - one and two so the third row then becomes my third column so as you can see those two are the same so I'm using then the definition of the transpose of the um Matrix and then here then we end up with two matrices they are actually the same so we can see that the A and the a in both the First Column they got 2 minus one and then zero the second column minus one 2 and minus one the third column 0 - 1 and two so their columns and their rows they are the same which means that we are dealing with a symmetric Matrix so whenever we want to check whether the Matrix is symmetric we just need to take the transpose of it and see where the the Matrix is equal to its transpose in that case we are dealing with symmetric Matrix do also note therefore for uh Matrix to be symmetric it needs to be a square Matrix so it needs to be 2x two into two dimensional space or 3 by3 in the three dimensional space or n by N in N dimensional space which means that the number of rows should be equal to number of columns because otherwise when you flip your number of rows with number of columns on in case there is no um uh Square version of that Matrix so m is not equal to n in that case a will have a dimension of M by n and then then a t so a t will have a dimension of n by m which means that there is no way that a can be equal to a this is not then possible therefore we need to have a square Matrix for them to be symmetric let's now talk about diagonal matrix so a diagonal Matrix has a nonzero element only on its diagonal which means that in this case we have this nonzero elements on the diagonal so let's call it d11 d22 and then d33 this equal to three this equal to 5 this equal to 7 and all the other elements as you can see in here they are zeros so the concept of diagonal matrices is very uh simple therefore we will then go through the next example which is about orthogonal Matrix now this is a concept that we haven't yet seen and we spoken about so let's Cod read through this bit slowly so an orthogonal Matrix is a square Matrix whose columns and rows are orthogonal unit vectors so oron normal vectors and its transpose equals its inverse so there are two two part of this elements so firstly it says that for the Matrix to be octogonal Matrix it should be a square Matrix so Square Matrix and then its columns and rows are orthogonal unit vectors so columns and rows are orthogonal unit vectors which means they need to be normalized so normalized so um we have seen when forming this orthonormal basis that we had this process of uh this condition of orthogonality the vectors had to be AAL and they had to have a length of one which means that they they had to be normal I we can see exactly the same in here so hence the name oron normal vectors so they are utal and they are unit vectors which means they are normalized so then the final condition is added in here which actually is not so much a condition but rather than a property something that we can prove that once we have all this we can also say that if we are dealing with toal Matrix then Q T * Q so the dotproduct of the transpose with that Matrix Q is equal to the Q * QT is equal to I why because the QT is equal to the Q minus one because the transpose of that Matrix Q is actually equal to its inverse and given that we learned that the Q minus one so the inverse time Q is = to Q inverse * Q is equal to I and given that here we are learning that Q T is = to Q minus one we are then making use of this to claim this so instead of minus On's that we are used to when we are dealing with inverses here we have t the transpose so in this case this orthogonal Matrix that we have just learned about this is the square Matrix whose columns and rows are orthogonal and they are also normalized meaning that we are dealing with QT QT sh Q minus one it will look like this so this q1 you can see that here we got the first row here we got the second row and if we calculate the dot product between this row and this row we can quickly see that we are getting a value of zero so we can prove that they are actually autogo those two rows let's go ahead and actually prove that so let's call this R1 let's call this R2 this is Row one and row two and I will leave the uh column version so q1 * Q2 that's do product on you to prove that the columns are perpendicular I will work with the rows so R1 * R2 for me to prove that they are orthogonal I need to prove that this equal to zero can we do that well let's try so 1 / 2 < of 2 1 / 2 of 2 ultied by 1 / 2 < of 2 needs some bigger space in here so 1 / 2 of 2 and then minus 1 / 2 of 2 that's how I can calculate the dot product between R1 and R2 R2 and R1 you can see that the elements in here are the same and here the elements are also the same so then this is equal to 1 / 2 < of 2 * 1 / 2 of 2 - 1 / to of 2 * 1 / 2 of 2 I'm simply taking this minus and given that the dotproduct is basically Plus and then this amount I'm just taking this and bringing up in here to a avoid one more step uh given the space is quite limited now what do we see in here this value is the same as this value which means that this is equal to zero and we know that the two vectors to be or token they need to have a DOT product equal to zero so here we have proven that dotproduct of R1 and R2 is equal to zero so this proves that R1 and R2 so the two rows of this Matrix so R1 and R2 are orogo we can also prove that the second criteria of auton normal vectors is also satisfied in here we can prove that when we look at the length of this vector and of this one then they are of the unit one so let's actually go ahead and do for one of them so let's prove that for 1 / 2 of 2 and then 1 / 2 Ro of 2 this is a vector that the length of it this is let's say our first row so this is R1 then the R1 length is equal to 1 / 2 of 2 2 + 1 / 2 of 2 2 this is equal to 1 / 2 + 1 / 2 and what is 1 / 2 + 1 / 2 it's equal to 1 so we have proven that the length of R1 is equal to 1 you can quickly and easily also compute that for the second row and you will then also prove that the R2 the length of it is also one which is then the second criter area which said that for the vectors to form this auton normal bases so to be auton normal vectors they uh also had to have a length of one so they had to be a unit vectors in this case then we can make use of the property that the Q 2 transpose is equal to Q inverse and this then result in Q2 transpose * Q2 Q2 which is equal to Q2 * Q2 transpose which is equal to the identity Matrix and specifically I2 because we are in the R2 so both this Q2 and the previous example those are orthogonal matrices and in here we have proven that the rows are indeed oroginal and we have also seen that the length of them are unit vectors meaning that we have automatically got this I will leave this one to you to do those proofs so to uh ensure that the row one and row two are orthogonal so they are perpendicular which means that the product of their uh the dot product of these two vectors is equal to zero and also that they are normalized which means the length of them is equal to one and this means that then this holds you can actually even go ahead and uh practice the material that we are uh learned as part of the previous units by calculating the inverse of this Matrix and checking that the inverse of this Matrix is indeed equal to the transpose of the Matrix so that QT is equal to Q minus 1 because we learned how we can compute the inverse of a matrix because the inverse of a matrix was equal to 1 / the determinant of this Matrix times and then the manipulated version of it which was in this case 0 0 and then we need to have here 1 so -1 * 1 and then 1 * - one so we have to multiply this and this by minus so one and then here minus one so in this way you can also prove that this inverse is actually equal to the Q to transpose because then you can prove that indeed and you can see for yourself that this formula is in equal to the Q2 transpose because then you can prove that indeed and you can see for yourself that this formula is indeed true in this module we are going to talk about Matrix factorization we are going to discuss the significance of Matrix factorization we are going to Define Matrix factorization we are going also to discuss the common applications of Matrix factorization across different fields and then we are going to see detailed examples of metrix factorization so let's talk about why Matrix factorization matters so metrix factorization techniques they are essential for various reasons they are used for simplifying metrix operations like solving linear systems or when we have this um many uh matrices but we want to to um simplify these operations that we apply to these matrices and we want to solve the problem then we can make this uh complex Matrix operations more manageable and make these uh calculations more manageable by using Matrix factorization techniques we can also use Matrix factorization directly to solve system all linear equations efficiently we can also use Matrix factorization to perform igon value de composition singular value de de composition or called SVD and other operations which are crucial in machine learning and data analysis so ion values and igon vectors you might have heard already they are part of also PCA which is the principal component analysis and this comes from uh fundamentals of statistics and the uh PCA is used as a dimensionality technique and in fact it's one of the most popular damage s techniques that you will find in the industry used in the data science using data analytics machine learning even in the Deep learning so Matrix factorization can also be used to reduce the computational complexity by making use of this factorization we can then simplify the process and also make it more efficient for computation and it's especially important when we are dealing with this High dimensional data when we have many features or we have a very large model and complex model then this uh meing factorization technique can make a huge difference in our data processing process so this techniques underpin many algorithms in numeric analysis in optimizations and Beyond so whenever it comes to machine learning or data science or many other fields you will see this uh process and this term metrix authorization appearing a lot even um in the example of a streaming company Netflix which I'm sure that you are aware of netrix is using uh metrix uh factorization to build a recommender system and uh metrix authorization usage in building recommender algorithm for personalized recommendations is actually one of the most popular applications of metric factorization therefore I wanted to specifically discuss this topic as part of our Advanced linear algebra course and some of the concepts might seem bit more complex than the ones that we have discussed as part of the previous units but once we go through them step by step and I will give you all the details in all these examples this entire process of this different metrix factorization techniques should become much more clear and straightforward so we will be discussing not just one but multiple fundamental metrix factorization techniques beside of talking the high level where they are used and how you can choose for what type of applications so we are going to demes this entire concept of metrix factorization and we are going to uh start from high level then we are going to go into the deepest details let's now formally Define the metrix factorization so metrix factorization refers to decomposing a matrix into product of two or more matrices revealing its structure and simplifying further analysis so what is this idea behind metric factorization the idea is that if we have a matrix a and we want to simplify our process of calculation or multiplication anything that's related to this a but this a in itself it contains this weird numbers or it is just too complex you know it contains this ton of different numbers you don't recognize whether the columns are linearly independent it's not very readable from the first View and you just want to make your life easier when performing this calculations well for that you you can make use of this Matrix factorization to write this a in terms of some other matrices let's say um and I'm calling here randomly Q or t so it's equal to for instance the dotproduct of these two matrices Q * T where Q is much simpler and the t is also much simpler so those may contain vectors that are um for instance this can be a diagonal matrix or it can be a matrix uh with specific properties when using those you will feel much more comfortable so it will be easier for you to use them in order to multiply uh with other matri matrices it can be easier for you to solve this problem but of course if you are in the two-dimensional space let's say you are in R2 or in R3 then most likely it will be quite straightforward for you to use the a itself but if you are in the r 100 or R 1,000 then of course this uh entire computations they become super complex it will be difficult to understand and compute this linear combinations find out whether you are dealing with a linearly independent columns find out um the um new space the calm space the basis of the new space and calm space and all these they might seem uh much more difficult when you are in high dimensional space for in those cases we can then make use of metric factorization to make the entire process much more simplified and also more efficient this entire calculation process so common types of Matrix factorization include lower upper uh Matrix factorization or in short L QR factorization an Infamous type of factorization which is called orthogonal triangular factorization and then we have SVD singular value de compos ition yet another in famous metrix factorization and then finally the igon de composition also another Super popular metrix factorization technique so uh the QR SVD and igod composition are in fact highly popular the composition and Metric factorization techniques that you will see appearing in the 90% of all the statistics related and machine learning related ated books so this just comes to prove how important these concepts are when it comes to properly learning and mastering these more applied uh science related fields like machine learn so if you want to go beyond the level of knowing algorithms but rather than to also be able to edit the algorithms tweak them adjust them be able to understand machine learning algorithms deep learning algorithms data science and at its core and in order to become a professional well-rounded professional then this I composition the singular valid composition and the QR metrix authorization techniques are techniques that you want to know and you want to understand at least higher level such that you can easier grasp more advanced concepts that come from the applied sciences like machine learning and AI so let's first discuss at high level what this C q r DEC composition is so what the QR DEC composition does is that it decomposes a matrix into an orthogonal Matrix which we are referring by q and then an upper triangular Matrix R so in here you can see that we have this two different matrices so we are basically saying a is equal to this product of this Matrix q and r R where the first one this Matrix Q this one should be orthogonal Matrix so this part is really important and we have learned as part of the previous module the definition of orthogonal Matrix we learned that the rows or columns they had to be orthogonal to each other and we also learned that they need to have a length of one they need to um be normalized and we learned that this means that the transpose of those matrices is equal to the inverse of the matrices so this was just the last part of the previous module and this is exactly what this Matrix Q is about so we are saying that we will decompose I into this two matrices as a product of these two matrices Q andr one of which this Matrix Q should uh be uh an autal Matrix which means the rows and the columns they should be or toal to each other so their uh dot product each of them should be equal to zero and they need to be normalized so the length of them should be one for each of those rows and vectors and then the second part of this U the composition is this Matrix R which says that the Matrix R should be an upper triangular Matrix and what is the definition of upper triangular well in this case you can think of this r as this Matrix where we have here all zeros and then here on the diagonal you have numbers nonzero numbers and then here let's say 1 2 3 4 5 and then here in the upper part you will also have numbers so unlike in the lower part of this Matrix R where you will have zeros in here you will have also noner numbers numbers let's say seven uh 10 uh 8 and I'm just writing these numbers randomly so of course in a real case when we have this Matrix a and we go through this process of QR de composition of course we will have an appropriate q and appropriate R where these numbers will be different and they will be specific numbers that we will be calculating but the idea is that we need to get this upper triangular Matrix R for this calculation to make sense so we will then be using this qard composition for solving linear um linear Le squares problems for instance which is part of the linear regression too because linear regression from machine learning and from statistics uh it is based on the least Square technique the estimation technique that we are using for linear regression in machine learning um to solve this linear regression problem is called Ordinary leas squares so the algorithm is based on this idea of Le squares which is trying to minimize a squared uh residuales of the morel and that can be done by using this idea of QR decomposition so it helps us to provide numerically stable solutions for this type of problems too and C de composition is used extensively in Signal processing and statistical analysis let's now briefly talk about the L decomposition so L decomposition decomposes a matrix into lower triangular Matrix so this is the opposite of what we had before we can have an upper triangular triangular Matrix like we had in the QR the composition we can also have a lower triangular Matrix so you might have already guessed how it will look like I want go into that very soon in the QR composition example you will see the idea of the upper triangular I will also show the idea of a lower triang so the composition in case of Lu uh is done by decomposing a matrix into lower triangular Matrix l and an upper triangular Matrix U so basically the difference between the QR de composition and L de composition is that in the QR DEC composition we are decomposing a matrix into orthogonal Matrix and an upper triangular Matrix while in case of L DEC composition we are decomposing a matrix into lower triangular Matrix and an upper triangular Matrix so here you can see that we no longer have this idea of orthogonal matrix but instead of that we are talking about lower triangle Matrix so in that aspect uh L U the composition is different from QR DEC composition so what the L de composition does is that it facilitates the solving of linear equations and Matrix inversions it is common in engineering and in physical sciences for systems of this linear equation to be solved by using lud de composition and in fact if you are learning Quantum uh mechanics that this L decomposition can definitely help you to better understand many Concepts but if your target fields are machine learning deep learning or artificial intelligence then for those using QR composition will be uh much more often a case than using this lud de composition let's now talk about the singular value decomposition so what the SVD does is that it decomposes a matrix into three matrices so first one is the orthogonal Matrix C the second one is a diagonal matrix and then the third one is this V which is the conjugate transpose of an orthogonal Matrix so for now this might s bit complex and you can see that unlike the QR or Lu de composition where we got just uh two uh matrices as a result of our decomposition in case of SD we got three matrices like the name suggests two by the way so three parts and this might seem bit complex but we are going to go through this process step by step and I'm going to provide you detailed example such that this will all make sense but for now let's focus at the high level usage of SVD so singular value decomposition is one of the most popular decomposition techniques and it is also Direct Al used as part of machine learning algorithms to form uh those machine learning algorithms it is also used in the data compression in the noise reduction so when we are trying to clean our data and remove the Noise by using SVD because SVD can help us to identify those outliers and then remove them from the data by performing noise reduction and it is also used in the principal component analysis the PCA the uh same dimensionality reduction technique that I just uh mentioned related to the ion de composition because SVD and the igon de composition are highly related to each other so this SVD is used as part of this PCA algorithm and PCA is the most popular the infamous dimensionality reduction technique that is used both in the advanced statistical studies in the statistics in general also in finance and is also used as part of many machine learning and deep learning applications so knowing PCA is a must if you want to get into uh data analytics or data science machine learning and AI but also uh it helped you it will help you also to uh understand uh many other Concepts when it comes to this Fields so the SVD provides insight into the structure but also the rank of the Matrix so we are going to see this as part of our example too let's now also briefly talk about the igen dec composition so igen DEC composition which is highly related to this concept of igen values and ion vectors it decomposes a matrix into this ion values and ion vectors which then shows the matrix's fundamental properties which are related to this idea of correlation what kind of information does this uh Matrix contain what is the variation in what direction is the variation the largest and this icon that composite which is then related to also this idea of SVD and in general this dimensions and correlations is critical for understanding linear Transformations the stability analysis and systems of differential equations but beside this uh mathematical side of uh Concepts and understanding these mathematical topics the ion de composition is also the basis for many algorithms in numerical deor algebra uh but also many applied linear algebra topics like in the data science in machine learning and it's used heavily in artificial intelligence for feature extraction for dimensionality reduction related again to the concept of PCA because PCA is based entirely on this concept of IG de composition PCA is the direct result of computing the igon values and igon vectors so without knowing what are Dion values and ion vectors you cannot perform PCA because the first step of the PCA is the computation of the icon values and icon vectors and then using different rules which we are referring uh as the elbow rule or Kaiser rule we can then use this icon values and icon vectors to understand what are the features in our data that contain the most variation so the most information and then we can use that in order to understand what are the most important features in our data and reduce the dimension of our model by selecting this most important features because what PCA basically does is that it it uses these icon values and icon vectors to understand how we can uh create a linear combination out of our features and understand the the amount of those linear combinations that contain the most variation and then select those and uh select the largest amount of information in the data and then Skip and drop those uninformative being your combinations while still keeping the most information and this definition of the most will then be decided by this differentials this is just higher level Insight background information on what you can expect when you are talking about applying this highly technical linear algebra concept of Icon de composition into an applied science Fields like data science or machine learning or AI but we will see this later and I'll also make comments regarding this and though PCA won't be discussed as part of this course because here we are talking about linear algebra but PCA is part of the fundamentals statistics course and in there we are no longer providing all these different details on how you can uh perform this I composition therefore knowing how to perform I composition will then set you for success to actually understand the mathematics behind the statistical Concepts like PCA and also later on understand how you can use that PCA in a machine learning Concepts and in AI Concepts like outo encoders and how you can relate your um out to encoders to this concept of PCA how they are related what are their commonalities and what are their differences so everything is about the choice and choosing the right tool for your problem when it comes to the decomposition tools metrix authorization tools we have seen that there are many options and the question is which one should we pick in what cases so choosing the right tool is really important when it comes to this different metrix factorization techniques because they are many choices and each of them they can be used for different sorts of problems so therefore in order to understand which one you need to pick in what kind of cases what kind of requirements you have and what kind of solve uh problem you are trying to solve that in those cases you will need to have this knowledge that you will learn as part of this course in order to make that right choice of the twool so the choice among Q are the composition the L de composition the SVD and IG de composition it really depends on your specific problems requirements and the data characteristics so are you dealing with a complex data are you dealing with a simple data with low Dimensions what is the goal that you uh want to uh achieve what is the problem that you are trying to solve is it to uh reduce the dimension of your feature space is it to solve a problem with linear equations is it to solve a quantum mechanics problem or is it to um incorporate this as part of your machine learning algorithm so QR and LU DEC compositions are usually preferred for solving linear systems while SVD and I Anda compositions they help us for deeper insights when it comes to the data and what kind of information it contains how we can reduce the dimension of the data or how we can use it as part of machine learning algorithm for uh noise reduction identifying outliers Etc so um this type of algorithms like SVD and ion de composition it helps us to also uh intuitively using geometry and our knowledge of geometry to um visualize the data for instance the PCA helps us to visualize this High dimensional data using just couple of principal components let's say we have 10 features in our model so we have a dimension of 10 we are in r10 but we want to visualize our data by using PCA we can then reduce the dimension and come up with uh three principal components which are a linear combination of our original 10 vectors and then we can use the three principal components to visualize our data in 3D and this basically helps us to geometrically visualize our data and then make presentation s make much more sense of our story so to do uh storytelling for our data and uh much more and these two uh models and tools they are invaluable when it comes to uh applications in machine learning in deep learning in data science and artificial intelligence so meing factorization techniques they are super important when it comes to computational mathematics they are also directly affecting the data science Ai and many other algorithms so they are not only important in terms of the problem that they are trying to solve but also in order to make the computation process so when coding in python or in other programming languages to make that process much more efficient they also help us to uh make these computations efficient and provide insights into different properties that we have in our data as part of this course we are not only going to discuss one but actually three of these four the composition techniques and this metrix factorization techniques in detail we are going to talk about the qard de composition we are going to not just discuss it but uh also to learn it step by step and we are going to do a detail example with all the steps involved such that you will feel confident doing a QR decomposition all by yourself then we are also going to do an SVD de comp position as well as ion the composition and then we are again going to discuss them in terms of their mathematical formulation the definition but also the application step-by-step process and a detailed example such that you can conduct each of those metric factorization techniques and these decomposition techniques by yourself manually doing all these calculations this understanding and this examples and this Concepts will help you to not just be able to formulate what these techniques are about but really and truly understand and then use them later on whether when doing your own research writing scientific papers or tweaking the algorithm all by yourself when inventing new algorithms I won't be discussing this L de composition technique because we already know uh that the QR and LU they are both used for similar type of problems therefore to save us time I have selected carefully the uh most important the composition techniques and Metric factorization techniques that you will most likely be dealing with will be dealing with in your future career in applied sciences 
hi and welcome to math for machine learning 
and data science Concepts like vectors matrices gradients optimization priority distributions p 
values occur a lot in machine learning When you learn about these concepts you gain a deeper 
understanding of how and why machine learning algorithms work And when you understand the math 
behind machine learning you be able to better go beyond just using out ofthe-box machine learning 
algorithms to build and customize the models more And you also be better able to judge when to 
apply which technique through learning the math of machine learning You also become more effective 
at debugging machine learning algorithms When I train a machine learning algorithm it pretty much 
never works the first time or the first few times   I run it And so the math has frequently helped me 
and I'm sure hope you too make better decisions about how to efficiently improve the performance 
of your machine learning algorithms And who knows maybe with the mathematical foundation that you 
gain from the specialization I hope that you might   someday use that skill to even invent new machine 
learning algorithms I'm thrilled to introduce to you the instructor for this specialization Luis 
Serrano When Deeplang.ai set out to create this specialization I hoped to find an instructor who 
could bring math to life with visual examples that convey the intuition behind these math concepts 
We spoke with many people to try to find the best instructor for these concepts and I was thrilled 
when we found Lewis who turned out to be a fantastic fit Lewis is a machine learning engineer 
researcher and an educator with a PhD in pure math He was a machine learning engineer at Google where 
he worked on the YouTube recommended system and he was also lead AI educator at Apple Lewis also 
holds a popular YouTube channel called Serrano Academy where he puts together math and machine 
learning concepts in a really illustrative really delightful visual style He's also the author of 
a best-selling book rocking machine learning with Lewis teaching the specialization um confidence 
you're in good hands and you enjoy learning these   materials about math for machine learning and 
data science So I'm thrilled to have you with us Louis Well thank you very much Andrew It's uh 
great to be here And it's actually funny because   it was about 10 years ago that I started hearing 
the term machine learning and I remember got my computer and Google searched uh introduction to 
machine learning courses I thought it was going to   be a very complicated thing but I looked and the 
first hit was your class So I started taking your class and uh I was blown away by how how wonderful 
and simple it was So thank you for that And for me it's very interesting that 10 years later I'm on 
the other side of the camera talking to you about   machine learning courses So it's it's great to be 
here Thank you No thank you I'm so glad to have you here And in fact uh I did not know that story 
but actually makes me reflect you know when you're   watching this um learning math from Lewis Maybe 
a few years from now maybe 10 years maybe even faster If you find it in you to teach something 
online I think that could be very cool too Yeah You'll be on this side of the camera We have 
another chair So this specialization has three   courses and the first one uh which is just four 
weeks long is on linear algebra Do you say a bit more about what the first course of the three is 
about yes So the first course is linear algebra   and in this course we cover anything regarding 
vectors matrices linear transformations systems of equations determinants etc Uh but we like to 
see it in a in a different light So for example a matrix can be seen as an array of numbers but 
that's like seeing a a book as an array of letters   You know books have a lot of interesting stories 
and can take you to place in your mind and matrix are the same They can be seen in much much deeper 
ways Yeah In fact um one thing I've heard you say is I think that um if you look at you know how 
a neuronet network or some learning algorithm   manipulates data a lot of a neuronet network is 
built on top of matrix operations to take the data set rotate it turn it warp it um and multiple 
times with multiple layers of a neural network So that eventually you get out some answer like 
predicting you know is this a counter and that's   what neural networks do right they're just a bunch 
of matrices with activations in the middle that that warp space Yeah So I think you know given 
that so many um of the most important learning algorithms including neuronet networks but many 
others are built on top of matrix operations   gaining that deeper intuition about how it all 
works will give you a better sense of how neuronet networks and many other learning algorithms 
actually you know do the magic that they   do Exactly Um and then the second course is on 
calculus Uh do you say a bit more about that yes So calculus is very important in machine learning 
for many things but one that's fundamental is   maximizing and minimizing functions Yeah In fact 
vast majority of learning algorithms are created by creating some cost function and then minimizing 
it So knowing how to do that is fundamental to a lot of learning algorithms And so I found that 
um I've actually found that when I'm using a   minimization algorithm you be a gradient descent 
or some more advanced algorithm like Adam I think that if you can picture in your head what's the 
derivative calculation doing than if it isn't working as well as you want I find that I'm better 
able to tune the algorithm to make it do a better   job solving this critical minimization task Yeah 
absolutely Knowing a lot about the calculus and the and the derivatives can really help demystify 
these optimizers that at first may look obscure And then um finally the third course of the 
specialization is um probably in statistics and even things like hypothesis testing and p value So 
tell tell us more about the third course Yeah the third course is very interesting because a lot 
of machine learning happens to be probabilities right many times uh model outputs a probability 
and probability can be used to train models For example maximum likelihood estimation is very 
important In maximum likelihood estimation what you want to do is let's say you have some evidence 
and you want to find the scenario that most likely   generated this evidence That's machine learning 
Your evidence is your data and you want to find the model that most likely generated this data 
that you want to maximize the probability that the model generated that data So sometimes I've 
said um when it comes to math I sometimes said don't worry about it and I actually stand by that 
uh when you're learning machine learning for the   first time to get it to work sometimes you don't 
need to worry about the intricacies of exactly how the math works but um as Louis was saying to get 
to that next level of expertise when you can then start to gain that deeper understanding of that 
math as well you can then learn a better deeper mastery of the algorithms that you're using and 
building and then one exciting element of this specialization is you know learners don't just 
learn the math also get to see it in uh Python code and see it run Do you want to say more about 
that yes that's absolutely right You will be able   to put these algorithms in practice and for that 
we have a bunch of Python labs So we assume some basic knowledge in Python to be able to run 
them but we're also going to have the resources for you to get up to speed Awesome So um this 
specialization will go from you know high school not very advanced level of math all the way up to 
the core concepts of math for machine learning and data science And so with that um I'm excited 
to have you jump in to this specialization So please go on to the next video to dive into linear 
algebra You are now starting the first course in the series of math for machine learning Welcome 
We're very excited to have you on this journey In this course you will learn to translate real world 
scenarios into practical applications using linear algebra In the first week you will construct 
systems of equations from real life examples and identify the matrix representation corresponding 
to these systems From there you will determine some important concepts such as singularity 
and linear independence You will also learn to calculate the determinant and you'll learn some 
useful row operations for simplifying matrices By the end of week two you will be comfortable 
solving 2x two and 3x3 systems of equations and you'll be familiar with concepts such as the 
rank of a matrix In week three we will zoom into the depth of the simplest representation of 
a grid of values the vector and how matrices can transform one vector into another And in week 
four you will learn some more advanced concepts such as values and vectors you might encounter 
working with vectors in your life without even realizing it So follow me along in the next video 
to get started In this course you will encounter many useful applications of vectors matrices 
and mathematical properties of each useful in extracting information about systems of equations 
To get the most out of this course we highly recommend that you are familiar with solving a 
simple equation with one unknown variable and that you know how to construct simple plots in the 
coordinate system An example of an equation in one variable is say 2x + 7 = 17 From there you can 
work out that 2x = 10 and therefore that x = 5 An example of a plot is the function y= x which if 
you plot in the plane it's a diagonal starting at the origin and going up 45° If you need to review 
some of these concepts don't worry about it We have some recommendations in the resources 
section For example you can check out Khan   Academyy's course on introduction to algebra 
and then come back join us in this course If you already feel comfortable with basic algebra 
then let us get started Let's talk quickly about programming experience This course is designed to 
both provide you a theoretical background in the mathematics underlying machine learning and show 
you how those concepts are applied in practice That means you'll need to do some programming This 
course includes graded programming assignments   and ungraded programming labs that focus on 
the application of the skills and concepts you're learning These exercises are written in 
Python and will appear as Jupyter notebooks an   interactive web-based interface that allows 
you to read run and edit those programs You won't need to be an expert Python programmer to 
be successful in the exercises but you should be   comfortable with the concepts that would typically 
be taught in an introductory Python programming course This includes things like different 
data types and data structures control flow   using conditional statements loops and functions 
as well as importing and using different Python libraries You should be comfortable reading and 
editing Python code using these concepts writing   and debugging your own code and occasionally 
referring to the documentation of new packages If you are a confident programmer in another 
programming language you should be just fine   learning the Python you need for this course 
as you go If you're new to programming however I would recommend you take an introductory 
Python course before you begin this course   In the next reading item you will find some great 
resources for where to get started learning Python Welcome to week one of the linear algebra course 
This week you will learn what a system of linear equations is and some of its representations 
One of them is as a set of lines in the plane and another one is as an array of numbers called 
a matrix Furthermore you will learn what singular or non-s singular systems are and why is this 
so important in linear algebra Linear algebra has applications in many fields of science and 
technology and machine learning is certainly not the exception In fact it is not a stretch 
to say that linear algebra is the most useful and widespread math field in machine learning In 
this video you will learn about some of the best applications of linear algebra in machine learning 
Starting from the most popular application that   you know by now linear regression This course is 
first and foremost a math course but I'm guessing you're here because you're interested in further 
studies in machine learning You may have already   explored some machine learning topics or maybe 
even taking some machine learning courses If you haven't yet studied machine learning I can 
recommend the machine learning specialization   from deep learning AI to provide you with great 
foundation in the field as a complement to this specialization Throughout these courses I'll be 
using machine learning examples to provide context   for the math you're studying and how these ideas 
are applied in practice In these machine learning courses sometimes Andrew says "Don't worry about 
the math." In this course I'll sometimes do the opposite and say "Don't worry about the machine 
learning." You'll see many examples of machine   learning techniques but if you don't understand 
all the details of how they work that's okay The goal is to build a strong mathematics 
foundation continue to build your interest   in machine learning topics and prepare 
you for further courses in machine learning With that approach in mind let me 
introduce this week's main topic systems   of linear equations using an example from 
machine learning A common machine learning approach to modeling systems is called 
linear regression Linear regression is   a supervised machine learning approach which 
means you've already collected data on many inputs and an output and your goal is to 
discover the relationships between them So for example suppose you want to predict the 
electrical power output from a wind turbine If you had just one feature like in this case 
wind speed shown here on the xaxis which is the horizontal axis and you plot your target of 
power output on the y-axis which is the vertical axis Then the data points here are representing 
real measurements of wind speed and power output Clearly there's a pattern And the goal of linear 
regression would be to find the line of best fit   for this data For example this one With a model 
like this you're making the assumption that this relationship is literally linear that it can be 
modeled by a line In other words that if you know the wind speed you can multiply it by a constant 
add a second constant and make a reasonable   estimate of the power output from the wind turbine 
So for example with this model I can say that if the wind is blowing at 5 m/s then I predict the 
power output from the wind turbine is going to be 1,500 kow Now this model is not perfect You 
can see the actual data scattered about the line representing the model but it's doing a reasonable 
job The model here is the familiar linear equation   y= mx plus b where y is the power output and 
x is the wind speed Your goal is to find the best values for M and B that fit the data In 
machine learning you'll often see the equation of this model written as Y= WX + B because the 
number multiplied by X is called a weight The B is called a bias And so luckily doesn't need 
to change Now linear regression with just one feature like this is easy to visualize but in many 
machine learning problems you'll be considering   more features In the case of predicting power 
output from a wind turbine you may want to include not only wind speed but temperature as 
well In order to account for the new input the equation of your line would need to change Now y 
= w1 * windspeed plus w2 * temperature + b with a new weight added for the second input variable If 
you graph this equation it no longer would form a line Instead it would be graphed as a plane in 
three-dimensional space But what if you wanted to consider more features like pressure humidity 
or anything else that might affect the performance of the wind turbine the idea is exactly the same 
as with one or two features You simply add a new weight for each new feature Even as the equation 
of this model gets longer conceptually it works the same By finding the right values for the 
weight and the bias terms it should be possible to make accurate predictions of the output or 
target under the assumption that this is a linear relationship If we write that out explicitly 
then you have w1 * x1 + w2 * x2 and again for as many features as you have up to wn * xn Then 
you add b and set that all equal to y your target If you imagine this equation as being one row 
in a data set then you already know the values   of the x's and the y's And your goal is to find 
the values of the w's and b to make this equation true Of course in reality you have many records 
in your data set So you have many equations you could write down like this one for each record 
in your data So if you add a superscript one with parenthesis up here on everything in this 
first equation I wrote down then I can write   down the same thing for the second example in the 
data set and denote that with a superscript 2 in parenthesis like this and then so on down to the 
superscript m in parenthesis for the last example in a data set containing m records Notice that 
these red superscripts are not exponents They simply act like subscripts except that we put them 
on top for clarity Ideally you'd find the values of the weight and bias terms that solve all these 
equations at the same time or at least get as   close as possible And so from this very common 
machine learning model appears a fundamental concept of linear algebra called a system of 
linear equations This will be a fundamental   topic that you'll study this week Join me in the 
next video to take a closer look at systems of linear equations and test your knowledge of linear 
algebra before we dive into the week's materials In the previous video you looked at a linear 
regression scenario for the problem of predicting   electrical power outputs from a wind turbine In 
this case you had a data set containing a series of features things like wind speed temperature 
atmospheric pressure humidity and so on And I call this x1 x2 and so on up to xn for a data set 
with n features And then I added a superscript to the data set to denote which row of data a set of 
features belong to Then you had the model weights multiplying it feature which we wrote as W1 W2 
on up to WN And then we also added a bias term B and we set that equal to the target Y which in 
this case is power output from the wind turbine An important thing to note about this system is that 
while the X's and Y's are unique in each row So all of these X supererscript ones are different 
from these X supererscripts 2 and so on And all this y superscript 1 is different from this y 
superscript 2 on down to y superscript m The w values and b are all the same across all rows So 
again what you're saying here with a linear model is that there exists some set of values w1 w2 and 
so on up to wn as well as some value b that when multiplied by any of these rows of features and 
add it up like this will be able to provide you with an estimate of your target y for that row In 
other words with this model you are saying "Give me a set of X's and I can estimate a value for 
Y because I have a model that tells me what all the W's and B's are." And instead of writing this 
model out in long form like this I can instead say I have a vector of weights called W that is made 
up of W1 W2 and so on And I multiply that by each row of features X in my matrix of features 
which I now call capital X And then I add a bias term and set that all equal to y which is 
a vector of my target variable And just like that we're back to a nice simple equation that 
looks just like the equation of a line I just jumped ahead a lot so don't worry if you're 
not already familiar with the terms vector   and matrix For now it's fine to think about 
them as just lists of numbers or grids of numbers Linear algebra is all about manipulating 
vectors and matrices to do powerful calculations And as you just saw this kind of math is the 
backbone of many machine learning techniques   Now if you're already a linear algebraic you may 
have noticed that I'm being a little imprecise with the notation here Like I might need to 
indicate whether this is really a transpose of W or X to make the math work out depending on 
how these vectors and matrices are defined But I'm not going to worry about that for now Instead 
I want to emphasize that when you're using linear regression as a machine learning model you are 
representing the system you're interested in as   a system of linear equations In fact if there 
were a set of W and B values that allowed you to perfectly predict Y given a set of X features 
then this would be a system of equations you could solve analytically without any machine learning 
And by that I mean you could solve this by just   applying basic algebra with a pencil and paper 
provided that you have a data set containing all the x and y's values and you have at least as 
many example records as you have unknowns the w's and the b's that is to solve for With linear 
regression you're solving the system empirically which is to say iteratively and approximately by 
finding the best fit linear solution to the system In this week of materials I'm going to start 
super simple and walk you through common vector   and matrix operations from the basis of linear 
algebra If you studied linear algebra before it is possible you will have seen some of these concepts 
already In order to give you a sense of what's   coming in each week in this course I will post a 
series of questions for you to think about If you can answer all these questions successfully then 
congratulations You are ready to skip right to   the end of the week and take the quiz If you have 
some uncertainty about any of these questions then you'll find some valuable learning in this week 
of materials This week we'll begin with systems of linear equations how you represent those systems 
of vectors and matrices and how you manipulate those systems to compute the determinant or 
other characteristics of the system So here's   your first set of questions This specialization 
has three courses: linear algebra calculus and probability and statistics Suppose I recorded a 
score for you in each of these three courses but I don't tell you the actual scores Instead I tell 
you the following information about your scores Your linear algebra score added to your calculus 
score minus your probability and statistics score   is equal to six Your linear algebra score minus 
your calculus score plus double your probability and statistics score is equal to four four 
times your linear algebra score minus double your calculus score added to your probability and 
statistics score is equal to 10 Okay now of course no instructor is ever going to give you scores 
like this That would be ridiculous But take a moment to think about these sentences and see if 
you could represent these statements as a system   of linear equations If we let A represent your 
linear algebra score C represent your calculus score and P represent your probability and 
statistics scores here is what that would look like Now in the context of what we looked at 
before with the linear regression for predicting the wind power output what would be the equivalent 
of the weights W the features X and the targets Y in this case your scores A C and P are the 
weights the thing that is consistent across all the statements The features are the numbers next 
to the weights in that case and the numbers on the other side of the equal sign are the targets 
y Now could you tell me whether this system is   singular or non-s singular or in other words do 
these equations either contradict one another or is there redundant information here could you 
solve this system of equations which is to say   could you now solve for what your score in each 
of these three courses was now could you represent this system of linear equations as a matrix and 
a vector and can you calculate the determinant   of that matrix if you could easily answer all 
these questions then congratulations You are ready to go ahead and take the quiz for this week 
and complete the labs for this week If not then also congratulations You are in the right place 
Join me through this week's materials as we go   through systems of linear equations step by step 
We'll start super simple with systems of sentences that will then turn into equations and then from 
there we'll get into solving these systems and the properties of such systems like singularity and 
the determinant As you can see linear algebra is used all over the place in machine learning and 
that is a great reason for you to master it In   the remainder of this course you'll be learning 
some very important concepts and techniques in linear algebra involving matrices linear equations 
linear transformations and much more Ready let's begin The first topic you learn in linear algebra 
is systems of linear equations However before you delve into equations it is important to understand 
the language of mathematics When I think of equations I think of sentences Sentences that are 
giving you information about things in the world And when there are many sentences or systems of 
sentences as I call them these sentences combine themselves to give you more information When 
looked at from a proper angle the way sentences combine to give you information is very similar to 
the way equations combine to give you information In other words systems of sentences behave a 
lot like systems of equations as you'll see in the following example So let's start with some 
examples of systems of sentences Now for the sake of this example assume that you only have one dog 
and one cat and they're both of only one color You are given some information and your goal is 
to try to figure out the color of each of the   animals So here's system one with the sentences 
the dog is black and the cat is orange Then you have system two with sentences the dog is black 
and the dog is black And finally you have system   three with the sentences the dog is black and the 
dog is white The sentences used here are simple sentences with one piece of information each 
So sentences such as the dog is black and the   dog is white are not allowed as they separately 
contain two pieces of information And the goal for a system is to convey as much information as 
possible with these simple sentences With regards to achieving that goal notice that these systems 
are quite different In particular the first system   of sentences contains two sentences and two pieces 
of information This means the system contains as many pieces of information as sentences and 
that's called a complete system System two is a bit less informative as it has two sentences but 
they're exactly the same Therefore the system only   carries one piece of information even though it 
contains two sentences The sentences here repeat themselves and therefore the system is called 
redundant And finally system three is strange as the sentences contradict each other This is 
because the dog can't be black and white at the same time Remember that we have one dog and it 
can only have one color So the system is called   a contradictory system The more information a 
system carries the more useful it'll be for you For this reason we'll introduce some terminology 
that you'll be using throughout the whole course   When a system is redundant or contradictory it's 
called a singular system And when a system is complete it's called a non-s singular system In a 
nutshell a non-s singular system is a system that carries as many pieces of information as sentences 
So it's the most informative a system can be And a singular system is less informative than a non-s 
singular one Systems of sentences can carry more   than two sentences In fact they can carry as many 
as we want Here are some examples of systems with three sentences In this new example you have 
three animals and are again trying to determine their color The first system has the sentences 
the dog is black the cat is orange and the bird is red The second one has a sentence says the 
dog is black the dog is black and the bird is   red The third one says the dog is black the dog is 
black and the dog is black And finally the fourth one has the sentences the dog is black the dog is 
white and the bird is red So the first system is complete as it carries three different pieces of 
information using three systems So it's complete   and non-s singular The second system is redundant 
and singular as the first and second sentences say the exact same thing Can you guess what the third 
system is going to be so if you guess redundant well done The third system is redundant since 
all the sentences say the same thing And finally the fourth system is contradictory because the 
dog can't be black and white at the same time   Notice that the third system is more redundant 
than the second system which has two sentences that say the dog is black Is there a measure of 
how redundant a system is and the answer is yes   and it's called a rank But you'll learn this a bit 
later this week And in terms of singularity and non-s singularity the terminology is exactly like 
before The first system is non-s singular as it is complete and the other three systems are singular 
as they are either redundant or contradictory Now system can be a bit more complicated than the ones 
we previously saw Consider the system of sentences   Sentence one says "Between the dog the cat 
and the bird one of them is red." Sentence two says "Between the dog and the cat one of 
them is orange." And sentence three says "The   dog is black." So problem one of this quiz says 
"Can you figure out what color is the bird?" And problem two says "Is this system singular or 
non-s singular?" Great job And the solution for question one is that the bird is red Why 
well if you look at the third sentence it says that the dog is black So now you know that the 
dog is black in the entire system of sentences When you look at the second sentence it says 
that between the dog and the cat one of them   is orange Since the dog is black then the cat 
must be orange And finally the first sentence says that among the three animals one of them 
is red Since the dog is black and the cat is orange then we must conclude that the bird must 
be red For question two since you figure out the color of the three animals that means the system 
carries three pieces of information with three   sentences In other words it has no redundances 
and no contradictions It carries as many pieces of information as sentences Therefore it is 
a complete system and it is non-s singular As I mentioned before equations behave a 
lot like sentences as they are statements   that give you information In this video you 
will learn what a linear equation is and what a system of linear equations is As a matter of 
fact you will be solving your first system of linear equations which is extracting all the 
possible information from that system Just like with systems of sentences systems of linear 
equations can also be singular or non-s singular based on how much information they carry And 
as you already learned these concepts with real life sentences you are more than ready to tackle 
them with equations In the previous video you saw sentences such as between the dog and the cat one 
is black For the rest of the course you'll focus on sentences that carry numerical information such 
as this one The price of an apple and a banana is $10 This sentence can easily be turned into 
equations as follows If A is the price of an apple and B is the price of a banana then the equation 
stemming from the sentence is A + B equals 10 Of course you might wonder why an apple and a banana 
together cost a whopping $10 To keep our example simple I'm going to use these hypothetical prices 
with nice whole numbers So while the thought of a $10 apple and banana might make your wallet 
shutter rest assured it's all in the name of   mathematical simplicity Now here's the first quiz 
in which you will be solving the first system of linear equations in this class The problem is 
the following You are going to a grocery store but this is a very peculiar grocery store In 
this store the individual items don't have   information about their prices You only get the 
information about the total price when you pay in the register Naturally being a math person as you 
are you're interested in figuring out the price of   each item So you keep track of the total prices of 
different combinations of items in order to deduce the individual prices So the first day that you go 
to the store you bought an apple and a banana and   they cost $10 The second day you bought an apple 
and two bananas and they cost $12 And the question is how much does each fruit cost so several things 
may happen You may be able to figure out the price of the apple and banana Or you may conclude that 
you don't have enough information to figure this   out Or even more you may conclude that there's a 
mistake with the prices given this information All of these are options in the quiz And the solution 
is that apples cost $8 and bananas cost $2 each Why well from day one you can see that an apple 
plus a banana is $10 From day two you can see that an apple plus two bananas is $12 So what was 
the difference between day one and day two well in day two you bought one more banana than in day one 
Also in day two you paid $2 more than in day one Thus you can safely conclude that that extra 
banana you bought on day two cost $2 The extra $2 you paid on day two were because of that extra 
banana you bought on day two And now that you know that bananas cost two well how much do apples cost 
well from day one you can see that an apple and a banana cost $10 So if a banana cost $2 then the 
remaining $8 must correspond to the apple Thus each apple costs $8 and each banana costs $2 To 
start you'll be solving the first system of three equations with three unknowns And the problem is 
a familiar one You're in a similar store as before   but now your goal is to find the prices of three 
items: an apple a banana and a cherry So you go to the store three days in a row On the first day you 
bought an apple a banana and a cherry and paid $10 On the second day you bought an apple two bananas 
and a cherry and paid $15 And on the third day you bought an apple a banana and two cherries and paid 
$12 Now the question is how much does each fruit cost so the way to solve this is very similar to 
the way to solve the previous systems with two equations and two unknowns The first equation says 
that an apple a banana and a cherry cost $10 The second one says that the same arrangement plus an 
extra banana cost $15 Therefore that extra banana must cost those extra $5 And the third one says 
that the same arrangement of day one plus an extra cherry cost $12 Therefore that extra cherry must 
cost that extra $2 So banana's five and cherries two We still need to find the price of the apple 
Now look at the first equation If the banana is five and the cherry is two and the three of them 
cost 10 then that apple must cost three So our solutions are the price of an apple is three the 
price of banana is five and the price of a cherry   is two The system of equations you just solved is 
this one over here System of equations 1 which has as equations a + b + c= 10 a + 2 b + c= 15 and a + 
b + 2 c = 12 And the solution you got was a= 3 b= 5 and c = 2 Now here's quiz 2 The scenario is the 
same except the prices in the store are a little different And you also bought different quantities 
of fruits On day one you bought an apple and a banana and they cost $10 On day two you bought 
two apples and two bananas and they cost $20 The question is how much does each fruit cost remember 
that the options of not having enough information or having a mistake in the information given are 
both valid as well For this problem the solution is that there's not enough information to tell 
the actual prices And why is this well you can use a similar reasoning than before From day one 
you can deduce that an apple and a banana cost   $10 From day two you can deduce that two apples 
and two bananas cost $20 But these two equations are the same thing They may not look the same but 
in disguise they're the exact same thing Because   you see if one apple and one banana cost $10 then 
twice of one apple and one banana cost twice of $10 which is $20 So two apples and two bananas 
cost $20 Therefore the system is redundant because it basically has the same equation twice 
It's like that system of sentences where   both sentences stated that the dog was black The 
system didn't carry enough information Now what are the solutions to the system well because the 
system doesn't carry enough information the system has infinitely many solutions Any two numbers 
that add to 10 are a solution to the system So for example if the apples eight and the bananas 
two then that works because apple plus bananas 10 and two apples plus two bananas is 20 But if 
they're five and five that also works If they're   8.3 and 1.7 that also works And even saying that 
the apples are free and the bananas are 10 works too So this system has infinitely many solutions 
because you simply don't have enough information You don't have the two equations to narrow it 
down to one single solution like you had with   the complete system And now you're ready for a 
final quiz Similar scenario except the first day you bought an apple and a banana and they cost 
$10 And the second day you bought two apples and   two bananas and they cost $24 Can you figure out 
how much each fruit costs and remember there are still the options of not enough information or a 
mistake in the information And the answer here is that there's no solution Why well in the same 
fashion as before if one apple and one banana cost $10 then two apples and two bananas must 
cost $20 But the store charged you $24 for two apples and two bananas Where are those four extra 
dollars if you assume that there are no extra fees for buying more than one fruit or discounts or 
anything of that sort then you must conclude that   that extra money must be due to a mistake with 
the register when you checked out in at least one of the two days This means that these two 
equations contradict each other just like the two sentences the dog is black and the dog is white 
contradicted themselves And this concludes that the system has no solutions So here's our recap 
You solved three systems of equations The first one has the equations a + b= 10 and a + 2 b = 12 
because the price of an apple and a banana was 10 and the price of apple and two bananas were 12 
The second one has the equations a + b = 10 and 2 a + 2 b = 20 And the third one has the equations 
a + b= 10 and 2 a + 2 b = 24 The first one had a unique solution which was a = 8 and b = 2 for a is 
the price of an apple and b the price of a banana The reason this system has a unique solution is 
because both equations give you one different   piece of information and thus you're able to 
narrow down the solution to one unique solution For this reason the system is complete and non-s 
singular The second system has infinitely many solutions which are any two numbers that add to 
10 In this system the two equations are the exact same one So you never had a second equation 
to help you narrow down the solution to a unique one This means the system is redundant 
and singular And finally the third system has no solution because the two equations contradict 
each other Therefore this system of equations is contradictory and singular So as you see we are 
using the same terminology as with systems of sentences and everything works in the exact same 
way Now you're ready to solve more systems Here   are three systems to solve Remember that just like 
before some of them may not have a solution Some of them may have an infinite number of solutions 
So if that's the case please state it even if any of the options already shows a solution to the 
system And so here are the solutions System two   has infinitely many solutions Why well you look 
at equation one it says A plus B plus C= 10 And from equation two there's an extra C and an extra 
five So that C must be equal to five The price of a cherry is five However when you go from equation 
2 to three there's also an extra five and also an extra C So equation three brings nothing new 
to the table When you replace C= 5 on the first equation you get A + B= 5 And actually any triplet 
where the third number is five and the first two add to five works So all of these are solutions 
to that system That system has infinitely many solutions Now let's look at system three System 
three has no solutions Why well from the first and the second equation C is equal to five However 
from the second and the third C is equal to three because from the second equation to the third one 
you bought an extra cherry and you paid an extra   $3 So C is equal to 5 but C is equal to 3 That's 
a contradiction So system 3 has no solution What about system four well system four has infinitely 
many solutions Why because as you can see the second equation is two times the first one and 
the third equation is three times the first one   So the first one is really the only equation 
here that matters or equivalently the second or the third but only one of them matters So any 
three numbers that add to 10 would work So for example 0 0 10 27 1 9 1 0 any three numbers that 
add to 10 are a solution So we have infinitely many solutions And finally some clarification You 
may have noticed the word linear equation several times What does that mean well linear equation 
can be anything like a + b = 10 2 a + 3 b = 15 3.4 a - 48.99 b + 2cals 122.5 anything like that 
And notice that it can have as many variables as we want but there's a special rule that must be 
followed in a linear equation Variables A B C etc We're only allowed to have numbers or scalers 
attached to them And there's also an extra number all by itself like the 122.5 here allowed to be 
in the equation So in short you can multiply the variable by scalers and then add them or subtract 
them and then add a constant and that's it So what's an equation that's nonlinear well nonlinear 
equations can be much more complicated They can have squares like a squar b^2 They can have 
things like s cosine tangent aran anything like that Powers like b to the 5 They can have powers 
like 2 to the a or 3 to the b And furthermore you can actually multiply the a's and b's In linear 
equations you can only add them but in a nonlinear equation you can have a ab 2 b / a 3 / b things 
like logarithms anything along those lines So linear algebra is the study of linear equations 
like the ones in the left And since they're much simpler then there are many things you can do 
with them such as manipulating them and extracting   information out of them So we're only going to 
worry about the linear equations in the left and the reason it's called linear algebra because it's 
the study of linear equations Now that you learned what a system of linear equations is and when 
they are singular and non-s singular it is time   for some visualizations It turns out that linear 
equations can easily be visualized as lines in the coordinate plane Well this is because you have 
two variables If you have three variables they   are planes in space And if you had more variables 
they look like highdimensional things But let's not worry about that yet So since linear equations 
can be represented as lines then systems of linear equations can be represented as arrangements of 
lines in the plane This way you can visualize their solutions and their singularity or non-s 
singularity in a much clearer way Okay So how can you visualize for example the equation a + b= 
10 as a line first let us get a grid in which the horizontal axis represents A which is the price 
of an apple and the vertical axis represents B which is the price of a banana Now let's look 
at solutions to this equation A + B= 10 In other words pairs of numbers that add to 10 And what 
you'll do is put them in this plot So two of your solutions are the point 10 So the A coordinate 
the price of an apple is 10 And the B coordinate the price of a banana is zero because 10 + 0 is 10 
Another obvious solution is the point 010 where a is 0 and b = 10 Other solutions are the 46 because 
4 + 6 = 10 So this is a = 4 and b = 6 or the 8 2 where a= 8 and b = 2 Notice that you can also have 
negative solutions For example -4 Now this makes no sense in the word problem because an apple 
cannot cost minus4 But these are two numbers that add to 10 - 4 + 14 = 10 So this is a legitimate 
solution to the equation And you can also have negative solutions like 12 - 2 Now notice that 
all these points form a line In fact every single point in this line is a solution to the equation 
So you can then associate the equation a + b= 10 with this line Now let's do another equation Say 
the equation a + 2 b = 12 That means points for which the horizontal coordinate plus two times the 
vertical coordinate add to 12 So some solutions for this equation are the point 06 since 0 + 2 
* 6 = 12 The point 12 0 because 12 + 2 * 0 is 12 The point 82 because 8 + 2 * 2 is 12 And again 
negative solutions like -4 8 for example because -4 + 2 * 8 is 12 And again these points form a 
line and every point in the line is a solution to this equation So the line is associated with the 
equation a + 2b equals 12 One small aside you may be familiarized with the notions of slope and y 
intercept in a line The slope is the ratio of rise over run which in the line of the left is minus 
one As for every unit you move to the right the line moves one unit down So the down is the minus 
the negative For the line on the right the slope is minus a half because for every unit you move to 
the right the line moves half a unit down For the y intercept for the line on the left it is 10 As 
this is the height of the intersection between the line and the vertical y-axis and for the line on 
the right it is six Now here's what's interesting Each equation is associated to a line So what 
happens with the system of two equations well the system of two equations is simply associated with 
the two lines in the same plane Notice that these two lines cross at a unique point The point A2 for 
A= 8 and B = 2 The point is precisely the unique solution to that system of equations This is 
exactly what we got before algebraically but now we can see it geometrically Now that we know how 
to plot the equation of line a + b= 10 let's try another one How about 2 a + 2 b= 20 well notice 
that that line still goes through the points 0 10 and 10 0 As the line is defined by only two points 
then the line is exactly the same and as the one with equation a plus b= 10 Recall that a few 
lessons ago you learned that the equations a plus b= 10 and 2 a + 2 b= 20 carry the same information 
This is a visual confirmation for that And now when we want to find the solution to this set of 
equations there is no single intersection point Instead the two lines overlap each other They are 
the same line And what happens now is that every point that belongs to both lines is a solution 
to the set of equations a + b= 10 and 2 a + 2 b= 20 That means we have infinitely many solutions 
because every point in that line is a solution And finally let's look at another system of equations 
The one with equations a + b = 10 and 2 a + 2 b = 24 So let's plot the one on the right Notice 
that the line of equation 2 a + 2 b = 24 goes through the point 0 12 and 12 0 because 2 * 0 + 2 
* 12 is 24 and 2 * 12 + 2 * 0 is 24 And therefore it has to be this line over here It's very similar 
to the original line except it's translated up by   two units So when we try to get the solutions 
to this set of equations take a look The system of two equations is simply associated to these two 
lines in the same plane that are parallel Parallel lines never meet So there are no solutions to 
this system There's no point that belongs to both of the lines So the system has no solutions Let's 
now summarize what you've seen in this video There are three systems of equations The first one has 
equations a + b= 10 and a + 2 b= 12 The second one has equations a + b= 10 and 2 a + 2 b = 20 And the 
third one has equations a + b = 10 and 2 a + 2 b = 24 And here are the plots for the three The first 
one corresponds to two lines that intersect at the unique point 8 2 So that's the unique solution to 
the system The second one corresponds to two lines that are exactly the same line corresponding to a 
system that has infinitely many solutions And the third one corresponds to two parallel lines that 
never meet which means the system has no solutions So we can use the exact same nomomenclature as we 
used with equations and with systems of sentences Since the first system has a unique solution it 
is complete and it is non-s singular Because every line brings something new to the table The second 
system has infinitely many solutions because the second line is exactly the same as the first one 
So the system is redundant and singular The second line brings nothing new to the table because 
it's exactly the same as the first line And   finally since the third system corresponds to two 
lines that never meet it means the second equation contradicts the first one We have no solutions 
Therefore the system is contradictory and singular Now you're ready for another quiz Problem one says 
which of the following plots correspond to the systems of equations 3 a + 2 b= 8 and 2 a minus 
b = 3 and problem two says by looking at the plot for problem one do you conclude that the system 
is singular or non-s singular and the answer is this In order to plot these lines you can notice 
that the line with equation 3 a + 2 b goes to the point 04 and 8/3 0 And the line with equation 2 a 
minus b = 3 goes to the point 0 minus 3 and three 0 And notice that the two lines cross at the point 
2 1 which is precisely the unique solution to the system of equations That's a= 2 and b = 1 Since 
the two lines intersect at a unique point then the system is non-s singular In a similar way a 
linear equation in three variables is represented by a plane in three-dimensional space At the right 
you have three dimensional space with three axis The axis A which is a horizontal axis the axis B 
which is vertical and the axis C which should stem from the screen and go all the way to your nose 
So how would the plot of for example the equation A + B + C = 1 look like in space well let's look 
at some points that would belong to this plot For example the point 1 0 0 belongs to this plot 
because 1 + 0 + 0 is equal to 1 That's the point where the a coordinate is 1 and the other two 
are zero The point 0 1 0 also belongs to here because 0 + 1 + 0 is 1 And finally the point 0 
0 1 also belongs to this plot Now three points define a plane and actually the entire plane that 
passes through those three points is the set of solutions of the equation a plus b + c is equal 
to 1 So in the same way as a linear equation with two variables correspond to a line in the plane 
A linear equation on three variables correspond   to a plane in space Now in the particular case 
where the constant of the equation is zero for example in the equation 3 a - 5 b + 2 c= 0 the 
plane must go through the origin which is the point 0 0 And the reason for this is because if 
we set a equ= 0 b= 0 and c= 0 This is a solution to the equation because the sum of 3 * 0 + 5 * 0 
+ 2 * 0 is equal to zero So now in the same way as we used to intersect lines to get points as 
the solutions to systems equations you can also   intersect planes Check this out So here we're not 
so concerned about getting the right visualization because these things are hard to visualize in two 
dimensions However we're going to be concerned about how the intersections of these planes appear 
So for this system of equations a plus b + c= 0 a + 2 b + c= 0 and a plus b + 2 c= 0 Let's look at 
the first equation that corresponds to some plane that goes to the point 0 0 The second equation 
corresponds to some other plane that goes to the point 0 0 And these two planes intersected a line 
And the third one corresponds to another plane that passes to the point 0 0 And the three planes 
intersect at a single point which is precisely the point 0 0 So this is important because this is a 
non-s singular system It has a unique solution and that unique solution is the point 0 0 Now let's 
look at system two The first equation is again a plane and the second equation is another plane 
and both of them go through the origin and they intersected a line Now this system is singular So 
what happens is that the third plane also crosses the other two and the origin but it actually 
crosses the other two at a line So these are three planes that all of them go through the same line 
So the set of solutions is not just a point it's an entire line So there are multiple solutions 
to the system of equations which means that the system is singular And finally we have this other 
system where the equations are a plus b + c= 0 2 a + 2 b + 2 c= 0 and 3 a + 3 b + 3 c= 0 The first 
one corresponds to a plane Now as you've seen before the second equation is just a multiple of 
the first one So it's actually the same equation So it's not surprising that it corresponds to 
the exact same plane And the third equation also corresponds to the exact same plane Therefore the 
set of solutions to this system is every single point in the plane There are multiple solutions 
and therefore this system is again singular Coming up next you'll have an opportunity to use 
some interactive tools that let you explore this concept in a hands-on way The first one allows you 
to build and manipulate 2x2 systems of equations visualize them as lines in the plane and see 
how changes to the system impact the number of solutions to your system In the second tool you 
can choose between several systems of equations   in three-dimensional space and then rotate those 
equations in three dimensions You're more than welcome to explore these tools on your own but 
I've also included instructions on how to use them   and some suggestions of activities to complete 
Enjoy trying them out and I'll see you once you're done Now that you learn how systems of linear 
equations are represented by lines in the plane and that non-s singularity equates to 
these lines intersecting at a unique point I'm going to show you an even simpler way to 
visualize singularity and non-s singularity   It involves slightly simplifying the system 
Recall that these are the three systems you've been studying where system one has a unique 
solution so it's complete and non-s singular   System two has infinitely many solutions so 
it's redundant and singular And system 3 has no solution so it's contradictory and singular 
Now forget for a moment about the system being complete redundant or contradictory and let's 
focus on the singularity and non-s singularity And the reason for this is that these are the 
terms we're going to be studying during the   whole class Note that system two and three the 
singular ones are very similar since both consist of parallel lines Difference lies and in system 
two these lines are the same and in system three   they're spaced apart but both are fundamentally 
different from the non-s singular system one since in system one the two lines are not parallel So 
can you somehow contract systems two and three into one in order to really separate singular 
and non-s singular systems into two buckets and the answer is yes And the way to do this 
is to look at the constants of the systems of equations These constants are the numbers in the 
equations that are not accompanying the variables a or b For instance in system one they are 10 and 
12 In system two they're 10 and 20 And in system three they're the 10 and the 24 So let's turn all 
these constants for all these three systems into zero and see what happens to the plots So when 
you turn these constants to zero the plots now become these Why because the new systems always 
have the point 0 as a solution So they must pass by the origin Notice that if I were to set a 
and b to be zero in any of these equations the equations would work So 0 0 is a solution to all 
of them Now notice that system one is still a pair of intersecting lines So it still has a unique 
solution So it's still complete and non-s singular   System two is still a pair of identical lines So 
it still has infinite solutions So it's redundant and singular But notice what happened to system 
three It went from a pair of non-intersecting   parallel lines to a pair of identical lines So 
now it has infinitely many solutions instead of none So it went from contradictory to 
redundant However it stayed singular as before and that's what matters So in conclusion 
the constants in the system don't matter when it comes to determine if the system is singular or 
non-s singular Now for the rest of this course singularity and non-s singularity will be the 
important concepts and completeless redundancy and contradiction will be used much less So 
for this reason you can now start considering systems of equations where the constants 
are always zero and these are much simpler Furthermore the geometric interpretation of these 
systems will be pairs of lines that go through the origin So now that you know that the constants in 
the system of linear equations are not important in order to determine if the system is singular 
or non-s singular you're about to jump into one   of the most important and fundamental 
objects of linear algebra the matrix Matrices have lots of very important properties 
and they arise from many different places in math In this case they will arise from the coefficients 
in a systems of equations in a very natural way Let us go back to the two systems of equations 
you previously saw Recall that systems two and three collapse into one system when you turn the 
constants into zero Now since the constants are zero you can forget about them If you take the 
coefficients of A and B and now put them in a   2x two shaped box which is called an array or 
a matrix that's the matrix corresponding to the system So the matrix corresponding to the 
first system is the one with entries 1 1 1 and 2 Why these numbers because the first equation a 
+ b can be considered the equation 1 * a + 1 * b Thus the 1 and the 1 and the equation a + 2 b can 
be considered the equation 1 * a + 2 * b the 1 and the 2 So in this matrix each row corresponds to 
each equation and each column to the coefficient of each one of the variables in this case 
a for the left column and b for the second   column Similarly the matrix corresponding to the 
second system is this one with entries 1 1 2 and two So a matrix is simply an array of numbers 
inside a rectangle These ones are simple as   they're in a 2x2 rectangle But later in the course 
you're going to see larger matrices And matrices just like systems of linear equations can also be 
singular or non-s singular Since the first system is non-s singular because it has a unique solution 
then we say that it corresponding matrix is non-s singular And since the second system is singular 
as it has infinitely many solutions we say that its corresponding matrix is singular Of course 
there are quicker ways to tell if a matrix is singular or non-s singular we have without having 
to find the solutions to its corresponding systems   of equations In the previous video you solve four 
systems of three equations and three unknowns You found that the first one has a unique solution 
The second one has infinitely many solutions The third one has no solutions And the fourth one 
has infinitely many solutions using the same terminology as before The first one's complete 
the second one's redundant the third one's   contradictory and the fourth one is redundant And 
the first one is non-s singular while the other three are all singular Now just like with systems 
of two equations and two unknowns an easy way to find if a system is singular or non-s singular is 
to turn the constants into zeros and to study that system And now that you have simplified the system 
with zeros as the constants then let's look at the solutions The first system we know that it has 
a unique solution since it is non-s singular And furthermore 000 is a solution because setting 
a b and c equal to zero is a solution to the system Therefore the unique solution is the point 
0 0 and the system is complete and non-s singular Now notice that systems two and three are now 
the same system because we set this constants   to be zero What are the solutions to this 
system well take a look at equations one and two The only difference between equations one 
and two is that equation two has an extra C and C must be equal to zero because that C doesn't 
change the result which is zero Therefore C is equal to zero And if you plug that into the first 
equation you get that A plus B must equal to zero So a is equal to minus b So all the points 
satisfy that the first coordinate is equal   to negative the second coordinate and the third 
one has to be zero And this system is redundant and singular And the fourth system well its 
solutions are all the points for which the three coordinates add to zero That means a can be 
anything b can be anything and c needs to be minus a minus b for the three to add to zero And the 
system is redundant and singular And just like before each of the systems is associated with a 
matrix that keeps track of its coefficients Again   in each row you have each equation and each 
column you have the coefficients of each of the variables A B and C So the matrices are these 
ones And using the same notation as before these matrices are singular or non-S singular based 
on if the systems are singular or non-s singular In this video I will show you a way to tell if 
a matrix is singular or non-s singular directly without having to solve the system of linear 
equations But first let us look back at systems   of sentences Recall that a system of sentences is 
singular if the second sentence carries the same information as the first one Similarly a system 
of equation is singular if the second equation carries the same information as the first 
one This is the concept of linear dependence Let's look back at the two systems of linear 
equations that you've seen so far with this   corresponding matrices and let's focus on the 
singular system on the right The reason this system is singular is because the second equation 
is a multiple of the first one In particular it's   two times the first one That means if we take 
the left hand and the right hand and multiply everything by two in the first equation you 
get the second equation Now if you look at   the corresponding matrix then the second row is 
a multiple of the first one And by that we mean that if you take every element in the first row 
and multiply them all by two you get the second row That means that the second row can be obtained 
from the first one So the second row is dependent of the first one Notice that you can also say that 
the first row is dependent of the second one by   taking the second row and multiplying everything 
by a half So in any way either one depends on the other one or the other one depends on the 
one They're both dependent So they're linearly dependent In contrast in the non-s singular system 
on the left the second equation is not a multiple of the first one or vice versa There's no constant 
that I can multiply the first equation to get the second equation or vice versa This is why the 
system is non-s singular Each equation tells you something completely different So in the 
corresponding matrix the same thing happens No row is a multiple of the other one I cannot take a 
number and multiply one row entirely by the number to get the other row That means the rows are 
linearly independent As you may imagine the same thing happens with columns and rows And one can 
define this concepts of linear dependency between rows and between columns and they determine 
the singularity and non-s singularity of the   matrix For larger matrix the concept of linear 
independence and dependence is a bit more complex but still very intuitive Follow along for the 
details In order to understand linear dependence consider the following example Take a look at the 
system with three equations and three unknowns The equations are A= 1 B= 2 and A + B= 3 Now notice 
that C doesn't appear but that doesn't matter Now this system is singular for the simple reason that 
the third equation is the sum of the first two And to see this more in detail the first equation 
can be written as a + 0 b + 0 c= 1 The second one is 0 a + b + 0 c= 2 And if we add them we get a + 
b + 0 c= 3 which is precisely the third equation Now consider the matrix of the coefficients 
of this system That's the matrix obtained by forgetting the constants The matrix has entries 1 
0 0 1 0 and 1 1 0 And in the same way that the sum of the first two equations is the third one the 
sum of the first two rows in the matrix is equal   to the third one because 1 plus 0 is 1 0 + 1 is 1 
and 0 + 0 is 0 Therefore row three depends on rows one and two Row three in the same way as equation 
three doesn't bring anything new to the table It's the sum of the first two So we say that the rows 
are linearly dependent because you can get the from the first and the second you can form the 
third one Therefore this matrix and this system are singular Now let's look at another system 
which you've already seen that it's singular The corresponding matrix is this one over here 
111 222 333 Notice that there are many relations between the equations of this system Here's 
one the first equation plus the second equation equals the third equation And in the matrix row 1 
+ row 2 is equal to row 3 So row 3 depends on rows one and two meaning that the rows are linearly 
dependent Now if you look closely this system and this matrix have a lot of other dependencies 
For example the second row is twice the first row and the third row is three times the first row 
So this is a highly singular system with a lot of dependencies between the rows Now in order to 
really understand the concept of linear dependence and independence let's take a look at this other 
system which you've also seen is singular Now this one is a little more subtle What are the relation 
between the equations well here's one Let's take the first equation and let's add it to the third 
equation What do you get you get 2 a + 2 b + 4 c is equal to zero Now that doesn't look like one 
of the equations perhaps or perhaps yes If you take this equation and divide it by two you get 
a plus b + 2 c= 0 which is precisely the second equation So in other words the second equation is 
the average of the first and third equation And in the same way the average between row one and row 
three of the matrix is row two So row two depends on rows one and three So therefore the rows of 
this matrix as well as the equations in the system are linearly dependent Now in contrast take a look 
at this system of equations You've seen before that this system has a unique solution So it's 
non-s singular Now no matter how much you try it is impossible to find any linear relations between 
these equations There's no way you can obtain the third one out of the other two or the second one 
or the first and the third There's nothing you can do here And the same thing happens in the matrix 
No row can be obtained as a linear combination of the other two rows Thus the rows are linearly 
independent And this implies that the matrix is non-s singular Now as you can see it's not easy 
to see and to verify that there's no relation So don't worry Soon I'm going to show you some 
methods to be able to verify this But for now   you're ready for a quiz Determine if the following 
matrices have linearly dependent or independent rows And the answers are in For the first matrix 
notice that the first row * 3 plus the second row * 2 is equal to the third row And so the 
rows are linearly dependent This implies that the matrix is singular For the second matrix notice 
that if you subtract the second row from the first one you get the third row Thus the rows are again 
linearly dependent showing that the matrix is   singular The third matrix has no relations Notice 
that no matter how much you try there's no way to get one row out of the other two For example the 
first row has that leftmost one There's no way you can obtain that from linear combinations of 
the second and third row and so on So this matrix is non-s singular because the rows are linearly 
independent And finally for the fourth matrix notice that the first row* 2 is equal to the third 
row So these rows are dependent and the matrix is singular Now it turns out that there is a much 
faster way to tell if a matrix is singular or non-s singular And in this video you will learn 
it It's called the determinant and it is a quick formula that returns a zero if the matrix is 
singular and a number different from zero if the matrix is non-s singular Take a look back at the 
matrix in the previous example and let's focus on the singular matrix on the right In this matrix 
you can multiply the first row by two to obtain the second row And therefore the rows are linearly 
dependent In contrast for the non-s singular matrix on the left there's no number that you can 
multiply the first row by to get the second row because the rows are linearly independent Let 
us look at this more carefully If a matrix has entries A B C and D then the matrix is singular 
If there exists a number k for which the first row time k is equal to the second row that means 
a k= c and bkals d for the same value of k This is equivalent to c / a= d / b equals k And that's the 
same thing forgetting about k that a d is equal to bc or equivalently a d minus bc equals zero This 
value of a d minus bc is very important We're going to call it the determinant of the matrix 
So the determinant of the matrix is a d minus bc And by construction this determinant is zero 
if the matrix is singular and non zero if it's non-s singular Now a way I like to look at the 
determinant is like this A D is the product of the numbers in the main diagonal and BC is the product 
of the numbers in the anti-diagonal So let's calculate the determinants of these two matrices 
in the ongoing example and see what you get The   first matrix has determinant 1 * 2 - 1 * 1 which 
is 1 and the second one 1 * 2 - 2 * 1 which is 0 So notice that the first matrix which is non-s 
singular has a determinant of one which is non zero and the second matrix which is singular has 
a determinant of zero This is no coincidence This is always going to be the case Non-s singular 
matrices have nonzero determinants They don't need to be one but they're non zero And singular 
matrices have zero determinant To summarize the determinant of a matrix with entries A B C and D 
is A D minus B C And it is zero precisely when the matrix is singular and non zero when the matrix 
is non-s singular So now you're ready for a quiz In problem one you have two matrices and you're 
asked to find their determinants And in problem two determine if the matrices are singular or 
non-s singular based on the results of problem one And the answers are for the first matrix 
the product of the terms in the main diagonal is 5 * 3 which is 15 The product of the terms in 
the main antidagonal is 1 * -1 which is -1 And the difference is 15 - -1 which is 15 + 1 which is 
16 Since the determinant is different from zero then the matrix is non-s singular For the second 
matrix the determinance is 2 * 3 - -1 * - 6 So the three minuses means that's 1 minus So the 
determinant is 6 - 6 which is precisely zero Since the determinant is zero then the matrix is 
singular The determinant for 3x3 matrix is a bit more complicated than that for 2x2 matrices but it 
is mostly the same Here it is Before calculating the determinant recall that for a 2x2 matrix you 
consider the main diagonals add the products of the elements in one of them and subtract the 
products of the element in the other one In a   larger matrix this is the same thing except you'll 
need some more diagonals The first diagonal is the main one over here Now consider the next diagonal 
here This one's incomplete So you can complete it by wrapping around the matrix and finishing it 
like this The same thing happens with the next diagonal So for the determinant you're simply 
going to add the products of the elements in these diagonals and subtract the products of the 
elements in these diagonals that go the other way around Let's do an example Consider this 3x3 
matrix over here In order to calculate determinant consider the main diagonal with entries 1 2 
and 2 Its product is 1 * 2 * 2 which is 4 Now consider the next one with entries one one So 
the product of these entries is one And now the next one again with product of entries one And 
now we're going to subtract the diagonals that go in the other direction So this one is 1 * 2 
* 1 which is 2 This one over here is 1 * 1 * 1 which is 1 And this one over here is 1 * 1 
* 2 which is 2 So we're adding 4 + 1 + 1 - 2 -1 - 2 and the determinant becomes 1 So a little 
more calculations but basically the same thing as with 2x2 matrices So now you're ready for 
a quiz Find the determinant of the following matrices and verify that those with determinant 
zero are precisely the singular matrices You can recognize that these are the exact same matrices 
from the previous quiz And here are the answers   The first one has determinant zero and so does the 
second one The third one has determinant six and the fourth one has determinant zero Therefore the 
first two are singular the third is non-s singular and the fourth is singular because the same thing 
happens as with 2x2 matrices Singular matrix have determinant zero and non-s singular matrices have 
determinant non zero Now let's look more carefully at the third matrix The one with determinant six 
This six was obtained as 6 + 0 + 0 - 0 - 0 - 0 Why well let's do it You first have the main diagonal 
which is 1 * 2 * 3 which is 6 The next one is zero The next one is zero and the negative ones are 0 
0 and zero Notice something peculiar here The only term that is non zero is the first one because 
this matrix is very special is upper triangular which means that everything below the diagonal is 
a zero And now since everything below the diagonal is a zero all the terms in the determinant are 
going to contain one of these elements below diagonal except for the one that takes the 
entire main diagonal So whenever you have a matrix where anything underneath the diagonal is 
zero the determinant is going to be the product of the elements in the main diagonal So that's a 
quick shortcut you can use And they can still be singular For example look at this one In the same 
way this determinant is going to be the product of 1* 2 * 0 the elements in the diagonal and that 
happens to be zero All right that's the last new concept this week After this you'll have 
a few different opportunities to deepen your   understanding There are two labs that will help 
you get familiar with NumPy the Python package you'll be using on labs and assignments throughout 
this course The first lab is an introduction to the NumPy library overall and in particular 
NumPy arrays If you're already familiar with this library feel free to just skim this first lab 
The second lab introduces the ways NumPy can be used to represent systems of equations and show 
you how to use plotting libraries to visualize those systems Both labs are ungraded but provide 
valuable background for the rest of the course Finally the week concludes with a graded quiz on 
all the topics you've studied this week Good luck You are now ready to learn something that has been 
lurking yet present in the previous lessons In one of the first quizzes you took a system of linear 
equations and found its solution In general this is hard to do by simply looking at the equations 
However there exists a simple way to do this In this video you will learn a method or an 
algorithm that helps you find the solution to a system of linear equations and that is also 
able to tell you if the system is singular or non-s singular First let's go back to how you 
solve the first system of equations the one with equations a + b= 10 and a + 2 b= 12 Recall 
that the scenario was that an apple and a banana cost 10 and an apple and two bananas cost 12 
So you figured out that since the second day you bought an extra banana and paid two more that 
that extra banana must cost two From this and the fact that they both cost 10 you concluded that the 
apple must cost 8 Thus you went from the original system of equations to a solved system one that 
is still a system of equations except that the equations are a= 8 and bals 2 The solved system is 
much simpler because each equation actually tells us the value of each of the variables The goal 
is to take every system and turn it into a solved system That is if the system is non-s singular and 
has exactly one solution Now in order to get from the system to the solved system you followed some 
process This process required manipulating the equations without noticing what you actually did 
in your head was to follow a specific process that involves manipulation such as swapping equations 
adding them and multiplying them by constants In this video you'll see what this means more 
in detail The first step in order to go from a   system to a solved system is the following Notice 
that in the system on the left the equations both have A and B on them You'd like to get to one 
in which the A and the B are isolated and they only appear in one equation each The first step 
will be to look at the second equation and try to eliminate the variable A from it But before 
you do that it's important to know how one can manipulate equations that is take some linear 
equations and produce some other ones that are   still true based on the original ones One way 
to manipulate equations is to multiply them by a constant For example if an apple and a banana 
cost $10 how much would seven apples and seven bananas cost well the answer is $70 as this is 7 
* 10 Equation a + b= 10 can be multiplied by 7 on both sides to get the equation 7 a + 7 b = 70 And 
these equations carry the exact same information Thus if the first one is true so is the second one 
Another way to manipulate equations is to add two equations For example if an apple and banana cost 
$10 and two apples and three bananas cost $26 then how much are three apples and four bananas well 
if you add these two equations you get that three apples plus four bananas is equal to 32 If the two 
first equations are true then so is their sum So now I'll show you how to solve a harder system 
of equations in order for you to see how these   manipulations are used So here's the system of 
equations that you learn to solve The equations are 5 a + b = 17 and 4 a minus 3b= 6 And recall 
that your first goal in order to get to a solved system is to try to eliminate the variable A from 
the second equation in order to leave B by itself The first step is not always necessary but it will 
make your life easier The step is to divide each   equation by the coefficient of A in order for both 
of them to have a coefficient of one beside the A So if we do this the new equations are A + 0.2 
B = 3.4 4 and a minus 0.75 b = 1.5 And as you saw before these are equivalent to the first 
two because all we did was multiply the entire   equation left and right by a constant Now in order 
to remove a from the second equation one way is to subtract the first equation from the second one 
Like this we take the second equation we subtract the first one and we can do this component wise A 
minus A is 0 A - 75 B -2 B is -.95 B and 1.5 - 3.4 is - 1.9 The resulting equation is -.95B is 
equal to - 1.9 Now we can divide by minus.95 at both sides to get that B is equal to 2 So you 
have succeeded you have found the value of b In other words you removed a from the first equation 
to get bals 2 in your solve system And now that you know that b equals 2 you can simply plug that 
into the first equation The first equation says a +2b is 3.4 So a +2 * 2 is equal to 3.4 Now2 * 
2 is4 So we have a +4 = 3.4 And subtracting 04 from both sides you get that a is equal to 3 
and that is the value of the second variable So in a nutshell that is how you solve a system 
of two equations and two variables Now here's a small caveat Imagine that you are solving this new 
system with equations 5 a + b = 17 and 3 b= 6 And you'd like to eliminate a from this equation The 
first step is to divide both by the coefficient of a For the first equation this goes exactly 
as before However for the second equation the   coefficient of a is zero and you can't divide by 
zero So are we doomed well actually the doom turns into good luck Check it out The equation says 3b= 
6 It already has a eliminated from it and actually implies that b is equal to two if you divide by 
three on both sides So this is the value of b already And the second equation is already solved 
in the system In order to solve the first one you proceed to do the same thing Replace on the first 
equation that bals 2 Get that a is equal to 3 and bring it back as the solution So you're 
now ready for a quiz Solve the following   system of equations 2 a + 5 b = 46 and 8 
a + b = 32 And the solution to the system is a= 3 and b = 8 I invite you to verify 
that those values actually work for the equations Now this method is actually more 
powerful than you think In the previous examples you saw it in action for a non-s singular system 
that has a unique solution But what happens with   a singular system namely one that does not have a 
unique solution let us solve the singular system that also came up before The system of equations 
a plus b= 10 and 2 a + 2 b= 20 Recall that this system was redundant since the second equation 
is equivalent to the first one So let's try the same steps as before to try to eliminate a from 
the second equation and see where that takes us The first step is to divide both equations by the 
coefficient of a getting the two equations a + b= 10 and a + b= 10 Notice that they're the same The 
next step is to remove a from the second equation by subtracting the equation a + b= 10 from the 
equation a + b = 10 Now what do you get here well you get 0als 0 because you're subtracting 
the entire thing from itself This equation C= 0 is trivially true and unfortunately gives you zero 
information about what B can be What happened well in your efforts of removing A from the second 
equation in order to leave B alone you also   remove B And there is nothing you can do to get B 
There's no manipulation you can do to remove only one of the variables because anything you do to 
remove one will also remove the other one because   the system is singular And again this is because 
the second equation adds no value to the system So in other words the solved system is this one 
where a + b= 10 and there's no other equation to give us more information So can you still get 
the solution to looks like a equals something   and b equals something else well you still can 
pick any number x and let a= be x How much is b then well if a plus b= 10 then b must be 10 - 
x This is the solution to the system It's not a unique solution because x can be any value So the 
solve system has one degree of freedom and if you vary this degree of freedom x you can get many 
different solutions to the system The solutions for example form a line as you see before Now what 
happens if you use the same method with a singular system that is contradictory recall that the 
contradictory system that you saw earlier was this one A + b= 10 and 2 a + 2 b= 24 And let's use the 
same steps to solve it First we want to eliminate a from this equation So we divide both equations 
by the coefficient of a We subtract equation one from equation two And we get that 0 on the left 
is equal to two on the right 0 is never equal to two So that's a contradiction Therefore there 
are no solutions to the system So now you're ready for a quiz The quiz is the following Can 
you solve the following system of equations 5 a + b = 11 and 10 a + 2 b = 22 So if you look 
closely into the two equations in the system you will find that equation 2 is equation 1 * 2 or 
equivalent If you take equation 2 and divide it by two you'll obtain equation 1 Therefore the system 
is singular and it has infinitely many solutions Now that you've learned how to solve systems of 
two equations with two variables let me show you   how to solve systems of three equations with three 
variables It's actually very similar The goal here is first to leave a by itself So we're going to 
make sure that the only equation that contains an a is the first one and that there's no a on the 
second and third equations So the way to do this is to normalize the first column namely divide 
each row by the coefficient of a to make sure that every equation has a coefficient of one for 
the variable a And now use the first equation to remove the variable a from all the other ones So 
subtract the first equation from the second and then from the third to get the following Now 
in the resulting system you have successfully isolated A and what remains is a system of two 
equations with two unknowns that are B and C and you know exactly how to solve these ones So when 
you solve it you get the values for B and C Let me show you how So now let's forget about the 
first equation and solve the second and third And you know how to do this First divide these 
two rows by the coefficient of B to get this where the leading coefficient of B is now one Now 
use this equation the second one to remove B from the third one So subtract the second equation 
from the third one to get this 11 - 11 / 6 C = -1 /2 In this way you have isolated B in the 
bottom two rows From here you can divide by - 11 / 6 to get that C is equal to 3 So now let's go 
back to the beginning And now we know that C is equal to 3 Then we can go propagate upstairs So 
replace C= 3 in the second equation to get B= 2 And now that you know B and C replace B = 2 
and C= 3 in the first equation to get A= 4 So the final solution is A= 4 B = 2 and C = 3 
After this you'll get to use a new interactive tool to explore systems of equations in three 
dimensions You'll be able to see each equation   as a plane floating in threedimensional space and 
the solution to the system as the intersection of these planes It's a fun tool to play with and 
hopefully deepens your understanding of the   concepts you've been studying As always some 
suggested activities are below the tool itself Enjoy Now that you know how to find the solutions 
to linear equations by manipulating them you pretty much know what matrix row reduction 
is Matrix row reduction also called Gausian elimination consists of applying the 
exact same manipulations except to the   rows of a matrix in order to turn that matrix 
into a much more simplified form from which you can extract lots of useful information Let 
me show you how Recall that when you want to solve a system of linear equations say the 
one with equations 5 a + b = 17 and 4 a - 3 b= 6 you first went through an intermediate step of 
removing the variable a from the bottom equation   in order to calculate the value of b Then you 
went through a step of replacing the value of b in the first equation in order to get the value 
of a The solve system has equations a= 3 and b= 2 Now what happens if you follow the same procedure 
but in the matrix of coefficients So here is the matrix corresponding to the original system And 
recall that we can forget about the constants 17   and six and only pick up the coefficients 5 1 4 
and minus 3 And here is the matrix corresponding to the intermediate system Now you'll see this 
more in detail later but for now bear with me From the original matrix you can get the matrix 
in the intermediate system by applying some row manipulations An important feature of this 
matrix is that it has ones in the main diagonal and zeros underneath the diagonal This form of 
matrix is called row echelon form And finally some more manipulation will get you to the matrix 
with ones in the diagonal and zeros everywhere   Why is this the matrix corresponding to the system 
above because you can see the system of equations a= 3 and b = 2 as a system of equations 1 a + 0 b 
= 3 and 0 a + 1 b = 2 from which this matrix with entries 1 0 0 and one stems So the first matrix 
the one in the middle is called the rochelon form and this one over here is called the reduced row 
echelon form This will be introduced more later but for now let's focus on the row echelon form 
one as it gives us a lot of useful information of the matrix Now what happens if you have a singular 
system of equations such as the one you've seen in the beginning As you've seen before this system 
can be manipulated and turned into this system over here where the first equation is the same but 
the second one is an obvious 0 equals 0 Thus the original matrix can be manipulated and turned 
into the matrix with entries 1 1 0 and 0 This is also a roelum form Here's another example from 
the quiz on last video In this system you have the equations 5 a + b= 11 and 10 a + 2 b= 22 Notice 
that subtracting twice the first equation from the second one you get the trivial equation 
0= 0 indicating that this system is singular In the matrix we've gone from this one over 
here with entries 5 1 10 and two to this one with entries 1 0.2 and 0 And that one is the 
row echelon form And finally let's look at this very singular system equation 0= 0 and 0= 0 This 
system cannot be manipulated any further So the row echelon form of this matrix is actually itself 
The general way that a matrix in row echelon form looks like is the following On the main diagonal 
you have a bunch of ones followed by perhaps a bunch of zeros You could potentially have all 
ones but you could also have all zeros Below   the diagonal everything is a zero To the right of 
the ones every any number is allowed And finally to the right of the zeros everything must be zero 
Following these rules in the case of 2x2 matrices only three things may happen You have two ones 
in the diagonal you have one one in the diagonal or you have zero ones in the diagonal These are 
the ones you've seen earlier in the video Before we get into the whole row reduction process it is 
important to note that the same manipulations that   you use to solve systems of linear equations 
can be used in matrices These are called row operations in a matrix And a very important 
property that they have is that they preserve the singularity of a matrix In other words if 
you apply them to a singular matrix you get a singular matrix And if you apply them to a non-s 
singular matrix you get a non-s singular matrix In this video you will learn more about these row 
operations Consider the matrix with entries 5 1 4 and 3 First check if this matrix is singular or 
non-s singular by calculating the determinant The determinant is 5 * 3 - 1 * 4 which is 11 Therefore 
the matrix is non-s singular Now the first row operation that you learn is switching rows 
If you switch the positions of the two rows you   get the matrix with entries four three five and 
one I promise you that since the original matrix was non-s singular then this one is also non-s 
singular How do I know this well let's calculate   the determinant The determinant is 4 * 1 - 3 * 5 
which is - 11 which is different than zero So it's non-s singular And in fact the determinant of the 
matrix obtained after switching rows like this is   always the negative of the original determinant 
The reason for this is that the diagonals change place So the one you're adding you're now 
subtracting and the one you're subtracting   now you're adding So now instead of getting 15 
- 4 you get 4 - 15 hence the minus 11 You can imagine that if the original determinant was a 
zero then the resulting one would also be a zero which means that if you apply this to a singular 
matrix you get a singular matrix and likewise in the if the original determinant was not zero then 
the resulting was also not zero So row switching preserves singularity or non-s singularity of a 
matrix The next operation is multiplying a row   by a non-zero scaler So let's use the same matrix 
with determinant 11 Now let's modify the first row So we're going to leave the second row by itself 
And the first row we're going to multiply it by   a number say 10 to get 5010 And now that's going 
to be the new first row So what's the determinant of the new modified matrix 50 10 43 it's this And 
notice that it is 10 * 11 10 times the determinant of the original matrix because in each of the 
diagonals you have taken exactly one element the one at the top row and multiplied it by 10 So 
the whole thing gets multiplied by 10 Now notice that the scalar has to be non zero If the scalar 
that 10 is non zero then this operation turns a non-zero determinant into non-zero determinant 
and a zero determinant into a zero determinant Thus this operation also preserves singularity and 
non-s singularity And the final operation is to take a row and add it to another row For example 
let's take the sum of the first and the second row and that is 94 So 94 is going to be the 
top one of the rows in the matrix and the bottom one is going to be the same And this new 
determinant is 9 * 3 - 4 * 4 which is actually 11 So believe it or not when you do this you get 
the same determinant as the beginning Proving this is slightly harder and you can find it in 
a written dendum in the resources of the class But the most important part is that since the 
determinant stays the same after this operation   then this operation also preserves singularity 
and non-s singularity just like the other ones Throughout the last few videos there has been 
a concept that you've seen yet still have not been formally introduced to It is the notion of the 
rank of a matrix which in some way measures how much information that matrix or its corresponding 
system of linear equations is carrying Follow along with me and see how to define and calculate 
the rank One great application of ranking machine learning is an image compression Take a look at 
this image It's very crisp but it also uses a lot of storage because every pixel intensity has 
to be stored as a number Could you store this   image or perhaps a slightly blurriier version 
of it using significantly less space the answer is yes and it uses the main topic of this video 
which is the rank of a matrix It turns out that pixelated images are matrices and the rank of 
a matrix is related to the amount of space that   is needed to store that corresponding image This 
particular image has rank 200 which is quite high There's a very powerful technique on singular 
value de composition or SVD in short which can   reduce the rank of a matrix while changing it as 
little as possible You can see SVD applied here to reduce a heavy image of rank 200 into images of 
rank 1 2 5 15 and 50 Notice how the images of rank 15 and 50 are very similar to the original and 
would take a lot less space to store So recall that in systems of sentences there was a notion 
of how much information the system carried Let's   look at three systems of sentences System one with 
sentences the dog is black and the cat is orange System two with sentences the dog is black and 
the dog is black And system three with sentences the dog and the dog Notice that system one has two 
sentences and it carries two pieces of information System two also has two sentences but they're 
the same So this system carries only one piece of information And system three well it has two 
sentences but it carries no information regarding   the color of the animals So it carries zero 
pieces of information Recall that your goal is to determine the color So the amount of information 
a system sentences carries is defined as the rank of the system The system one has rank two system 
two has rank one and system three has rank zero Next you'll see how this notion applies to 
matrices and the corresponding systems of linear   equation Now let's go back to the three systems 
of equations from the previous videos As you've already seen the first system has two equations 
and each equation brings something new to the table some new piece of information That's why 
you're able to narrow down the solutions to one point The first equation narrows down to a line 
and the second one narrows them down to a point Thus the system has two pieces of information This 
is how the rank of the system is defined So the rank is two The second system has two equations 
but recall that the second equation was the same   as the first one Therefore this system really 
only carries one piece of information which is the first equation This is why you're able to 
narrow down the set of solutions to a line but that's as far as you can get Thus the rank of 
the system is defined as one And finally the third system has two equations but they carry no 
information as any number a and b satisfy these equations Thus the system carries zero pieces of 
information and its rank is defined to be zero And now on to define the rank of a matrix since it 
system of information has a corresponding matrix Then the rank of the matrix is defined as the rank 
of the corresponding system of equations Thus the   matrix corresponding to the first system has rank 
two The one corresponding to the second system has rank one and the one corresponding to the 
third system has rank zero Now there's a special relationship between the rank of a matrix and its 
solution space Recall that the solution space for each of these matrices is the set of solutions 
to the system of equations when the constants are zero So for the first one recall that the 
solutions are only a= 0 and b equals 0 So that's a point and the dimension of the solution space 
is zero because dimension of a point is zero For   the second one the set of solutions was some line 
and a line has dimension one So the dimension of the solution space was one And for the third one 
well every A and B works here because any point A and B is a solution to that system Therefore the 
solution space is a plane and it has dimension two So what happens here in all three cases is 
that the rank is equal to two the number of rows in the matrix minus the dimension of the 
solution space This is always the case for 2x2 matrix And in general as you'll see later this 
rank and solution space always and then dimension of solution space always add to the number of 
rows in the matrix Notice also that the first   matrix is non-s singular and the other two are 
singular So a matrix is non-s singular if and only if it has full rank Namely if the rank is equal 
to the number of rows This is the same as saying that a system of equations is non-s singular if it 
carries as many pieces of information as equations it has Meaning that you carry the maximum amount 
of information possible Thus that each equation brings a new piece of information to the table 
and there's no redundancy between equations And now you're ready for a quiz For this quiz 
please determine the rank of the same two   matrices you've seen recently And the solutions 
are in So since the first one has a solution space of dimension zero the rank is two And the 
second one has a solution space of dimension one So the rank is one Notice that the first matrix 
is non-s singular and the second one is singular Now just as it happened with 2x2 matrices the 
rank is a measure of how non-s singular a 3x3 matric is and its geometric meaning is very 
similar to that for 2x2 matrices Follow along with me and I'll show you how the rank works 
for larger matrices So in order to define the   rank of a 3x3 matrix I look at the systems of 
three equations and three unknowns that you've been looking at so far And let's focus on what 
equations bring new information to the table And by not bringing any information to the 
table I mean an equation that is already a   linear combination of the other equations in the 
system So in system one you can check that all the equations are linearly dependent There's no 
way to obtain one out of the other two Therefore the system has three equations and all of them are 
new pieces of information So it has three pieces   of information The number of independent equations 
is the rank So this system has rank three And by convention we say that the matrix has rank three 
Now let's take a look at system two And this can be done in different ways but one way to look at 
it is to realize that the second equation is the average of the first and the third So you can 
think of the first and the third to be the new pieces of information And the second one doesn't 
bring anything new to the table And if you do   this in a different order you could get different 
things but you will always get that there's three equations and two pieces of information because 
the system has rank two And thus the matrix is defined to have rank two System three is simpler 
since the first equation is new but the second   one is twice the first one so it depends on 
it And the third one is also three times the first one so it depends on the first one as well 
And so we have three equations and one piece of information This system therefore has rank one and 
so does the matrix And as usual system four is the simplest one No equation brings anything new to 
the table because no equation tells you anything   about A B and C So you have three equations zero 
piece of information and the matrix has rank zero There is a question It seems that calculating 
the rank is not that easy So is there a simpler way to calculate the rank and the answer is yes 
And it has to do with the row echelon form of a matrix In the previous videos you learned one 
particular form of a matrix called row echelon form This gives you lots of information about 
the matrix and it can be obtained with simple   row operations So in this video you'll see this 
more in detail Recall from previous videos that the row echelon forms of these three matrices 
are the ones in the right However previously we use the systems of equations to 
find them Now you can simply use the   row operations you've learned Follow me along 
to see how to do this In order to calculate the rowum form for this matrix with entries 
5 1 4 and minus3 here's what you would do The idea is to get rid of that four 
in the bottom left So first divide each row by the leftmost nonzero coefficient to get 
the matrix with entries 1 0.2 1 and minus 0.75 Now in order to remove that bottom left one keep 
the first row the same but subtract the first row from the second row to obtain the row with entries 
0 and minus 0.95 We have succeeded at getting a zero in the bottom left corner which is what 
we wanted Now as a final step divide the second row by the leftmost nonzero coefficient in order 
to get a one on the bottom right corner And now the matrix is in row echelon form And what if you 
do this for a singular matrix well if you try it for this matrix with entries 5 1 10 and two Let's 
see what happens The first step is to divide each row by the leftmost coefficient So you get the 
matrix with entries 1 0.2 1 and 0.2 Now in order to disappear that bottom left one what you can do 
is take the bottom row subtract the top row and what you get is 0 0 So now that's the new bottom 
row And now let's see what happens when you try to divide the second row by the leftmost non-zero 
coefficient Well that's impossible because you'd be dividing 0 by 0 which is undefined So that's 
no problem What you do is you let this one be the rochelon form instead And finally for the very 
singular matrix entry 0 0 0 there's not a lot you can do because you cannot divide each row by 
the last coefficients those are zeros So instead you just say this is the rorow echelon form In 
summary here are the matrices and here are the row echelum forms So here is a very interesting 
connection with rank Look at the first one It has two ones in the diagonal and it happened to have 
rank two The next one has one one in the diagonal and it happened to have rank one And the third one 
has zero ones in the diagonal and it happened to have rank zero So that's actually the connection 
The rank of a matrix is the number of ones in the diagonal of the row echelon form That's an easy 
way to calculate the rank Notice furthermore that the first one is non-s singular the second 
one is singular and the third one is singular So a matrix is singular if and only if 
the row echelon form has only ones and no zeros Now that you've seen row echelon form for 
2x2 matrices let me show you what it is for bigger matrices Recall this system of equations and 
recall that the process you took to solve it was this intermediate step over here where the first 
equation has variables A B and C The second one only has variables B and C and the third one only 
has variable C The exact same row operations can be done in the matrix corresponding to the system 
in order to get the matrix on the right which now   has ones in the diagonal and zeros underneath 
the diagonal This is the row echelon form of the matrix Now this is the way a rochelon form matrix 
looks like in general Here are two different examples and in here the stars represent numbers 
that could be zero or non zero It doesn't matter So the matrix may or may not have rows full 
of zeros However if it does they need to go at the bottom Furthermore notice that every row 
that is non zero has a leftmost non-zero entry So these are called the pivots So every row has a 
particular pivot And there's a rule about pivots which said that every pivot has to be strictly 
to the right of the pivots of the row above So in other words if you stand at a pivot and you 
look at all the pivots above they all need to be   strictly to its left Now the role form as you saw 
before is very useful to tell you the rank of a matrix It's actually the number of pivots So the 
matrix on the left has rank five and the matrix on the right has rank three Now here's a very 
important note in terms of notation that we're going to use in this class On the left you see a 
roelon for matrix Now you could do some cosmetics and divide the first row by three the second one 
by one or leaving it the same and the third one   by minus4 to get a matrix that looks like this 
Obviously the stars are now different values but the important thing is that the pivots are now one 
and they're in the same location because dividing by a number won't turn a zero into a nonzero or 
a nonzero into a zero So in most textbooks the form is the one on the left So in general pivots 
different than one are allowed However for this class we are going to use the form in the right So 
we're going to take that extra step of dividing by the leading entry in the pivot in order to get 
the pivots to be one This makes no mathematical difference in terms of rank It's the same rank and 
it's more consistent with the way we are solving our system of equations by dividing by the leading 
coefficient But this is definitely something to   keep in mind So here are some other examples of 
row forms or matrices The matrix over here that you've seen before can be reduced to this one 
by simply subtracting the first row from the second row and third rows Notice that the one on 
the right is in ro echelon form Now what are the matrixes singular let's try to find the ro echelon 
form of this singular matrix over here which   you've seen before First subtract the first row 
from the second and third ones to get this matrix over here Now take the second row multiply it by 
two and subtract it from the third one and you get this matrix over here and that is the row echelon 
form of that matrix Now let's find the row echelon form of another singular matrix that you've seen 
before For this matrix what you can do is take   the first row multiplied by two and subtract it 
from the second row to get this matrix Now you can again take the first row and multiply it by three 
and subtract it from the third row to get this one   over here Now check this out Just like for 2x2 
matrices the rank of the matrix is the number of pivot ones in the row echelon form So here we have 
the row echelon forms that we just calculated And if you look at the pivot ones there's three here 
because the matrix has rank three two here because the matrix has rank two and one here and zero here 
And that's exactly the ranks of the corresponding matrices So now that you've learned the rochelon 
form let me show you the reduced ro form which is just one more step So let's say that you're 
solving this system 5 a + b = 17 and 4 a - 3 b = 6 So recall that what you do is go through 
an intermediate step of removing the variable a from the bottom equation in order to calculate 
the value of b And then you want to a step of   replacing the value of b in the first equation 
to get the value of a The solve system then has equations a= 3 and b= 2 Now following the 
same procedure with the corresponding matrix of coefficients forgetting about the constants 
17 and six you go through the intermediate   row echelon form with entries 1 0.20 and one using 
manipulations of the rows And finally some more manipulation will get you to the matrix with ones 
in the diagonals and zeros everywhere else Why is this the matrix corresponding to the system above 
because you can see the system with equations a=   3 and b = 2 as the system with equations 1 a + 
0 b = 3 and 0 a + 1 b = 2 from which this matrix with entries 1 0 0 1 comes out and recall 
that the intermediate matrix is called the row echelon form and the final one is called the 
reduced row echelon form is the one corresponding   to the solve the system And the way to get from 
the row echelon form of a matrix to the reduced roum form is simply to use each one in the 
diagonal to disappear all the nonzero entries   above For example here you'd like to get rid of 
that pesky 0.2 at the top right So in this case you can leave the bottom row untouched and from 
the first row you subtract 0.2 times the second row So you get 0 0.2 two subtract it from the 
first row and get the row one zero That's your new row one of the matrix and that's the reduced rorow 
echelon form The way a reduced ro row echelon form   matrix looks in general is this one Here are two 
examples The first rule is that it has to be in row echelon form Furthermore each pivot has to 
be a one and any number above the pivot has to be a zero So that's the main difference that 
the numbers above the pivots have to be zero And a nice property that it has is the same as the 
row echelon form property which is that the rank   of the matrix is actually the number of pivots So 
this one on the left has rank five and the one on the right has rank three And here is the general 
method to go from a row echelon form matrix to a reduced row echelon form Let's say you have one 
over here where these are the pivots and they're   allowed to be not ones And remember that for 
this course we're using a row echelon form where the pivots are one But if you were to not have 
ones that's no problem You can just divide each row by the leading coefficient So the first one 
by three the second one by two and the third one   by minus4 to get this matrix on the right where 
the pivots are ones And now all you have to do to get to the reduced roum form is to use each one 
to clear out any number above it So for example if above the one you have a five all you do is 
multiply by five the row with the one and subtract it from the previous one And so here's a small 
example Let's say that this is the row echelon form of the matrix And we're going to turn that 
into a reduced row echelon form The first thing   is to get rid of that two in the first row And 
for that we can subtract two times the second row from the first one to get rid of that And that 
changed some numbers around But at least it turned that into a zero Now let's get rid of the minus5 
So take the third row multiply by five and add it to the first row So that gets rid of that minus 5 
Now to get rid of that four take the third row and subtract it from the first one after multiplying 
it by four So multiply by four subtract it from the first one And here you get the reduced row 
echelon form of the matrix Now you're ready to learn a very famous and classic algorithm called 
Gausian elimination You will see that this is   just the elimination method you studied before 
but restructured to solve a system of equations and formalized so that it can be followed by hand 
or implemented in code Recall that when you learn   about the singularity of a matrix you ignore the 
constant values on the right hand side of your equations Those equations have constants 1 minus 
2 and minus1 but you treated them as though they were just zero To actually solve a system of 
equations you will now need to pay attention to   them Here's how it's done To begin make a matrix 
from the coefficients in your system of equations just as you always have Now add another column 
to the right side of your matrix which holds the   constant values 1 -2 and minus1 This is called 
the augmented matrix The vertical line is used to separate the constants So you remember they're 
not part of the variables And now if you proceed with the elimination method as normal you can 
use the augmented matrix to solve your system of   equations Recall that to complete the elimination 
method you'll repeatedly find an element called a pivot on the diagonal of the matrix To start 
you'll select the top left cell as your pivot Your first task will be to use row operations 
to set your pivot to one Next you'll use row operations to set all the values below your 
pivot to zero You then repeat the process row by row using row operations to simplify the matrix 
down to the reduced row echelon form Whatever row operations you perform on the matrix will also be 
applied to the column of constants you included   to form the augmented matrix As you see they'll 
eventually help us solve the system of equations So this is the highle overview Now let's see how 
this process actually works To start you'll need to turn the pivot from the first row into a one 
Since the pivot is currently at two multiply the row by 1/2 After the row operation the pivot is 
now a one and every other value in the row is also divided by two Replace R1 with the updated row 
you just calculated Notice that even the constant on the right hand side was also changed and is now 
1/2 Remember that this matrix represent the system of equations you started with So let me also 
update the system of equations to match what's   in the matrix Remember that next you want to 
set all the values below the pivot to zero using row operations I'll start with this two in row 
two This is the row operation I'll use to update   row two And hopefully you can see why The pivot 
is one and the value you want to cancel is two If you subtract 2 * row one from this row two you'll 
be left with a zero below the pivot just like you wanted Completing this row operation will leave 
you with a new row 0 3 minus 3 which I'll use to update row two in a moment Next you'll need to 
cancel out this four in row three Again since the pivot is a one it's straightforward to choose 
the row operation to do this You can just subtract 4 * row 1 from row 3 to cancel out the pivot 
Completing this row operation results in this new set of values for row three Great Now the first 
column is done I'll update the system of equations to reflect the changes in the matrix Notice that 
the first column is a one followed by two zeros So you're ready to move on to the second column and 
the second pivot Moving along the diagonal your new pivot is this three Just as before you need 
to set the pivot to one and then set the values below your pivot to zero Since the new pivot is 
a three multiply the row by 1/3 to set the pivot to one This works out as follows And once again 
update the system of equations Now you'll need to turn the second element of the third row into 
zero Once again it's easy to come up with the new row operation to do this since your pivot is 
already a one Subtracting 3 * row two from   row 3 will cancel out the three and leave you 
with a zero Completing this row operation will give you the new row 0 0 0 - 5 a zero Once again 
I'll update the system of equations to match the new values in the matrix You're almost done Make 
the minus 5 along the diagonal your final pivot As before you will need to set the pivot to 
one So divide the third row by minus 5 The   final values for row three will therefore be 
0 0 1 0 And once again update the system of equations Note that the matrix is in row 
echelon form The diagonal is all ones and below the diagonal there's only zeros The row 
operations performed in the matrix have also   changed the values of the constants and 
this is where you'll use the information in that column to actually solve the system 
of equations through a process called back substitution Here's what back substitution 
looks like You will start from the bottom row and work your way to the top You'll use the 
pivot from each row to cancel the values in the   cells above it This process actually looks 
very similar to creating the pivots in the first place So start with the pivot in the last 
row and start by canceling out the one above it The pivot is a one and you need to cancel out 
a one So the row operation will be row 2 minus   row 3 After this row operation the new values 
for row two will therefore be 0 1 0 - 1 Next cancel out this 1/2 and row 1 The row operation 
you'll need this time is row 1 minus 1/2 * row 3 Completing this row operation gives new values 
for row 1 of 1 - 1/2 0 and 1/2 Now you get the following matrix with this updated system of 
equations Finally repeat the process and use the pivot in the second row to create zeros in 
the rows above In this case you will add 1/2 of row two to row one as follows And you're done You 
get this matrix over here Notice that you got a matrix that has all ones in the diagonal and zeros 
on all other positions I will update the system of equations one more time to represent the matrix 
And you see that you now have a solution to the system where a= 0 b= -1 and c= 0 Note that the 
square part of the augmented matrix has only ones in the diagonal Such a matrix is called the 
identity matrix And by simplifying the matrix to this form using Gausian elimination you have 
solved the original system of equations Now   let's talk about the singular case Will Gausian 
elimination work if the matrix is singular well you already know that if the matrix is singular 
then in reduced row echelon form you will have   a row that is all zeros There's no need to 
worry Once you get here the algorithm just stops The whole point of gausian elimination 
is to find solutions to a system of equations If you find a row of zeros however you know your 
matrix is singular and there is no solution That said you can still determine if your matrix is 
contradictory and has no solutions or if it has   infinitely many solutions To do that you just need 
to look at the column of constants If the constant value in the row of zeros is also zero the row 
just says 0 a plus 0 b + 0 c equals 0 No matter what values you choose for a b and c the left side 
will always equal zero and this equation will be   true So this system has infinitely many solutions 
What if the system of equations changed slightly so that the third equation equals 10 well after 
row reduction you get the following matrix with the constant value in the third row changing to 
a 4 Now the last row states that 0 a + 0 b + 0 c equals 4 No matter what values of a b and c 
you choose the left side of this equation will   equal zero but the right side equals four This 
means that the system has no possible solutions To recap if you find a full row of zeros in 
row echelon form and the constant in that   row is zero then the system has infinitely many 
solutions If that constant is not zero however the system has no solutions And here's a recap 
of the whole process of gausian elimination You first create the augmented matrix by adding the 
constants to a new column on the right Next get the matrix into the reduced row echelon form And 
finally complete back substitution to find the values of your variables If you ever encounter 
a row of zeros stop as the system is singular You'll have more opportunities to study this in 
the coming assignment Okay that brings us to the end of the lecture for this week After this you'll 
have a graded quiz and a programming assignment The quiz covers all the topics you learned this 
week The programming assignment is designed to   guide you through actually implementing the 
Gaussian elimination algorithm Think of this assignment as a capstone to everything you've 
learned about systems of equations and how to   solve them through the first half of the course 
It's also a great opportunity to hone your skills with the NumPy library which you'll be using 
in the coming weeks This is definitely the most math focused assignment in the course and 
it helps prepare you for the machine learning   focused applications of these concepts and skills 
awaiting you in weeks three and four Good luck Welcome to week three The main topics for this 
week are vectors and matrices and some of their   properties such as the size and direction of 
a vector You will also learn many operations you can apply in them See in a way vectors and 
matrices are a lot like numbers Numbers can be added multiplied divided and many of these 
operations can be propagated into vectors and matrices For example one can think of the 
sum of two vectors the product of two vectors or the product of two matrices or the product of 
a vector and a matrix Furthermore just just like you can find the multiplicative inverse of a 
number For example the inverse of two is 1/2 you can also find the multiplicative inverse of a 
matrix That is under some conditions that you'll see but actually you've seen before You'll see 
throughout the week that matrices and vectors are essential in any kind of data set Another very 
important concept you learn this week is a concept of a linear transformation Linear transformations 
are very very special way to visualize matrices graphically and one that can make many concepts 
much more clear But before diving into all of that let's take a look at how these concepts 
appear in machine learning At the start of week   one we looked at linear regression as an example 
of a machine learning modeling situation where you treat your data set as a system of linear 
equations As a reminder the way that worked was that you have a data set made up of some set of 
features you called X1 X2 all the way up to XN And then you had multiple examples in your data set 
So you had multiple rows of X's in your feature matrix like this and you had a superscript 
along each row denoting the number of each example And then you had a vector of targets or 
y values And the idea with linear regression is that you come up with a set of weights w1 w2 and 
so on up to wn one for each x feature as well as a bias value that allows you to characterize 
this data set as a system of linear equations And then to simplify this expression I can just 
write a capital W to denote the vector of W values and a capital X to denote this matrix of X values 
and add B to that and set all that equal to Y hat to represent the vector of target values And just 
like that you're back to the simple equation of a line And of course if you're a linear algebra 
expert already you may notice that I might be missing a transpose on the W or X here depending 
on how I've set up these vectors and matrices But I'm not going to worry about that for the 
moment So of course real world data sets are not typically the kind of systems of equations that 
you can solve analytically like you were doing   in last week's materials But your assumption that 
this data set could be approximated as a system of linear equations turns out to be a reasonable 
one then machine learning will allow you to solve   this system in an iterative fashion and make 
predictions of your target Y for any new set of X values It turns out however that you can only 
get so far by approximating real world data sets with linear models because in many cases the 
relationship between features and targets is   nonlinear One of the most powerful machine 
learning models for representing nonlinear systems is the neural network And one of the most 
amazing things about neural networks is that under the hood they're really just a large collection of 
linear models You might have seen neural networks represented like this with these vertically 
oriented layers so-called artificial neurons that are all connected with lines like this And the way 
you can think of what this diagram represents is that you have this layer on the left representing 
your inputs to the network which are your features So x1 x2 and so on up to xn like you had before 
And I'm going to write these vertically this time inside these nodes in the input layer of the 
network And then what these lines are indicating is that you're going to send all those X values 
to each one of the individual neurons in the next layer So for example all these X's go to the 
first neuron and that neuron has a vector of W values and a B value associated with it So you can 
write what's going on up here in this first neuron as your W vector times your matrix of X features 
plus the B value So you have a linear model just like before In fact you have a whole system of 
linear equations but now it's all contained in   this one little neuron And just like before I can 
represent the system of linear equations inside this neuron as capital W * capital X + B And now 
something you don't need to worry about but that I'll mention just for completeness is that what's 
really going to happen up here in this neuron is   that you're shoving this whole wx plusb expression 
into something called an activation function And that activation function generates some output 
that here I'll just call a which is another vector And then each of these neurons in the layer does 
that exact same thing but with a different set of weights and biases in each case and generate 
their own output a vector And just to be clear that each of these neurons is unique I am going 
to add subscripts to all the W's and B's and A's where the subscript one means you're in this first 
neuron at the top and similarly on the way down And of course the matrix of X values you're 
passing through each neuron is the same in   each case So that doesn't need any subscript And 
as if we didn't have enough notation already I'm going to add a little superscript one in square 
brackets up here to denote that these operations are happening in the first layer of the network 
And that's because what happens next is that   you pass these a values to the next layer and the 
process happens all over again Now in the second layer the inputs are your A values from the 
first layer So the linear model in the second   layer here looks the same except that instead 
of X's you have A's And just to be clear these A superscript ones are now matrices containing all 
the A vectors from the previous layer This A1 A2 and so on to A N And you're multiplying those by 
a whole new set of weights and adding a different bias value in each case So everything else the 
W's and B's and the new A values you generate get a superscript square brackets two to indicate 
they're part of the second layer Then this all repeats as you propagate forward through each of 
the layers of the network multiplying the outputs   of the previous layer by a set of weights adding 
a bias term and applying an activation function until you get to your final output Now this may 
all sound pretty complicated but I wanted to walk through this in a bit of detail so you can see how 
systems of linear equations are a core component   of neural networks If you want to understand all 
this stuff in detail I will again recommend the machine learning specialization by deep learning 
AI But here in this course the point is not to   worry about all the intricate details of how a 
neural network functions Quite the opposite The point here is that there's nothing particularly 
fancy going on inside this neural network   It's really just a large collection of linear 
models that taken together can work to model highly nonlinear systems Instead of writing 
down a zillion little linear equations to represent the neural network you will instead 
represent the inputs and outputs of each layer   as vectors matrices and tensors which spoiler 
alert are just like higher dimensional matrices You will apply linear algebra to operate 
on those vectors matrices and tensors and   compute your machine learning results This 
week similar to the last two weeks if you have studied linear algebra before you might 
be familiar with some of the concepts we're   going to cover this week Take a look at the 
knowledge check in the next reading item And if you can easily answer all the questions 
then congratulations You're ready to move on   to the assessments at the end of the week And if 
you're not sure how to answer any or all of those questions then also congratulations you're in 
the right place Join me in the next video to get started In the previous weeks you've learned 
matrices as arrays of numbers A vector is a simpler array of number one that only has 
one column It turns out that vectors can be seen as arrows in the plane or 
in a higher dimensional space Two   very important components of vectors are their 
magnitude namely their size and their direction This is what you'll learn next A vector is simply 
a tuple of numbers It could be two numbers three numbers anything The number of coordinates 
in the vector is the dimension of the space   in which it lives For example the vector with 
coordinates 4 three lives in the plane and is precisely the arrow that points at the point with 
horizontal coordinate 4 and vertical coordinate 3 You can have vector spaces of higher dimensions 
For example in 3D the vector with coordinates 4 31 lives in this space and is the arrow pointing 
at the point with coordinates four three and one with respect to the axis x y and z A vector has 
two very important components The magnitude or size and the direction which I will show you next 
The magnitude or size of the vector can be defined in several ways but the great news is that all of 
them emulate the distances you use in real life The first way to measure distance is imagine 
that you live in a city with lots of buildings   and blocks formed by streets that are either 
horizontal or vertical You need to go from home to the store but you can only go on the 
streets Therefore the distance between points say if there are four blocks horizontally and 
three blocks vertically between your house and   the store is the sum 4 + 3 This is called the 
taxi cap distance And no matter what route you take you will always end up at a distance of 
seven blocks The other way you can go to the store is to forget your car and actually hop 
into a helicopter The helicopter doesn't need   to follow the streets or corners and it can simply 
fly directly Best you can do with a helicopter is the square root of 3^ 2 + 4^ 2 which is precisely 
5 This is because of the Pythagorean theorem that states that the length of the hypotenuse in a 
right triangle is the square root of the sum of   the squares of both sides Now why these distances 
because both give us ways to find the size of the vector with coordinates AB The first one the L1 
norm is defined as the taxi cap distance between the origin and the point AB which is precisely the 
sum of the absolute values of A and B Why absolute values because A and B can be positive or negative 
but the distance to walk is always positive   The second measure of the size of vector is the 
L2 norm defined by the helicopter distance between the origin and the point AB As you've seen this is 
the square root of the sums of the squares of the coordinates By default when you specify which norm 
to use we are using the L2 norm The reason is that it's the more natural one because precisely it is 
the length of the arrow The direction of a vector can also be deduced from its coordinates For 
example for this vector with coordinates 4 3 if the angle with the horizontal axis is theta then 
the tangent of theta is precisely 3 over4 This means that theta is the arc tan or the inverse 
tangent of 3/4 That is 0.64 in radians or if you prefer 36.87° Vectors can have different norms 
while pointing in the same direction For example notice that the vector 2 1.5 points in the same 
direction as the vector 43 while having a smaller norm There are many notations for referring 
to vectors some of which you'll see in this course and others that you may see in textbooks or 
resources you find online Let's show many ways of representing a vector simply called X Vectors 
can be written horizontally as row vectors or vertically as column vectors The components of a 
vector are numbered with subscripts So the second component in X is called X2 In other resources 
you may see vector names written with a little   arrow or in bold font to call out that they're 
a vector But in this course I'll just stick the plain old X Vectors can also be written 
with square brackets instead of parenthesis Square brackets can sometimes be a helpful 
reminder that a vector is part of a matrix   or is simply a small skinny matrix But there is 
absolutely no conceptual difference between these notations You'll see both parenthesis and brackets 
used in this course depending on context So let's use that vector notation to generalize the 
definition of the L1 and L2 norms to any vector with n total components In general the L1 norm 
will be the sum of the absolute values of all the components And the L2 norm will be the square root 
of the sum of all the square components of the vector Just like numbers can be added and 
subtracted to obtain other numbers vectors can also be added and subtracted to obtain other 
vectors And this can be done in a very natural   way Here it is If you'd like to add two vectors 
for example the vectors U with coordinates 4 1 and V with coordinates 1 3 All you've got to do 
is add the coordinates to get the vector 54 Now this is a nice geometric interpretation The sum 
vector is precisely diagonal in a parallelogram formed by the vectors U and V Something similar 
happens with the difference except with the other diagonal of the parallelogram So the difference 
between the vectors 1 3 and 4 1 is also taken component-wise to be 3 minus 2 That is this vector 
over here which doesn't look like anything special except if you were to translate it it matches 
precisely with the vector obtained by joining   the points 1 3 and 4 1 Here is the general case 
definition for the sum and difference of vectors Suppose you have two vectors x and y The sum can 
be expressed as the sum component by component This means that the first component of the new 
vector will be the sum of the first component of   x and the first component of y and so on Notice 
that with this definition x and y must have the same number of components Similarly the difference 
between two vectors is defined as the component wise difference The difference between two vectors 
is helpful to tell how far apart two vectors are from each other For example how different is the 
vector 15 from the vector 62 one way to tell is by the L1 distance or the L1 norm of their 
difference This is the sum of the absolute values of the components which is 8 Another way to 
tell is by the L2 distance or the L2 norm of their difference which in this case is 5.83 In machine 
learning is very useful to know distances between vectors because many times you want to calculate 
different similarities between data points and these measures are very useful Another very useful 
and simple operation you can do on vectors is multiply them by a scalar For example if the 
vector is 1 2 and you like to multiply by the scalar lambda= 3 then the result is the element 
wise product That means you multiply each of the elements of the vector by three to obtain the 
vector 36 Graphically it means you stretch the vector 1 2 by a factor of three What if the scaler 
is negative no problem What happens here is that the vector gets again stretched by the factor but 
also reflected about the origin So for example if the vector is again 1 2 and the scalar is minus2 
then the product of the scaler time the vector is -2 -4 which also corresponds to the entry- wise 
product as expected Let's formalize the definition Consider once again an n-dimensional vector x and 
let lambda be a scalar Then lambda x is simply what you obtain when you multiply each component 
of the vector x by the scalar lambda like this In this video you will learn a very 
nice and compact way to express systems of linear equations using matrices and vectors 
called the dotproduct The dot product is a   very important operation in linear algebra In 
fact you've already learned the dot product but here you will learn it formally This is how 
it works So imagine that you have the following problem You buy some fruit Let's say two apples 
four bananas and one cherry And each apple costs $3 Each banana costs $5 And each cherry costs $2 
And the question is how much does everything cost so that's an easy problem but the point is the 
way to express it The way to express the number of fruits is using a vector which is simply a 
column of numbers 2 4 and 1 That's the amount of fruits we bought of each And the prices can 
also be expressed as the vector with entries 3 5 and two That's the price of each of the fruits In 
order to find the price of all the fruits you can find the price of each fruit individually namely 
two apples multiplied by three which is the price of each apple to get 2 * 3 equals $6 That's the 
amount of money you spent on apples And you can do the same thing for bananas and get 4 * 5 = 20 
And the same thing for cherries So 1 * 2 equals 2 So six is the amount of money you spent on 
apples 20 on bananas and two on cherries And the total price of fruit is 6 + 20 + 2 which is 28 
Now what you just did was the dot product because you can abbreviate it like this And that means 2 
* 3 + 4 * 5 + 1 * 2 and that's 28 We can actually express it like this It's more common to have 
the first vector as a row and the second vector as a column then forget about the fruits and get 
the dotproduct So dotproduct of the vectors 2 41 and 352 is the sum of each corresponding pair 
of entries And that's it The dot product is very very often used in linear algebra And 
next you'll see some of its uses There's a nice connection between dot productduct and norm 
Let's go back to the vector with coordinates 43   whose norm was five And notice one thing and it's 
that 4^2 + 3 squar is actually the dotproduct of the vector and itself So it's 4 * 4 + 3 * 3 which 
is 25 And this is always the case The L2 norm is always the square root of the dotproduct between 
the vector and itself And sometimes you'll see it   written in this notation with an angle bracket on 
the left and an angle bracket on the right Let's go back to that operation I showed you before 
where the column vector was transformed into a   row vector This is called a transpose and it 
essentially transforms columns into rows You denote the operation by a superscript t and the 
result will convert this column vector into a row vector You can also apply the transpose to a 
row vector and it works exactly in the same way except now the row vector has been converted into 
a column vector You can also transpose a matrix Let me show you how by adding a second column 
This is now a 3x2 matrix To transpose the matrix simply transpose each of the columns First you 
will get the first column of the original matrix and transpose it to get the first row of the 
transpose matrix And then you do the same thing   with the second column Again you are turning 
columns into rows Notice that dimensions of the matrix swap So if you start with a 3x2 matrix 
it transpose will be a 2x3 matrix Let's conclude by looking at the formal definition of the dot 
product Start with two vectors x and y with the same number of components Then the dot product is 
just x1 * y1 + x2 * y2 and you add these products over all n components Notice that angle brackets 
are another notation for the dot product In some context it's expected that the dotproduct has a 
row vector on the left and a column vector on the   right In those cases you might see the transpose 
used on one of the vectors to get them arranged properly As you can imagine the angle between 
two vectors is very important There are some nice relations between the angle and the 
dotproduct which I'll show you in this video To begin take a look at these two perpendicular 
also called orthogonal vectors with entries   minus 1 3 and 6 2 Now take the dot productduct and 
notice that the dot productduct is 6 * -1 + 2 * 3 which is zero This always happens In fact two 
vectors are orthogonal if and only if the dot productduct is zero Now for a recap of what you've 
seen the dot product between a vector and itself is precisely the norm squared or the length of 
the vector squared The dot product between two perpendicular or orthogonal vectors is always 
zero What about the dot product between two random vectors u and v is there a nice way 
to write this and it turns out that there is   So let's look at it from the slightly different 
way You saw the dot productduct on a vector and itself is the norm squared Or the product of the 
norm and itself The dot product between a vector and a longer vector in the same direction is very 
similar It's simply the product of the two norms norm of u time norm of v Now what if vectors 
have an angle in between it's almost the same except you have to perpendicular project one 
vector onto the other one This precisely the shade that one vector leaves on the other one So 
if this vector is u prime then the dot product u and v is the product of the magnitude of u prime 
and the product the magnitude of v The interesting thing is that it doesn't matter if you project 
u on to v or v on u you still get the same dot   product An easier way to see this is this If 
the angle between the vectors is theta then the dotproduct of u and v is the magnitude of u * 
the magnitude of v * the cosine of the angle Using what you know about the dot product you can now 
tell if the dot productduct between two vectors   is positive negative or zero For example take a 
look at a vector 6 2 A vector perpendicular to it such as minus13 has a dotproduct of zero with 
the vector 62 A vector to the right of it such as 24 for example has a dotproduct of 20 which is 
positive A vector to the left of it such as minus 41 has a dotproduct of -22 which is negative 
Now why is the dotproduct with 2a 4 positive well the reason is because the projection of this 
vector on 262 has positive length And why is the dotproduct with -41 negative and the reason 
is because the projection onto the vector 6 comma 2 is negative because you have to go in 
the opposite direction of the vector 6 comma   2 So therefore the sign of the dotproduct of a 
vector corresponds to being on one side or the other one of that perpendicular vector In other 
words this happens in general So the vector is u Then the vectors that have dotproduct of zero 
with u are all the perpendicular vectors to u The vectors that have a positive dotproduct 
with u are all the vectors in this region And the vectors that have a negative dotproduct 
with u are the vectors in this region over here In this video you will learn how to multiply 
a matrix and a vector This is also something you've already learned as it is precisely how 
a system of linear equations looks Here's how it works Recall from previous videos that the 
dotproduct between two vectors is the sum of   the products of the corresponding entries So 
an equation with a known variable such as for example 2 a + 4 b + c = 28 can be written as 
the product of the row vector with entries 2 41 and a column vector with the unknown entries a 
b and c So vectors can have variables on them This represents an equation where the unknowns in the 
column vector represent the price of each of the   fruits and the number in the row vector represents 
the amount of each fruit that you bought The $28 represents the dotproduct of the two vectors 
So now imagine that you have a system of three equations with three unknowns Each one of these 
equations can be expressed as a dot product For example the first one a plus b plus c equals 10 
can be expressed as the dotproduct of the vector 1 1 which is the coefficients of a b and c And the 
vector with the variables a b and c and that's equal to 10 The equation a + 2 b + c= 15 can be 
seen as a dotproduct of 1 2 1 and ab c and that's 15 And finally the equation a + b= 2 c can be seen 
as a dotproduct of vector 112 and the vector a b c and that's equal to 12 Now it seems kind of 
clumsy to represent this with three different dot products Is there a nicer way and the answer 
is yes First thing we do is put them like this So on the left we have our system of three equations 
and three unknown variables And on the right we   have three dotproducts Now notice that the 
column vector is the same So we can simply unite the three vectors and get a matrix So now 
we get the product of a matrix and a vector And the product of a matrix and a vector is nothing 
more than three dotproducts stacked together And if you have more equations in your system then 
you have a bigger matrix But from now on we're   going to be expressing systems of equations as 
a product of a matrix times a vector Note that this is a 3x3 matrix with three rows and three 
columns And this vector has a length of three The number of columns in the matrix must equal 
the length of the vector If they don't match you are essentially trying to take the dotproduct of 
vectors with different lengths which is undefined The interesting thing however is that the matrix 
could be rectangular as long as these two circle dimensions match For example you could add a 
fourth linear equation creating a 4x3 matrix and still multiply it by the vector containing 
the three components A B and C Notice that the results of vector with length four which will 
match the number of rows in the matrix That   brings us to the end of the first lesson of this 
week After this you will have an ungraded practice quiz to help prepare you for the graded quiz at 
the end of the week There is also an ungraded lab in which you will learn how to perform the 
dot product in Python using the NumPy library   You'll need the skills you learn there on the 
programming assignment you'll complete at the end of this week Enjoy those activities and 
I'll see you when you come back for lesson two In the previous videos you have seen matrices 
as arrays of numbers that represent systems of   linear equations But there is another very 
powerful and very useful representation of matrices and it is as linear transformations A 
linear transformation is a way to send each point in the plane into another point in the plane in 
a very structured way Here I show you how In this video you'll see an illustration of how a matrix 
is a linear transformation in two dimensions But   I invite you to imagine it in more dimensions 
such as a three-dimensional space For example say you have this 2x2 matrix with entries 3 1 
1 and 2 So here's the linear transformation it corresponds to First consider two planes with axis 
labeled A and B The transformation will send every point on the plane in the left to a point in the 
plane in the right in the following way Any point has two coordinates Those two coordinates form 
a column vector To get the vector in the right we multiply the first vector by the matrix and 
whatever we get is the point in the right So this will be easier with some examples First let's 
look at the point in the origin The point 0 0 that becomes the vector 0 0 which when multiply 
by the matrix we get the vector 0 0 So 0 0 goes to 0 0 This actually always happens with linear 
transformations The origin gets sent to the origin Now let's look at the point 1 0 So the matrix 
times the vector 1 0 gives us the vector 3 1 So 1 0 goes to 3 1 Now let's look at 0 1 The matrix 
* 01 is equal to 1 2 So 01 goes to 1 2 And finally let's look at 1 1 Matrix times the vector 1 1 is 
the vector 4 3 So 1 one goes to 43 And actually this defines the entire transformation So let's 
look at this little square form that these four   points it goes to this parallelogram The square 
on the left is called a basis and so is the parallelogram on the right And they're very very 
important concepts in linear algebra and you'll   know why they're called bases in a little bit 
And a very special property that bases have is that they cover the entire plane So actually since 
this square actually tessellates the whole plane and the parallelogram teslates the whole plane 
as well then the linear transformation is simply defined as a change of coordinates So for example 
if we want to find where the point - 2a 3 goes Well the point minus 2a 3 on the left is obtained 
by starting at the origin and walking two blocks to the left and three blocks up So to find where 
that point goes in the linear transformation we simply start at the origin and walk two blocks 
to the left and three blocks up in these new   coordinates and we get the point - 3a 4 And so 
the matrix time vector - 2a 3 is the vector -3a 4 In the previous video you learned how to turn a 
matrix into a linear transformation But going the other way around is just as easy Here I'll show 
you how to start with a linear transformation and   then find the corresponding matrix So let's say 
we have some incognito matrix and we know it sends this fundamental square or basis into this one 
over here and the goal is to find the entries of the matrix So let's look at where it sent each of 
the points The 0 0 gets sent to 0 0 which always happens The 010 gets sent to 3 minus one which is 
the bottom corner The point 01 gets sent to 23 and the.11 gets sent to 52 So it turns out we don't 
need that much information We only need these two points over here And I'm going to use arrows to 
denote this point since vectors tend to be denoted by an arrow growing from the origin all the way 
to the point So the vector with coordinates 1 0 gets sent to the vector with coordinates 3 minus 
one And the one with coordinates 01 gets sent to   the one with coordinates 2 3 Now I invite you to 
take a piece of paper and do the math and convince yourself that that's how matrix multiplication 
works But the matrix that sends the vector 1 0 and   0 1 to 3 - 1 and 2 3 is precisely the one that has 
columns 3 - 1 and 2a 3 And so that is how you turn a linear transformation into its corresponding 
matrix You only look at where the two fundamental vectors 1 0 and 0 1 go And those are your columns 
of the matrix Next you'll have a chance to use an interactive tool to explore the relationship 
between matrices and linear transformations   This is probably my favorite one in the course 
You'll be able to choose different values for a 2x2 matrix and watch the way it squeezes and 
rotates a two-dimensional space when viewed as a linear transformation As always there are some 
suggested activities for you below the tool Enjoy Previously you've learned how to multiply matrices 
and vectors In this video you learn how to   multiply matrices and matrices The product formula 
is very intuitive but what I really enjoy is how it looks like for linear transformations In short 
matrix multiplication corresponds to combining two linear transformations into a third one Follow me 
and I'll show you how So imagine that you have a matrix with entries 3 1,11 2 the one you've 
already seen So let's quickly take a look at the linear transformation here The vector 1 0 goes 
to 3 1 which gets plotted here And the vector 01 goes to the vector 1 2 which gets plotted here And 
then the fundamental basis on the left gets sent to this basis on the right So let's do another 
one but let's start with the basis on the right So let's see what linear transformation corresponds 
to the matrix 2 - 1 02 So let's see how it acts on the two basis vectors which now are the vector 31 
31 goes to 25 Therefore this vector over here goes to the point 255 on the right And the other vector 
is 12 This matrix time 2 becomes the vector 04 So we plot 04 And then this parallelogram on the left 
turns into this parallelogram on the right So that is a way to see that linear transformation 
And now let's put them together So we have the first linear transformation corresponding to 
the matrix 3112 and the second one corresponding   to the matrix 2 - 1 02 Now if we forget about the 
middle one then there's a linear transformation between the first and the third and that has to 
correspond to some matrix The question is what matrix corresponds to that linear transformation 
so let's simplify a bit and only look at the basis vectors So by looking only at the left and the 
right we'd see that the vector 1 0 on the left gets sent to 52 on the right Therefore as we saw 
before the first column of the matrix is 52 And the other basis vector the vector 01 gets sent to 
the vector 04 Therefore the second column of the matrix is 04 So the combination of the two linear 
transformation gives us the linear transformation corresponding to the matrix 5024 Now is there 
any way we can obtain the matrix from the first to the third from the other two matrices and 
the answer is yes we can And that's what matrix multiplication is This operation over here is 
matrix of the left times matrix in the right equals the third matrix Now notice something that 
the matrix got flipped This corresponded to the first linear transformation the one on the left 
and this corresponded to the second one So that's   something very important to keep in mind And the 
reason is because the linear transformations act on the vector on the left So you multiply matrix 
times vector Therefore you first multiply the matrix 3112 and then multiply the matrix 2 - 1 02 
So they go in the opposite direction Now is there a fast way to see matrix multiplication without 
having to draw linear transformations the answer is yes and you've pretty much seen it already 
because it's a lot of dot products Basically look at the matrix on the left as two rows and the 
matrix on the right as two columns and take every possible combination of dot productducts between 
row and columns So for example the first row and the first column go in the top left corner and 
the first row of the first and the second column of the right go in the top right corner Now let's 
calculate these dot productducts like we already know The first one is five the second one is zero 
The third one is two and the fourth one is four So the matrix 5024 is the product of these other two 
matrices So in summary you can look at a matrix product this way as multiplying two matrices 
But you can also see it as combining two linear transformations into a third linear transformation 
Every matrix in this example is a 2x2 matrix But multiplication is also defined when your matrices 
aren't square like this Let me show you an example This time I'll multiply a 2x3 matrix and a 3x4 
matrix The result will be a 2x4 matrix Before I multiply these matrices see if you can predict 
what value would be in the bottom left cell of the result Let's see if your prediction was correct 
Remember to multiply a matrix you take rows from the first matrix and columns from the second The 
first dotproduct is the dotproduct of 314 and 3 1 - 2 The result is two The next dot product is 
314 and 051 giving a result of 9 Completing this row of the matrix gives the dotproducts of 21 and 
minus 6 Now let's check that bottom left cell of the matrix Following the pattern I'll take the 
dotproduct of the second row of the first matrix and the first column of the second matrix So 
that's 2 * 3 + -1 * 1 + 2 * -2 which gives a final answer of 1 Continuing with the same 
pattern you can solve for the remaining values of the matrix So this example show you that it's 
possible to multiply matrices even if they're both rectangular Here are the key takeaways from the 
example First the number of columns of the first matrix must match the number of rows of the second 
matrix In other words these numbers circled in here need to match Second the result matrix takes 
the number of rows from the first matrix In other words these numbers circled in blue need to match 
And finally the result takes the number of columns from the second matrix So these numbers circled in 
purple need to match Otherwise though this works   just like the 2x2 examples you saw before You 
are simply taking the dotproduct of rows from the first matrix and columns from the second matrix 
to fill in each cell of your resulting matrix When I think of numbers and multiplication I think 
of the number one as a very special number See one is special because when you multiply any number by 
one you get the same number you started with The identity matrix satisfies this exact same role 
among the matrices The identity matrix is the matrix that when multiplied by any other matrix it 
gives the same matrix And it corresponding linear transformation is very simple It is the one that 
leaves the plane intact Here's how it looked The identity matrix has a very simple look It has once 
in the diagonal and zeros everywhere else And why does it work because when you multiply it by any 
vector say of entries A B C D and E you can verify that the resulting vector is also the one with 
entries A B C D and E I encourage you to verify this with pen and paper For example the first 
entry gets calculated as follows the dotproduct between the first row and the vector which you 
can check that it's a and the identity matrix in a linear transformation is very simple as it sends 
each point precisely to itself As you can see this matrix sends the vector 0 0 to 0 0 the vector 1 0 
to 1 0 etc And that's why it's called the identity matrix A matrix can have a very special matrix 
associated with it called the inverse When I think of the inverse of a matrix I think of the inverse 
of a number Namely the number that when multiplied to it gives us one For example the inverse of the 
number two is the number 1/2 and the inverse of -5 is - 1/5 The inverse matrix is precisely that 
matrix for which the product of the matrices is the identity matrix In a linear transformation the 
inverse matrix is the one that undoes the job of the original matrix Namely the one that returns 
the plane to where it was at the beginning And here's how an inverse matrix works Imagine that 
you have a linear transformation corresponding to   the usual matrix with entries 3 1 1 and 2 And 
that turns the square into this parallelogram over here Now there exists some transformation 
that would turn this parallelogram back into   the original square And don't worry about the 
entries of the matrix We'll find them later Just know that there is one and that would mean 
that the composition of the two transformations is the one corresponding to the identity matrix 
The one that does nothing to the plane It leaves   it by itself So if the inverse matrix is the one 
with entries A B C and D that means that when we multiply the original matrix by the inverse we 
get the identity Now just like with numbers we're going to call the inverse the original matrix 
elevated to the power of minus1 In the same way that you say 2 to the minus one is 12 then this 
matrix 2 the minus one is the inverse matrix And this happens to have these entries And you can 
verify that if you were to multiply the original matrix by this one you get the identity matrix 
Now how do we find the entries in this inverse matrix the answer is by solving a system of linear 
equations So notice that if we take the entries in this matrix on the left to be equals to its 
counterpart on the right then we really have the following four dot productducts Each one of these 
dotroducts gives us some linear equation Now you have a system of four linear equations with four 
unknowns a b c and d We can use our usual methods of elimination to solve it to get A= 2s B= - 1/5 
C= - 1/5 and D= 35ths So now here's a quiz Find the inverse of the following matrix If you find 
that the task is impossible feel free to click on   I couldn't find it So by solving the corresponding 
system of linear equations we get the following we get that the answers are a = 1/4 b= -4 c = -18 
and d is equal to 5/8 Now here's another quiz Find the inverse of the following matrix And if 
you find that the task is impossible feel free to   click on I'm reaching a dead end And it turns out 
that the inverse doesn't exist We need to solve the following system of linear equations A + C = 
1 2 B + 2D equals 0 2 A + 2 C= 0 and B + D= 1 Now this is clearly a contradiction since the first 
equation says A + C= 1 And the third equation says 2 A + 2 C= 0 But if A + C= 1 then 2 A + 2 C 
has to be 2 It cannot be zero So therefore this matrix doesn't have an inverse In the last video 
you learned how to find matrix inverses and also that some matrices have no inverse What is the 
rule then for a matrix to have an inverse or not you will be happy to find that the rule is 
something you've already learned See matrices behave a lot like numbers You just realize that 
some matrix have multiplicative inverses in the   same way that some numbers have multiplicative 
inverses For example the inverse of five is 1/5 or 0.2 The inverse of 8 is 18 or 0.125 However 
not all numbers have multiplicative inverses For example what is the inverse of zero the 
inverse of 0 is not defined as there is no number which when multiplied by zero gives us one 
Now from the last video in the previous quizzes you know that this is also the case with matrices 
Some matrices have inverses like these two over here and some don't like this one over here 
So what is special about these three matrices can you look at something special well as you've 
seen before the first two are non-s singular and the third one is singular Could it be that this is 
the requirement and you'll be happy to see this is   exactly the requirement Non-S singular matrices 
always have an inverse and that's why we also call them invertible matrix and singular matrices 
never have an inverse which is why we call them non-invertible And something interesting 
happens when you look at the determinant   The determinant is non zero for invertible 
matrices in the same way that nonzero numbers have an inverse and the determinant is zero 
for non-invertible matrices in the same way that zero doesn't have a multiplicative inverse So 
I like to remember that way nonzero determinants mean that the matrix has an inverse and a zero 
determinant that the matrix does not have an inverse Now onto a machine learning application 
Recall from week one that neural networks one of the most successful and powerful machine learning 
models out there with numerous applications is based largely on matrices and matrix products Let 
me show you a concrete example of a simple neural network which actually works with the dot product 
First let's start with a small quiz Imagine that you have a spam data set and in this spam data 
set you've pinpoint two words that are quite   deterministic for spam which are the words 
lottery and win These seem to appear more on spam emails but of course their appearance doesn't 
guarantee that the email is spam So you've counted   the number of appearances in the emails that are 
spam and not spam and got this table Now the goal is the following You want to build a spam filter 
the best possible spam filter That is called a classifier which is some little machine that will 
try to guess if an email is spam or not based on the contents of this table And this particular 
classifier is going to work in the following   way You assign a score to the word lottery and 
a score to the word win Then you calculate the score of a sentence by adding the scores of the 
words with repetition For example if the score for   the words lottery and winner are three and two 
then the sentence win the lottery gets a total score of seven points Two for each appearance of 
win three for the appearance of lottery and none for the appearance of the word the Now the rule to 
guess if an email is spam or not is the following If the number of points of the sentence is bigger 
than some amount called a threshold then the email   is classified as spam Otherwise it's not Watch out 
This doesn't mean the email is spam simply means that the classifier thinks it's spam and sends 
it to the spam box Now the goal of the quiz is to find the best possible classifier That means 
one that fits the results of the table as well as possible In other words you want to find the 
best score for the word lottery and for the word win and for the threshold in such a way that the 
results of the classifier are as close as possible to the spam column of the table I'll give you a 
hint It's actually possible to find three numbers so that the classifier makes the correct guess for 
every single email in the table Give it a try And here's the answer Actually many answers work but 
out of the options of the quiz the only one that   worked was this one The words lottery and win both 
have a score of one point and the threshold is 1.5 points Notice that when you calculate the scores 
of the sentences they end up being the sums of the number of times each word appears And when you 
check if that number is larger than or equal to   the threshold of 1.5 the column of the answers 
is exactly the same column that records if the email is spam or not So this is the perfect spam 
filter for that particular data set That means the classifier did a pretty good job in the data 
set that it was given Now this is called natural language processing because the input was language 
It was words and it used these words to make a prediction This classifier can also see been seen 
graphically as follows Let's plot the data set in a plane in which the horizontal axis the number 
of times the word lottery appears and the vertical   axis the number of time the word win appears 
So the data set looks like this Notice that it's quite peculiar as a line can separate spam 
from non-spam Also this line happens to have the equation given by the scores and the threshold 
which is 1 * win + 1 * lottery is equal to 1.5 The line also gives rise to two regions The 
positive and the negative regions The positive   region is the one for which the score in the 
sentence is larger than the threshold And the negative region is that one for which the score of 
the sentence is smaller than the threshold This is precisely a classifier a linear classifier and 
it's actually the simplest neural network It's a neural network with one layer Also note that the 
model does make sense The more appearances of the words lottery and win the higher the score in the 
sentence and the more likely the email is spam A one layer neural network can be seen as a matrix 
product followed by a threshold check Here it is On the left there's the data set And in the middle 
there is the model Those two numbers one one are the scores of the words lottery and win Now let's 
look at one of the rows Say the second one It's a sentence with two appearances of the word lottery 
and one appearance of the word win Now in order to find the score of that sentence one team 
simply takes the dotproduct of the sentence   row and the model column to get 2 + 1 equals 3 Now 
after the dotproduct you can apply the check with the threshold This one returns a yes if the result 
is more than 1.5 and a no if it isn't In this case three is bigger than 1.5 So the prediction is spam 
That means the classifier thinks the email is spam and that is correct Let's do another one Say the 
fifth row This one only has one appearance of the word win and no appearance of the word lottery 
The drop product is equal to one which after the   check results in a prediction of no because the 
classifier doesn't think they will spam And it's correct again Now notice that if you had a lot 
more words you would simply have a much wider   matrix on the left and a much longer model vector 
on the right But this still works Now an easier way to do all these predictions at the same time 
is to actually take the product of the matrix and the vector So if we take the product of the matrix 
and this vector we get the vector of scores and all we have to do is apply the check to all the 
elements in this vector to get our predictions And as I mentioned before if you had many words 
you would just have a much wider matrix and a   much longer model vector But this would work 
exactly the same But for now let's go back to the two column case Here's another way to look at 
this classifier The equation to check for spam is that the score of the sentence is bigger than the 
threshold which is 1.5 But that's the same thing as saying that the score minus 1.5 is larger than 
zero In this case this minus 1.5 is called a bias So the way to include this into the matrix 
multiplication is to add an entire column with   the number one in the data table and a row with 
the bias with a minus 1.5 in the model And now instead of checking if the results is larger than 
the threshold of 1.5 we only have to check if the modified score is positive or negative That gives 
us the exact same classifier Sometimes you'll see classifiers with a bias and sometimes with a 
threshold For more complicated neural networks   the bias tends to be more common But let's not 
worry so much about the bias and continue with the threshold And instead let's look at a simpler 
problem This is called the and operator And the data set is actually very similar It's just four 
rows of the previous data set with the same labels   as before except now it's not a spam or not spam 
problem Now the end column simply says yes if both the x and the y columns contain a one and says 
no otherwise That's just the very well-known and operator for two elements Now why do I bring the 
and operator up because if you consider this as a small data set then you can model it with a 
neural network In fact the exact same one you   used for the spam detector When you multiply the 
matrix by this model vector you get these results over here which when you check with the same 
threshold as before you get predictions that look exactly like the end data set So the end data 
set can be modeled as a perceptron of a one layer neural network And graphically here's a classifier 
Notice that it's the exact same line as before which separates the points except that now there's 
only a subset of the points The equation of the line is the same 1 * x + 1 * y - 1.5 is equal to 
0 Here's a graphical way to represent the previous and perceptron The inputs are x and y and the bias 
is minus 1.5 In this diagram the number inside the node gets multiplied by the weight in the edge 
and then they get added in the next node Then   the activation function is applied The activation 
is precisely the check is the one that returns a one or a yes if what it comes is not negative 
and a zero or no if what it comes is negative And using variables this is the perceptron that 
appears in the lab The input is X coming from the data set The weights of the model are the W's and 
the bias is B And inside the node there appears a dotproduct that is added to the bias The output 
of the node then gets passed to the activation   function and that generates the output of the 
perceptor This is the end of the lectures for week three But there are still some exciting 
activities waiting for you You have a graded   quiz that covers all the topics in week three Then 
you have a programming assignment in which you'll use your knowledge of matrix multiplication to 
implement pieces of a neural network yourself   You should already be able to see the ways that 
the linear algebra concepts you're learning power some of the most widely used machine learning 
algorithms To prepare you for that assignment you'll find a couple of ungraded labs that 
teach you important skills in Python Good luck Welcome to week four This week you'll 
continue learning some of the most important concepts of linear algebra through the lens of 
linear transformations First you'll see what some concepts such as determinance and singularity 
mean in terms of linear transformations and you'll be delighted to see how naturally they appear Then 
you'll learn one of the most powerful applications of linear algebra in the real world values and IM 
vectors They are used extensively in many fields including machine learning in particular in a 
very important dimensionality reduction algorithm called a principal component analysis or PCA 
Here's a quick explanation of principal component analysis the topic we'll be covering at the end of 
this week Imagine that these points form your data set and you want to simplify your data set a bit 
For example even though the points are in space it seems that all of them lie so close to this 
line over here Perhaps we can look at the data set from the point of view of the line Perhaps 
we can look at the data set from the point of view of the line PCA would be able to find this 
line And what it would do is to simplify the data set by imagining this line to be the space and 
projecting all the points to the line Now you have a simpler data set that pretty much captures 
all the information of the original data set Right   see what happened here is that PCA helped 
you go from a two-dimensional data set to a one-dimensional data set that carried almost the 
same amount of information That's why it's called   a dimensionality reduction algorithm So in summary 
principal component analysis is a technique that is used in data science applications to reduce 
the dimensions of a data set while losing as little data as possible You can imagine a data 
set with many columns of data which can sometimes   be cumbersome to use or difficult to visualize 
The goal of PCA is to intelligently reduce the number of columns providing the benefits of a 
more compact data set without losing all the   useful information the original data set contained 
Here's a quick summary of the path we'll take to understand these concepts For lesson one you will 
start with a linear transformation represented by   a matrix just as you learned in week three 
And the goal is to be able to characterize this transformation The first thing you'll learn 
is if the transformation is singular or not For   example this first transformation of the plane is 
non-s singular But this second one from the plane to a line is singular as you may have guessed 
But these concepts are closely related to the   singularity of a matrix which you studied earlier 
in the course Next we'll return to the concept of the determinant this time with a geometric 
interpretation of its value Linear transformations can stretch or shrink space and you'll see how 
the determinant helps quantify how much stretching or shrinking that particular transformation 
produces Finally we'll learn some properties of the determinant For example if you multiply two 
matrices or find the inverse of a matrix there are some very intuitive properties of the determinant 
that arise These are very useful when you want to   analyze the inverse of your transformations or if 
you have a cascade of different transformations one after another So to sum up in the first 
lesson you will learn to characterize your linear   transformation using the determinant Now let's 
take a look into lesson two which will conclude with an exploration of principal component 
analysis which we just described First thing we will need to introduce is the definition of a 
basis A basis is a group of vectors that define   a space Moving along the vectors that make up a 
basis allows you to move to any point in a space For example any of the combinations of vectors 
shown on the right is a basis for the plane Next we'll look at the concept of span This concept is 
super useful because it tells us which space can   we access or generate with linear combinations 
of groups of vectors For example a single vector will always span a line Two vectors which are 
not pointing in the same direction and also not   pointing in completely opposite directions of 180° 
will always span a plane The last ingredient you will need to learn for PCA is the concept of 
vectors and values You will learn about these concepts in more detail but for now you can kind 
of think of vectors as the direction that a matrix points to Applying a linear transformation 
to a point along its igen vector will just move the point to a different location on that 
same vector In other words for points on these special vectors multiplying by a matrix is the 
same thing as just multiplying by a constant Een   vectors help us quickly characterize the linear 
transformations associated with a given matrix and are used in many machine learning applications 
One of those is PCA which will be the final   topic we cover in this week Okay that's a quick 
summary of what you can look forward to Let's dive in In week one of this course you learn what 
singular and non-s singular matrices are Since matrices correspond to linear transformations 
then linear transformations can also be singular   and non-s singular And you'll be pleasantly 
surprised to see that there's a very nice way to identify linear transformations that are singular 
and non-s singular Even the rank of a linear transformation can be identified easily Let me 
show you how You already saw previously how the matrix with entries 3 1 1 and 2 corresponds to the 
linear transformation that sends the basis on the left to the basis on the right and equivalently 
sends this blue grid to this orange grid over here This is also called the change of basis since 
it sends the blue square basis on the left into the orange parallelogram basis on the right But 
one special thing is this The orange grid covers every single point on the plane It turns out that 
this is precisely the trait of a non-s singular transformation If the resulting points after 
multiplying them by a matrix cover the entire plane then the transformation is non-s singular 
and vice versa The points that are covered on the right are called the image of the transformation 
So to see this let's look at an example of a   singular matrix such as this one The matrix with 
entries 1 1 22 Notice that if you multiply the matrix by the vector 0 0 you get 0 0 As usual if 
you multiply by the vector 1 0 you get 1 2 Now notice what happens if I multiply by the vector 
01 I get again one two And if I multiply it by 1 one I get 24 Which means that the square basis on 
the left doesn't go to a parallelogram Or it does but it's a degenerate parallelogram It's actually 
a line segment And there's a problem because the grid on the left doesn't get sent to the entire 
plane It gets sent to this line because when we had a parallelogram no matter how thin it was 
able to cover the entire plane on the right But   if our parallelogram is flat and it's only one 
line segment then that can only cover the orange line on the right It cannot cover the entire plane 
So in summary when a matrix is singular this thing happens You're not covering the entire plane on 
the right You're only covering a small portion of it In this case you're covering a line Now let's 
look at a very singular matrix with entries 0 0 0 This matrix has any vector with any coordinates 
into the vector 0 0 which is the origin So the entire plane gets sent to that orange point 
in the origin So that matrix is definitely very singular because the image is not even 
a plane It's not even a line It's actually   a point So here's a summary The first matrix 
sends the whole plane to the whole plane So it is non-s singular The second one sends the 
whole plane to simply a line So it's singular And the third one sends the whole plane to a 
point So it's even more singular Notice that the dimension of the image of the first one is 
two And that's precisely the rank of this matrix The dimension of the image here is one because 
it's a line And that's the rank of this matrix And the dimension here of the point is zero which 
is the rank of the third matrix So that's another way to calculate rank the dimension of 
the image of the linear transformation In the first week of this course you learn 
what a determinant is and how it's related   to the singularity or non-s singularity of a 
matrix You'll be pleased to see that in the linear transformation the determinant is very 
nicely explained as an area or as a volume And when this area or volume is zero then the matrix 
is singular Allow me to show you how Take a look at the matrix again with the entries 31 1 and 2 
Its determinant is 3 * 2 - 1 * 1 which is 5 Now recall that this matrix sends the blue square 
on the left into the orange parallelogram on   the right Now take a look at the area of the 
square It is one The area of the parallelogram I encourage you to calculate it by adding 
and subtracting areas of triangles It turns   out that this area is five which is precisely 
the determinant And that is always the case The determinant of the matrix is the area of 
the image of the fundamental basis formed by the unit square on the left So what happens with 
a singular transformation well recall that in this transformation over here with determinant 1 * 
2 - 2 * 1 = 0 The fundamental square gets sent to a very skinny parallelogram It's so skinny 
that it's actually a line segment and the line segment has area zero which is precisely the 
determinant of the matrix And finally for the very singular transformation with entry 0 0 0 
an obvious determinant of 0 The transformations has a fundamental unit squared to the point 0 
0 This point has an area of 0 corresponding to the determinant of zero So in summary we got a 
sing a non-s singular matrix with determinant five which is precisely the area of the image 
of the fundamental square Then another singular   matrix with determinant zero which is the area 
of the image of the fundamental square which is a line segment And another singular matrix 
again with determinant zero since the area   of the image of the fundamental square this time 
is a point is also zero Now here's something you may be wondering What about negative determinants 
notice that the matrix on the right that's formed   by permuting the two columns on the matrix 
on the left and for that reason it's got the negative determinant of -5 So here's the small 
technicality A parallelogram can have a negative area depending on what order we take the two 
basis vectors as follows Notice that the matrix with entries 1 3 2 and 1 sends the vector with 
coordinates 1 0 to the vector of coordinates 1 2 and the vector of coordinates 01 to the vector 
of coordinates 3 1 So it's the same as the other matrix except in the opposite order So it turns 
out that by that small technicality we say that the area of the parallelogram is negative if 
we take the vectors in counterclockwise order and positive if we take them in clockwise order 
Therefore the area of the square on the left is one but the area of the parallelogram on the 
right is going to be minus 5 But notice that a determinant being positive or negative doesn't 
actually affect the singularity of the matrix   since all that matters for the matrix to be non-s 
singular is that the determinant is different than zero Now that you know what a determinant means in 
a linear transformation you'll be able to see that the determinant of a product of matrices follows a 
really nice rule Check it out So here's a product of matrices that I encourage you to check for 
yourself and make sure that it works Now what are   the determinants of these matrices the first one 
has determinant five which is 3 * 2 - 1 * 1 The second one has determinant three which is 1 * 1 - 
1 * -2 And the third one has determinant 15 which is 1 * 3 - 4 * -3 Notice that 5 * 3 is 15 Could 
that always be the case that the determinant of the product of matrices is equal to the product 
of the determinants and you'll be happy to see   that this is always true The determinant of A is 
equal to determinant of A time the determinant of B Now if you try to work it out with matrices 
it can be a little messy I like to think of it in a much more simple way which is with linear 
transformations So the first matrix 3112 we've seen it already and is the matrix that sends this 
fundamental basis to this basis over here And D5 is the area of the parallelogram That means that 
this transformation actually blows up the areas by five The second matrix has a determinant of three 
and if you work it out is the one that sends this fundamental basis here into this parallelogram 
here that has area three and the three correspond to the determinant That means that this 
transformation blows up the areas by three Now that happens with everything So this parallelogram 
here goes to this parallelogram over here with area 15 because what the transformation does is 
no matter what area you have it blows it up by a factor of the determinant So if you consider the 
combination of these two linear transformations first you're blowing up the areas by five and 
then you're blowing up the areas by three So   you're blowing them up by 5 * 3 which is 15 And 
that is what this transformation does it blows up the area of the fundamental basis of area 1 into 
a basis of area 15 and therefore the determinant is 15 which is a product of the two determinants 
So here's a quiz to test your intuition What would you say about the product of a singular and a 
non-s singular matrix in any order would it be singular would it be non-s singular or could it be 
either one well let's say that A is non-s singular and B is singular So we know that determinant of 
AB is determinant of A times determinant of B But determinant of B is zero because B is singular 
So determinant of A has to be zero because his determinant of A * Z Therefore A is singular So if 
one matrix is singular then that matrix multiplied with any other matrix in the world is also 
singular Now it's quite logical when you think   of numbers See if you have any number doesn't 
matter which number let's say five when multiplied by zero it becomes zero So if you have any matrix 
when multiplied by a singular matrix it becomes a singular matrix because whatever it determinant 
is when multiplied by zero it becomes zero And geometrically this makes sense If you multiply 
two matrix for example this one over here which is non-s singular with this one over here which 
is singular Well the second matrix being singular means it sends everything into a part of a line 
and that means that when you combine them then it sends the fundamental basis into some segment So 
it has this determinant zero because this segment has area zero So in other words whatever area 
you have on the left it gets blown up by zero And next I'm going to show you a pretty 
interesting rule for determinants of   inverses One that you may already be suspecting 
And the way you can figure this out is with a quiz So please find the determinants of these 
two matrices And the determinants are 0.2 and 0.125 Now recall that this matrix over here is 
actually the inverse of the matrix we've been using for most of the examples This matrix 
has determinant five And as you've recently calculated this matrix has determinant 0.2 which 
is exactly 1/5 or the multiplicative inverse of five Now the second matrix is the inverse of this 
one over here which has determinant 8 and you've calculated this one to have determinant of 0.125 
which is the multiplicative inverse of 8 precisely And for the last one if you recall from a few 
videos ago this matrix over here doesn't have an inverse and it's actually a singular matrix 
of determinant zero and it has no inverse And precisely 0 has no inverse So a nice coincidence 
is spot over here which is that it seems that the determinant of an inverse matrix is the 
inverse of the determinant of the matrix And that is precisely the case When the matrix is 
invertible then the determinant of the inverse is 1 over the determinant of the matrix And why 
is this well let's check We know by the product formula that the determinant of a product is equal 
to the determinant of a * the determinant of b Now let b be actually a inverse So the determinant 
of a * a inverse is equal to the determinant of a * the determinant of a inverse Now a a inverse 
is precisely identity matrix So the determinant of the identity matrix is determinant of a * 
determinant of a inverse Now the determinant of the identity matrix is always one and therefore 
that means that the determinant of a inverse is precisely 1 / the determinant of a Now why is 
the determinant of the identity matrix 1 well let's actually calculate it Notice that for 
a 2x2 matrix it is 1 * 1 - 0 * 0 which is 1 And I'll leave you the exercise of taking a 
big matrix and identity matrix and actually calculating the determinant and verifying 
for yourself that it's always going to be one A fundamental concept in linear algebra 
is the concept of a basis You've actually seen bases many times through this course in several 
different ways but this week you learn how to identify them and use them to your advantage 
So what do we mean by a basis in the previous   week you learned that a matrix can be seen as 
a linear transformation from the plane to the plane which sends this fundamental square into 
this parallelogram and both were called bases Now why are they called bases well in reality all 
that matters here is not the four points of the square or the parallelogram but the two vectors 
that define it Namely those two vectors coming from the origin Those are the ones that I'll 
refer from now on as a basis The main property of a basis is that every point in the space can 
be expressed as a linear combination of elements   in the basis What do I mean by that well let's say 
we have these two vectors and I'm telling you that those two are a basis Why well pick any point in 
space Say this one Can we get to that point only walking in the two directions defined by the basis 
well of course we do We can do it this way or this way or this other way There are many ways to do 
this Actually there's infinitely many since you don't have to take unit steps You can walk in tiny 
fractions of steps And you can even walk backwards in that direction if you want So this is one of 
the possible bases for the plane Can you help me think of some more well there are lots and lots 
For example look at this basis I could get to any point For example this point using those 
two directions Here's one way I walk in this way and then in this one Remember that I can walk 
backwards if I want to So those two vectors also form a basis Do these two vectors form basis yes 
they do I can walk like this and then like this to get to the point So as a matter of fact pretty 
much any two vectors form a basis Now it's almost hard to think of what's not a basis What would 
be a non-basis so here's an example Let's take the two vectors over here These two vectors I can 
arrive for for example to this point over here But I could not arrive to this point over here by 
walking in those two directions because I only   have one direction So I really can only cover 
that line they cannot cover the entire plane So anything that comprises two vectors that go in 
the same direction and they could be opposites or they could go in the same direction but as long 
as they belong to the same line the two vectors do not form a basis So that is what a basis is and 
what a non-basis is Now that you know what a basis of the plane is the following question arises 
Can something not be a basis of the plane but still be a basis for some other space what exactly 
constitutes a basis for this let me introduce the concept of a span The span of a set of vectors is 
simply the set of points that can be reached by walking in the direction of these vectors in any 
combination For example you've seen that the span of these two vectors is the plane as you can get 
to any point in the plane by walking in these two   directions Likewise the span of these two vectors 
is also the plane may be a while to get to many points but one can using only these two directions 
These two vectors however don't span the plane   because as you saw before not every point can be 
reached by walking in these two directions They're the same direction So what set do they span well 
any point in this line can be reached by walking in the directions of the vectors So the span of 
these two vectors is that line What about these two that are at an angle of 180° apart well the 
span is again the line that contains them because you can get to every point on that line by walking 
on these two directions What about this one vector what's the span of that one vector well the span 
is the line that contains it and goes through the origin because that's the set of points that you 
can reach by walking in that direction So here's a question Do these two vectors form a basis 
of this line what do you think so the answer is actually no And the reason is that a basis needs 
to be a minimal spanning set And here there are too many vectors See any one of those two vectors 
spans a line So the two are one too many A basis is a minimal spanning set So each one of them 
separately is a basis of that line But the two of them are not a basis They're a spanning set 
but they're not a basis Now is this a basis of something yep That is a basis of this line And 
as a matter of fact any vector that starts at the origin and goes in that same direction is 
also a basis for that line So in in summary a basis is a minimal spanning set So the vector on 
the left forms a basis of this line But these two vectors don't because there are too many And this 
happens also for higher dimensions So let's look at this set This set forms a basis of the plane 
because it spans the plane because we can get to any point by walking in those two directions 
But if we remove any of them they don't span the   plane anymore Now they just span one line However 
this set of vectors over here do not form a basis of the plane See they span the plane because any 
point in the plane can be reached by walking in   some combination of these three directions but 
they're too big to be a basis Any subset of two of these three vectors is a basis But the third 
one is redundant So that's not a basis Now notice something interesting and is that the length of 
the basis is the space of the dimension of the   plane Now notice something interesting which is 
that the length of the basis of a space is the dimension of that space So on the left the line 
has a basis of length one and it has a dimension of one because line has dimension one and on the 
right the plane has a basis of length two formed by two vectors and the plane has a dimension of 
two So this implies that any basis of any space actually has the same number of elements in as 
any other basis And I encourage you to take a   look at for example a three-dimensional space and 
imagine how would a basis look there Now that you have a good intuition of what bases are let's 
see the formal definition To do that we'll need   to introduce the concept of linearly independent 
and linearly dependent vectors A group of vectors is said to be linearly independent if none of the 
vectors in the group can be obtained as a linear   combination of the others If you consider just 
one vector in the plane it is always going to be linearly independent Now let's add another vector 
Notice that there is no way you can get the new vector as a linear combination of the first one 
simply because they point in different directions These two vectors are still linearly independent 
What if you added this other vector instead in this case the new vector in red is pointing in the 
same direction as the green one but it's simply   twice as long as the original vector Since one 
vector can be obtained as a linear combination of the others this set of vectors is called 
linearly dependent Notice also that even though   we added a new vector to our set the span of these 
vectors did not change It remained a straight line Now let's look at a different example This time 
we'll keep the orange and green vectors that were   linearly independent and we'll add a third one in 
red While none of these two vectors is a multiple of any other one they are no longer independent 
That is because you can obtain the third vector by adding one times the green vector to three 
times the orange vector A giveaway that this is the case is that once again we added a new vector 
to our set but the span of the vectors did not change They still only span the plane It turns out 
that any other vector you add can be written as a linear combination of the first two This result 
will hold in general If you have more vectors than the dimension of the space you're trying to span 
you will always have a linearly dependent group This means having three or more vectors in the 
plane or four or more vectors in 3D space and so on Let's see how we can check if a set is linearly 
dependent or not Let's use the example from the last slide Let's call the orange vector v1 with 
coordinates minus11 V2 will be the teal vector which has coordinates 2 1 And finally v3 will 
be the red vector with coordinates - 53 To see that the set of vectors is linearly dependent you 
need to find constants alpha and beta that satisfy that alpha * v1 plus beta * v2 equals v3 In other 
words you are looking for the coefficients of the linear combination that gives you v3 So alpha 
and beta are actually numbers This gives place to a set of two equations with two unknowns 
Minus alpha + 2 beta= -5 and alpha + beta= 3 In other words we have a system of equations 
which you already learned to solve earlier in this   course Let's review one approach you could take 
Let's call them equations 1 and 2 respectively If you sum equations 1 and two you get that 3 beta 
equals -2 which means that beta= -2/3 Using this resulting equation 2 you get that alpha - 2/3 = 
3 So alpha is 11 over 3 Since you could find a solution for the system of equations then v3 is a 
linear combination of v_sub_1 and v2 and this set is linearly dependent If you were to find that the 
system has no solution then that means the set is linearly independent Now let's do a quiz I'm going 
to give you three vectors and you tell me if these vectors are linearly independent or not Actually 
they are not because the first vector * 1 plus the second vector * -1 equals the third vector So 
in fact they are linearly dependent However if you were to remove any of these three vectors then 
you get a linearly independent set But since each linearly independent set only has two vectors and 
they live in three-dimensional space because they have three coordinates None of this is a basis 
for the three-dimensional space Geometrically it would mean that if you were to plot these three 
vectors in three-dimensional space they will all   lie inside the same plane Now that you've seen 
some examples of linear independence let's come back to the formal definition of a basis A basis 
is a set of vectors that satisfies two conditions The set must span a vector space and the 
vectors in that set must be linearly independent Coming back to the examples you saw earlier these 
first two span a line or a one-dimensional space and a plane or a two-dimensional space and 
they are linearly independent So they form a basis The last two are linearly dependent and 
they do not form a basis As we see in these two examples we need to be careful to remember 
that not all sets of n vectors will form a   basis for an n-dimensional space Hopefully you're 
starting to feel more confident about the concept of span Next you'll get a chance to use a new 
interactive tool that will help you visualize the span of vectors in three-dimensional space 
As always there are some suggested activities   provided but you're more than welcome 
to just explore the tool on your own as well Now that you know what a basis is you 
should know that some bases are more useful than others In particular there is one basis 
to rule them all the basis The Igen basis is tremendously useful in particular for machine 
learning applications such as principal component   analysis as we mentioned before Here's how 
I basis work So let's look at the linear transformation corresponding to the matrix with 
entries 2 1 0 and 3 And we're going to see how it acts with respect to the fundamental basis 
So this basis has two vectors the vector 1 0 which when you multiply by the matrix you see 
that it gets the vector 2 0 and the vector 01 which when you put in the product you get the 
vector 1 3 So this square on the left goes to this parallelogram on the right and the rest 
of the plane follows So this is also called a change of coordinates or a change of bases 
because that's exactly what you're doing You're going from the square coordinates in the left 
to the parallelogram coordinates in the right   But that's something you've already seen Now 
let's get to something more interesting The choice of the square was very arbitrary Let's 
actually pick a different basis and see what happens So let's pick again the vector 1 0 that 
goes to the vector 2 0 And as a second element of that basis let's pick this vector 1 just to 
see what happens And that goes to the vector 33 So this parallelogram goes to this parallelogram 
Now what is special here well notice that the sides of the two parallelograms are parallel 
to the corresponding one on the other basis And that is very special And notice that the rest 
of the plane also follows So because these sides are parallel then what we're doing to the plane 
is we're stretching by two in this direction the horizontal direction and stretching by three in 
this diagonal direction So this is a very special basis It only consists of two stretchings That 
is what's called a nigen basis It's a very very special way of looking at a linear transformation 
with respect to a basis that sends a parallelogram to another parallelogram size parallel to the 
original one And why is that useful well let's say that you want to find the image of the point 
32 So you can multiply by the matrix and get 86 which is over here But you can also express that 
point as a combination of elements in the basis What does that mean that we find a path to 
get to that point using the two directions that we have And now the linear transformations 
corresponds to stretching the horizontal vector by two and the vertical vector by three And so 
this really simplifies the linear transformation It just consists of two stretchings So the two 
vectors in the basis are going to be called the en vectors And the stretching factors the two and 
the three are going to be called values And values and vectors are tremendously important in linear 
algebra because they really really simplify our calculations Let's look a bit more closely 
why values and vectors are so special I'm going to multiply the matrix 2 1 03 by the two 
vectors that I already shown you are vectors and one that isn't The first vector 1 0 is an igen 
vector After the transformation it becomes 2 0 Notice that the vector is still pointing in 
the same direction If I wanted I could write   the result like this 2 * the original vector 1 0 
The second vector 1 1 is also an vector After the transformation it becomes 33 Again it's pointing 
in the same direction and so I can rewrite the result as three times the original vector The 
final vector minus one 2 is not an vector After the transformation it becomes the vector 06 Notice 
that it's no longer pointing in the same direction and there's no constant that I can multiply the 
original vector by to get the final vector 0A 6 Let me formalize what these two equations are 
saying I'll call our matrix here A and I'll   call the first vector here V1 Then this equation 
says that for this special vector V1 A * V1 is the same as multiplying V1 by the scalar lambda 
1 I'm using more formal notation but in this case you already know V1 is the vector 1 0 and lambda 
1 is the scalar 2 The second equation is nearly identical It says that a * v2 is equal to the 
scalar lambda 2 * v2 You already know that the vector v2 is 1 one and the scaler is three V_sub1 
and v2 are the matrix aim vectors and lambda 1 and lambda 2 are the matrix a's values Notice that 
vectors and values come in pairs So v_sub_1 and lambda 1 form a pair and v2 and lambda 2 form 
a second pair You may still be wondering why again vectors are so special Let's look more 
closely at what this equation is saying On the left side of these equations you have a matrix 
multiplication and on the right side you have a scalar multiplication And one big difference 
here is that matrix multiplication is just a lot more work Even for this small 2x two example 
calculating the left side of the equation requires eight multiplications while the right side only 
takes two multiplications For matrices with hundreds or thousands of columns the difference 
becomes astronomical What this equation is saying then is that at least along a matrix vectors 
you can turn a large computation into a smaller one And if you apply what you know about bases 
you can actually use this shortcut everywhere Let me show you how This is the example I showed 
you earlier The red vector is not an igen vector So to find out where it was sent you needed to 
just complete the matrix multiplication as normal Notice however that the two vectors 1 0 and 1 are 
linearly independent and span the plane So they form a basis This is actually the basis of the 
matrix 213 I will use this to rewrite the equation Since the vectors 1 0 and 1 1 form a basis then 
you know that you can write the vector -12 as a linear combination of those basis vectors In this 
case it's -3 * 1 0 + 2 * 1 1 So you can simply replace -1 2 by - 3 * 1 0 + 2 * 1 1 This is the 
same vector -12 you had before but rewritten as a linear combination of the vectors in your basis 
Now I'll rearrange the equation slightly All I did was move the matrix multiplication inside the 
brackets and move the minus3 and the two forward And at this point you are ready to use your vector 
shortcut Look up here You have already calculated these matrix multiplications And you can just 
substitute the scalar multiplications like this Now your equation is just scalar multiplications 
You have -3 * 2 * the vector 1 0 + 2 * 3 * the vector 1 1 You can quickly multiply through 
these scalers and finally solve this equation to get the vector 06 which if you remember is 
the vector you solved for before Only this time even though the red vector is not an igen vector 
you were able to solve this linear transformation   without doing any matrix multiplication You just 
used scalar multiplication the entire way Okay I need to admit I lied a little bit here Remember 
earlier where I just gave you the red vectors coordinates with respect to the igen basis -32 
Perhaps you could have eyeballed that answer in this simple case But in general there's 
actually substantial work involved in getting   those coordinates I haven't taught you why this 
is the case But to do this for every point in the plane you need to calculate the inverse of your 
basis and then multiply that inverse by the red   vector Or in other words kind of a lot of work 
and even a matrix multiplication So at least in this example a more accurate way of describing 
a vectors is not that they remove all the work but they let you decide when you want to do the 
work In some machine learning context it may be worth to calculate these coordinates ahead of time 
so that when it's time to apply a transformation   you can do it much more quickly You'll see other 
powerful applications of vectors in this course as well For now here are the main takeaways about 
values and vectors They are characterized by the equation avals lambda v for each pair of vectors 
and values As you just saw this means that along that vector a matrix multiplication just becomes 
a scalar multiplication Visually you can think of vectors as telling you the direction in which a 
linear transformation is just a stretch and the values tell you how much it is stretched You can 
create a basis for the vectors called the basis You will often see this basis represented 
as a matrix with one vector in each column From the perspective of this basis the linear 
transformation A is just a collection of stretches And as you'll see vectors can save work and help 
characterize a linear transformation in powerful ways Now that you know what an igen basis 
is you may be wondering how to find it The process is actually not too difficult It entails 
solving an equation with a determinant Here it is First take a look at the matrix with entries 2 1 
0 and 3 and take a look at how it acts on these points around the square This should make the 
entire transformation clear As you've seen before   the two horizontal vectors get stretched by two 
and the diagonals get stretched by three And for the other points this happens Now compare this 
to another matrix one that simply stretches the entire plane by a factor of three in any direction 
This matrix has entries 3 0 0 and three And it's really three times the identity matrix Notice 
one thing These two transformations are not the same transformation but they do coincide in many 
points In other words they act the exact same way for infinitely many points All the points in this 
line So to be more specific in this diagonal the two transformations do the exact same thing So 
they match in infinitely many points Now that is strange The transformations should only match 
at one point the point 0 0 When they match at infinitely many points something non-s singular 
is happening And what's happening well let's look at their difference If these two transformations 
match at infinitely many points that means their difference is zero at infinitely many points So 
if you apply the difference of these two matrix to any vector in any of these diagonals you get 
the vector 0 0 In other words this matrix times vectors xy for infinitely metric vectors is 0 0 
Now that is the trait of a singular transformation Recall that a non-s singular transformation has 
a unique solution to the equation matrix* vector equals 0 And that's the vector 0 0 So if you have 
an infinitely many solutions to this means your matrix is singular and you can verify that this is 
indeed a singular matrix as a determinant is zero Now let's do something similar but for another 
transformation on the right Our transformation does exactly as it did before And now let's 
compare it to the transformation that stretches the plane by two in every direction These two are 
not the same but they match in this entire line So in other words matrix of the left * xy is equal 
to matrix of the right * xy for any vector xy on this line that's infinitely many points So we can 
do the same procedure take the difference and that matrix times a vector equals 0 for infinitely 
many vectors That means that the matrix 0 1 01 or the difference between our matrix and 2 * 
the identity is a singular matrix And you can check indeed that it is singular matrix because 
it determinant is zero So what is special about the values so what happened for the value two and 
the value three let's think about it in general If lambda was an value then the transformation 
given by our matrix and the transformation given by scaling the plane entirely by a factor of 
lambda are equal for infinitely many vectors xy That means their difference times a vector 
is equal to 0 0 for infinitely many vectors It's an equation with infinitely many solutions 
Therefore this matrix 2 - lambda 1 0 3 - lambda has to be a singular matrix So it determinant is 
zero It determinant is given by this equation when we expand it and that's called the characteristic 
polomial So basically to find the values lambda all we need to do is look at the characteristic 
polinomial and find the roots The place where the characteristic polinomial is zero are the 
values So in this case they're going to be 2 and three So now that you have the values let's 
try to find the vector So recall that the igen vector is the vector that satisfy the equation 
matrix time vector equals value time vector So if we expand this we get these equations over here 
and the solution for these are x= 1 y = 0 or any multiple of it So here is one of the val vectors 
the one corresponding to the value two We do the same thing with three Solve for these equations 
and get 1 one So the vector 1 one is corresponding to the value three Now you're ready for a quiz 
Find the values and vectors of this matrix And the solution is that for the values it's 111 and for 
the en vectors is 21 corresponding to the value 11 and minus 1 2 corresponding to the value 1 Why 
well if you look at the characteristic polomial it is the matrix with entries 9 - lambda 4 and 
3 - lambda So that expands as lambda square - 12 lambda + 11 which factors as lambda - 11 * lambda 
- 1 Therefore the Igen values are lambda= 11 and lambda= 1 And I'll leave it as an exercise for you 
to solve the equations for the vectors and verify that they are going to be 2 1 and minus12 or some 
multiple of them Right because all that matters is the direction The process of finding vectors for 
a 3x3 matrix is very similar Consider a= 2 1 - 1 1 0 - 3 - 1 - 3 0 The characteristic polomial is the 
determinant of a minus lambda* the identity matrix of whatever size you need In this case lambda 
minus i will be a 3x3 matrix that looks like this The characteristic polinomial with the determinant 
of the difference of these two matrices using the   diagonal method to calculate the determinant 
of a 3x3 matrix You can construct the pieces of the characteristic polomial Then combine terms to 
get the polomial lambda cubed + 2 lambda^ 2 + 11 lambda - 12 = 0 Factoring this polinomial which 
I won't show in detail here will give you these three factors And now it's easy to find the zeros 
of this equation which are when lambda is minus3 1 or 4 And so that gives you the three values of 
this matrix Now that you found the three values for matrix A let's find the vectors associated 
to each value Let's begin with our last value four Remember that to find the value you need 
to solve the equation AV= lambda V Then that means you need to solve this equation where X1 
X2 and X3 are the three components of the vector Multiplying the four by the vector gives the new 
vector 4x1 4x2 4x3 And completing this dotproduct on the left gives you this new vector Now all you 
need to do is set these three vectors equal to one   another and solve for x1 x2 and x3 I'll rewrite 
the system of equations up here Now subtract the terms from the right hand side leaving just zeros 
This gives the final system of equations you'll   need to solve which I'll label rows 1 2 and 3 You 
can use these equations to solve for x1 x2 and x3 Adding rows 2 and 3 you get -7 x1 - 7 x3 = 0 which 
simplifies to x2 being equal tox3 Adding 3 * row 1 to row 3 you get -7 x1 - 7 x3 = 0 which simplifies 
to x1 also being equal to x3 This is actually as far as you can get solving the system And the 
result is that there are infinitely many solutions   Any vector that satisfies x1= k x2= k x3= minus k 
will be a solution for any value of k For example 1 1 -1 works But 2 2 - 2 would work as well And 
this makes sense There are actually always going to be an infinite number of potential vectors that 
lie along the same line In this case though let's   just keep it simple and say the vector is 1 one 
minus one And that's it You found the first vector If you want to find the other two vectors you 
can just repeat the process with the other two   values You already found that the value force 
is associated with the vector 1 1 - 1 And if you follow the same process for vectors 1 and -3 
you could find their vectors 011 and 2 - 1 1 If you notice the two examples you worked on were 
for 2x2 and 3x3 matrices all of which are square matrices Could you comput the values and vectors 
for any matrix of any shape remember to get the values you need to solve for a determinant But as 
you learned in previous lessons the determinant is   only defined for square matrices So for any square 
matrix you can find values and vectors However the matrix is not squared like the one right here 
then it doesn't have any vectors or values In the example from last video you had a 3x3 matrix which 
had three different values and three different vectors one for each of the values Could it be 
that all 3x3 matrices always have three vectors well it turns out that that's not always the case 
Let's look into some examples Consider this 3x3 matrix A with the values shown here Now find 
the characteristic polomial as the determinant of A minus lambda I Using the formula for the 
determinant for 3x3 matrices we get 2 - lambda^ 2 * - 4 lambda All the remaining terms are zero 
due to all the zeros in the original matrix Now to find the igen values you need to find the zeros of 
this polinomial which in this case is super simple   This gives the igen values 4 2 and two Notice that 
the number two is repeated twice Now watch what happens when we find the vectors associated with 
them Let's start with the value four The vector needs to satisfy that a * the vector x1 x2 x3 
equals the vector 4 * x1 4 * x2 4 * x3 The product between the matrix and the vector can be expanded 
as the vector with entries 2x1 - x1 + 4x2 - 0.5 x3 and 2x3 And we want this to be equal to the 
vector 4x1 4x2 4x3 which leads to this system of equations This one can be written as -2x1 = 0x1 - 
0.5x3 = 0 and -2x3 is equal to 0 Notice that all I did here was moving everything to the left of 
the equal sign Now we can simplify a little bit and get that the first one is equivalent to x1= 
0 The third one is equivalent to x3= 0 And now since x2 doesn't appear in any of these equations 
at all then the second equation is zero for free and x2 can be any number in order to satisfy these 
equations So to keep it simple we can set x2 to be equal to one and you get the igen vector 0 1 0 
associated to the IG value four Now let's repeat for the value 2 In this case a dotproduct with 
the vector x1 x2 x3 should be equal to the vector 2x1 2x2 2x3 and the dotproduct can be expanded 
with the same expression as before We again get a system of equations 2x1 = 2x1x1 + 4x2 - 0.5 x3 
is 2x2 and 2x3 = 2x3 When we manipulate this by taking everything to the left of the equal sign 
we get that the first equation is quite trivial It's 0= 0 The second one is -1 + 2x2 - 0.5x3 = 
0 And the third one is again trivial It's 0= 0 You can rewrite the second equation as x1 = 2 * 
x2 - 0.5 * x3 This equation has infinitely many solutions depending on what values of x2 and x3 
you choose So let's say that you choose x2 = 1 and x3 = 0 That means x1 has to be 2 for it 
to be an igen vector given the vector 2 1 0 But you could also choose x2 to be one and x3 to 
be two which gives the igen vector 1 2 These two vectors point in different directions So they 
are actually two different vectors Remember that you only care about the direction of the igen 
vector since any scaled version of it will still   be an igen vector to the same value It is worth 
mentioning that you could have found different pairs of vectors depending on the values of x2 and 
x3 But the important thing is that you can always   find two distinct directions for the igen value 
two To sum up this matrix A has the following pairs of values and vectors The first value lambda 
1 which is four has an vector 0 1 0 The second value lambda 2 which is 2 has an vector the vector 
2 1 0 And finally the third value lambda 3 also equals 2 and has as an igen vector 1 1 2 In this 
case you're able to find three distinct vectors even though you had a repeated value Let's look 
into one more example where this doesn't happen Let's change just one value in the matrix and see 
what happens So the characteristic polomial stays the same as before and once again we're going to 
have the values four 2 and again two as values Let's repeat the process from before to find 
the IGEN vectors and start with the IGEN value   four The only equation that changes here is the 
last one where now you have 4 * X2 + 2 * X3 And that gives us this system of equations where the 
first two remain the same being -2 * X1 = 0 and -1 - 0.5x3 = 0 And again let's name them equations 
1 2 and 3 respectively From equation one you get that x1 has to be zero And combining equations 
three and one you get that x3 has to be zero This leaves again x2 to take on any value Just as with 
the previous matrix you can choose the vector 0 1 0 as the igen vector And it's the same one for 
the previous matrix For the igen value 2 the same thing happens when you solve this equation You get 
that 2x1 is equal to 2x1 That -x1 + 4x2 - 0.5 x3 is 2x2 And that 4x1 + 2x3 is 2x3 Which gives rise 
to the following system of equations that can be simplified as 0 = 0 as x1 + 2x2 -.5 x3 = 0 And 
finally 4 * x1 = 0 This imposes the restriction that x1 can only be zero Now from equations 2 and 
3 you get that x3 must equal 4 * x2 Now you only have one degree of freedom since x1 is always 
zero and once you fix x2 the value of x3 gets fixed as well For example you can consider x1 = 
0 and x2 = 1 which makes x3= 4 given the vector 0 1 4 What would happen if for example you choose 
x2 to be 1/2 then x3 must be two given the vector 0 1/2 2 However these two vectors lie on the same 
line One is simply a scaling from the other which means that they are actually the same vector 
You can test it You can't find any vector which   satisfies equations 1 2 and three that spans a 
different line This means that the number two is 2 * an value But you can only find one vector 
associated with it Then the igen vectors are of the form 0 k 4k But there's only one direction 
which can be paired with value two Even though the number two appears twice as an igen value So recap 
for the matrix entries 2 0 0 1 4.5 4 02 you have an igen value of four associated to the vector 0 
1 0 then an value of two associated to the vector 014 and again an value of two which doesn't have 
an associated vector That means that you can't create an igen basis for the three-dimensional 
space because you're lacking one vector to span   the whole space As a summary if you have a 2x2 
matrix with IGEN values lambda 1 and lambda 2 If the two values are different then you always get 
two distinct vectors However if the igen values are the same then you can have either one or 
two vectors If you have a 3x3 matrix with values lambda 1 lambda 2 and lambda 3 then you get some 
more options to explore If all the three values are different then you can always find three 
distinct vectors If you have one value repeated twice with the other being different then you can 
have either two or three vectors These are the two examples you just explored However if you have the 
same value repeated three times you can have any number of vectors between 1 and three In this 
week's programming assignments you will get to explore many more interesting examples of matrices 
with different properties and number of vectors The final major topic of this course will be 
principal component analysis or PCA The goal of PCA is to reduce the dimensions or number 
of columns of a data set while preserving as much information as possible Put simply PCA 
takes a large table or data set and converts it into a smaller one The original data set has 
many rows or observations and many columns or features that store useful information about 
each observation PCA will reduce the number of features in your table while maintaining the same 
number of observations Put another way the data set will have the same number of rows and fewer 
columns It'll be just as tall but it will get skinnier Here's an example data set collected by 
an online store about its customers This table has four observations and five features Customer age 
account age days since last login total purchases and total money spent There are two primary 
reasons you may want to reduce the dimensions of a data set The first is just that it's literally too 
big and you want to work with something smaller In this example you only have five features but 
in some machine learning contexts you could easily have hundreds or thousands of features 
It is useful to be able to shrink data sets to   a more manageable size The second is to help 
with visualization especially in exploratory analysis Many common charts like a scatter plot 
or bar charts really only help you visualize your data when looking at one or two features at a time 
Reducing the number of dimensions or columns you need to consider makes it easier to quickly 
visualize your data and look for patterns So you want to reduce your table's dimensions 
And the question is how should you do it well here's one easy approach Just start deleting 
columns You could easily delete the last two that contain the total purchases and total amount spent 
by each customer This data set is looking easier to use already Unfortunately however you've 
also deleted a lot of useful information Your data set has fewer dimensions but any insight 
you could have gained from those two columns   is lost PCA is designed to address this problem 
It will allow you to still reduce the dimensions of your data but it also preserves much of the 
information you might lose if you simply started   deleting columns The idea behind dimensionality 
reduction is to move your data points into a vector space with fewer dimensions This is 
called a projection You already have studied all those concepts underlying projections So 
let me show you how they work with an example Suppose you have this table of data where you have 
four observations of variables x and y which on the plane looks something like this Now imagine 
you want to move or project your data onto this line which has equation y = x All the points move 
perpendicularly towards the line except the point one which was already on the line But where do 
these points end up i'll start with the easiest example the point one one which didn't move at all 
Before I would have given this point location as one one but those are actually two-dimensional 
coordinates I can actually now give this point's location as one coordinate Its distance along 
the line from origin that is this distance here From some basic trigonometry you can tell that 
the length of this segment is the square root   of two With just this number you could find the 
location of the point along the line It might not seem super clear to you why you would want to 
do this but notice that square of two can be rewritten as 1 + 1 divided by the square 
of two Here the coordinates of the original point are starting to show up Let's see why this happens 
Let's start by trying to get that 1 + 1 This will come from the dotproduct of the coordinates of 
the first point with some other vector shown here in orange to choose that orange vector Note that 
the line y= x is actually the span of the vector with coordinates 1 one So let's use that vector 1 
one Now if you take the dotproduct of that first row in the table and the orange vector 
you're essentially saying take 1 * the   point x coordinate and 1 * the point y coordinate 
to find the new location projected along the line However summing the two variables gives you a 
longer vector than expected This vector's length is two but you know the final length of this 
new vector is supposed to be the square root   of two In other words it overshot by a factor 
of 2 So go ahead and divide by that Now you get exactly the point you were looking for 
Note that 1 / 2 is actually 1 / the norm of the vector 1 one And this is the main idea of 
a projection Multiplying by the vector projects the points along that vector and dividing by the 
vector's norm ensures that there is no stretching introduced Another way of thinking about this 
is that you're just changing your vector to   have a new norm of one Now let's see what happens 
for the second row When you multiply the second row by the vector 1 1 you end up getting the 
sum of 1.2 + 1.6 which again you went too far If you scale it back by square of 
two however you get exactly the point   you were looking for Same thing happens 
for row 3 you will multiply with vector 1 Just as before you overshoot And 
again dividing by 2 will get you to the desired point Finally the same 
process holds for the fourth observation So this vector gives the final coordinates of 
each point after projecting each point on to the x= y line So the four points with two variables 
each simplify to the vector 1.4142 1.9799 minus 0.2121 and minus 1.344 As you may have noticed you 
now need only one column vector instead of a two column matrix in order to represent your points 
locations along this line In general if you want to project any matrix A onto the direction given 
by vector V you first need to multiply matrix A by vector B However as you just saw you need to 
scale vector V so that it has norm one So divide B by its own L2 norm And this is the projected 
matrix which we are calling A It is important to keep track of dimensions If A has R rows and 
C columns then vector V must have length C You can also think of this as a C by one matrix So 
the projection has R rows and one column You can project onto multiple vectors at once Projecting 
onto two vectors is the same thing as projecting   onto the plane those vectors span In this case 
simply create a matrix of size C * 2 where each column is each vector V1 and V2 each divided by 
their norm Let's call this matrix V The result will be a projection with R rows and two columns 
which means you have the same number of data points but now only two variables In the end the 
projection can just be represented by the simple equation A= A V Once you build your V matrix all 
you need to do is multiply Projections are a very useful way of reducing the amount of information 
you need to store in your data set The question now becomes how do you pick the vectors to 
project onto and that's what we'll be looking into next Now that you've seen the concept of a 
projection let's see how it would be used by PCA to reduce the dimensions of a data set 
In this graph each dot represents a different observation composed of two features graphed as 
their x and y position Reducing the dimensions of this data means moving from two-dimensional data 
graphed as points in the plane to one-dimensional data graphed as points along a line As you 
can see the set is not centered around 0 0 So let's start by doing that Now let's see 
what happens if you project onto the xaxis Let's save this projection for later Now let's 
project onto the y ais You can already see that the points in this projection are less spread 
because the points are closer to each other You can project into any line For example this 
one which can be represented by the equation   y + x= 0 which is equivalent to projecting 
onto the 1 - one vector since it spans the line Or you could also consider this other line 
that solves the equation 2x - y = 0 which is projecting onto the one two vector or even 
in this direction over here Notice that this line fits the data pretty well and the resulting 
projected points are still relatively spread out As you can see after these projections the data 
points may be more or less spread out and that's going to end up being very important The reason 
is that more spread out data points preserve more   information from the original data set In other 
words preserving more spread means preserving more information Now I'll sort the projections from the 
one that has the points most spread out the least The projection at the top has the points most 
spread out and therefore it preserves the most   information from the original data set And the 
projections at the bottom have the least spread and therefore preserve the least information So 
the goal of PCA will be to find the projection   that preserves the maximum possible spread in 
your data even as you reduce the dimensionality of your data set And again here are the main 
benefits of dimensionality reduction in general and in PCA Dimensionality reduction makes data 
sets that are easier to manage simply because they're smaller PCA in particular allows you to 
reduce dimensions while minimizing information loss Thanks to this reduction in dimensions 
it is possible to analyze and visualize data   in ways which might not have previously been 
easy or possible Now this was a really quick introduction to the concepts underlying PCA In 
the following videos you'll learn how it actually works PCA relies on a few concepts from statistics 
which I'll be introducing in this video If you studied statistics in the past this will be 
a helpful review And if not you'll learn more   about these ideas in the third course of this 
specialization With that let's start with the first statistical concept the mean Consider this 
data set Each point in the plot is an observation composed of two variables x and y Each point is 
at a location x i y i The mean of your data is simply the average value of all the observations 
which will be somewhere around this point Here's how you find it mathematically For the x variable 
sum all n values of x and divide by n The mean of y works in the same way You just average the 
values of each feature This middle point therefore has the coordinates mean of x and mean of y 
Next you'll understand the concept of variance which describes how spread out your data is If 
you wanted to describe how the dots appear in the chart you might say that the dots are more 
spread out or have a larger spread if we look at them along the horizontal axis and are more 
compact or have a smaller spread if we look at them along the vertical axis In statistics this 
spread is measured or described by the variance of a data set A data set with no spread of values 
has a variance of zero and a data set with a large spread will have a large variance To see this 
more clearly I'm going to take our two-dimensional chart and move every point to the horizontal 
axis like this Again without yet worrying about how variance is calculated we can see that this 
variance along the horizontal x-axis is relatively large If we repeat the process along the y-axis 
the values are spread out across a smaller range The y variance is relatively small but still 
bigger than zero since there is some variation   in the values Here's the equation for calculating 
the variance of a list of values To help you understand the different parts of this equation 
consider a very simple data set with one column   representing the variable X and five rows numbered 
1 to 5 Each row is an observation XI First you'll need to calculate the mean of X by simply adding 
up the values of X I and dividing by five This gives us a total of 45 and a mean of 9 Next find 
the difference between every value in column Xi and the mean you just calculated or in other words 
just subtract nine from each row Now square each of these differences placing the results in a new 
column The summation tells us to sum these five squared values giving a total of 64 And finally 
divide this total by n minus one Since n here is 5 we divide by 4 giving you a variance of 16 When 
this equation is written in formal notation some of these terms are typically shortened Variance is 
often shortened to bar and the Greek letter mu is used to represent the mean Another way to think of 
variance is as the average squared distance from the mean The fact that it's the average square 
difference is a little odd but the most important takeaway here is that as your data becomes more 
spread out and an average is farther from the mean variance will increase Returning to the data set 
from earlier you now refer to that average point as mux mu y And you could calculate the variance 
of X and Y using the formula you just reviewed The X variance is larger than the Y variance And from 
that formula it's clear why that's the case Viewed along the X-axis these points are further away 
from mux So the average square distance will also   be larger Meanwhile along the y-axis the average 
squared distance from muy is smaller Variance helps us quantify how spread out your data is 
But now consider a situation where variance   alone wouldn't be helpful These two data sets have 
three observations each They would have identical Y variance and X variance And yet it's obvious 
that there's a significant difference in the patterns of these data sets The solution is now a 
measure called coariance Coariance helps measure how two features of a data set varies with respect 
to one another Notice that in the left data set the pattern within the data is down and to the 
right As X values increase Y values decrease In the right data set we have the opposite pattern 
As x values get larger y values get larger too Coariance quantifies this relationship resulting 
in the left data set having a negative coariance and the right data set having a positive coariance 
Now that you have a highle understanding of what it's trying to measure let's see how coariance 
is actually calculated The equation for coariance looks like this It can be a little complicated 
at first so I'll break it down into pieces As you can see however it looks pretty similar to the 
variance equation And if you expand out the square   term at the end they're nearly identical The 
only difference is that inside the circle items which now depend on the values of both the x and 
y variables as well as the average of these values   mux and muy Let's look at these three example data 
sets to understand how this equation works For the first data set we expect to calculate a negative 
coariance as the data is trending downwards In the second case the trend line fitting the observation 
seems flat So we'd expect a coariance of zero or   a very small value And in the third case X and 
Y seem to trend upwards together which should result in a positive coariance To understand 
the impact of the circled items you can draw   the mean point new X new Y on top of each data set 
Subtracting mux from each X and mu Y from each Y essentially recent the data around this point And 
you can imagine it splitting the plane into four quadrants Each point in the data set lies in 
one of these quadrants and results in either a   positive or negative contribution to the overall 
coariance In the first quadrant both X and Y are greater than their means So the product in the 
circle term is positive In the second quadrant   X is less than its mean but Y is still greater 
The circle term is now the product of a negative and a positive number and the results are negative 
In this third quadrant both X and Y are less than their mean The circle term is now the product of 
two negative numbers so the result is positive And   for dots in the final quadrant x is greater than 
its mean but y is smaller The circle term is a product of a positive and negative number and the 
result is negative Recall that the first part of   the coariance equation is just asking to take 
a sum and divide by the number of values minus one In other words we're more or less averaging 
all these products So in a somewhat simplistic way you can think of coariance as just asking 
on average are there more dots in the positive quadrants or in the negative ones for the first 
data set more are in the negative quadrants So   the coariance will be negative In the second the 
dots are about equally spread between positive and negative quadrants which results in a coariance 
close to zero And in the third most of the dots   are in the positive quadrants leading to a 
positive coariance Whether you feel like you fully understand the coariance equation what's 
most important at this point is to understand   intuitively what it measures For now you can 
think of coariance as measuring the direction of the relationship between two variables A 
negative coariance indicates a negative trend A small coariance indicates a flat trend or no 
relationship And a positive coariance indicates a positive trend So long as you're comfortable 
with these concepts you're ready to move on You've been learning a lot about variance and 
coariance because your ultimate goal will be to build a special matrix called the coariance 
matrix It is a compact way of storing all the   relationships between pairs of variables in your 
data set To introduce this concept let's look at three data sets All three X variables measured 
on the horizontal axis have more or less the   same spread I won't worry about calculating the 
variance here precisely so I'll just say that each one has an Xvariance of three Viewed along 
the vertical axis these data sets also have more or less the same spread though it's a bit smaller 
than the spread in the X direction Here I'll say they have a Y variance of one Again don't worry 
if these values are exactly correct It is clear however that the coariances are different The 
first example has a downward slope so I'll say it has a coariance of -2 The second one seems to 
have a flat slope so I'll say the coariance is   zero And the last one has a positive trend so I'll 
say it has a coariance of positive2 Using these measures I will now build a coariance matrix for 
each data set In the diagonal I will put the X and the Y variances and in the off diagonal I will put 
the coariances This is called the coariance matrix and it simply store the coariances and variances 
for each pair of variables Let's formalize the process a little bit more You first calculate the 
variance of each variable and the coariance of each combination of variables Here that's just the 
variance of X the variance of Y and the coariance of the two variables Next you build a square 
matrix which I'll refer to with a capital C with a row and column for each variable in your data 
set Here we have just two so they are named X and   Y At every position in your matrix you place the 
coariances of the variables of that row and column So in these two positions we place the coariance 
of X and Y Notice that the coariance of X Y is the same as the coariance of YX And you can verify 
this by using the formula you learned previously Along the main diagonal you place the variance of 
each variable like this Remember how similar the coariance and variance formulas were it turns out 
that the coariance of a variable with itself it's actually just the variance So you could rewrite 
your coariance matrix like this This truly is just a matrix of coariances between variables 
Normally you'll see it written with variance along the diagonal But the important point here is 
that we're performing the exact same calculations at every cell in this matrix finding the coariance 
of the variables in that row and column It is very common to see the coariance matrix expressed in 
matrix notation which gives a straightforward   and efficient way of computing the coariance 
matrix from your data or observations For this you first need to store all your data points in 
a matrix Each row will be an observation of the variables x and y and each column contains all 
the observations for a single variable So let me call this matrix A You will also need to define 
a matrix that has the same shape as a and each column takes on the mean value for the variable 
This matrix is called mu Using these two matrices you can write the coariance matrix as 1 / n -1 * 
a - mupose * a - mu Now let's begin by replacing a and mu by their expression Here I'm just replacing 
the symbols with the matrices they represent Next you need to complete the calculation a minus mu 
in both terms Subtracting element wise gives this new expression Next you need to transpose the 
first matrix which gives this new expression Note that the first matrix is size two which 
is the number of variables by n which is the   number of observations while the second matrix 
is size n by two This means that the product between the two matrices will be size 2x two 
which is precisely the size of the coariance matrix This is a good sign that we're on track 
Now begin doing the matrix multiplication For the first element of the resulting matrix you need 
to multiply the first row of the first matrix by   the first column of the second matrix which is 
simply x1 - mux * x1 - mux + x2 - mux * x2 - mux and so on for all the n terms which actually 
reduces to the sum of n terms of the square of the difference between each observation of 
variable x and the mean of that variable If we   incorporate the n minus one then this 
is the expression for the variance of x Now let's complete the second element of the 
first row Here you will multiply the first row of the matrix by the second column of the second 
matrix which gives you x1 - mux * y1 - muy plus x2 - mux * y2 - muy etc etc This expression can 
be simplified to this sum right here And again if you include the 1 / n minus one term you 
get exactly the expression for the coariance between x and y Notice that everything is really 
symmetrical in this matrix multiplication So if we were to look into the product of the second row 
of the first matrix and the first column of the   second matrix then you would again get coariance 
of yx And lastly the product between the second row of the first matrix and the second column 
of the second matrix gives you the variance of   Y This means you recover the original expression 
of the coariance matrix from much smaller simpler matrix operations All right that admittedly got 
somewhat complicated but let's see what those two equations actually look like with a real data set 
The data set has eight observations plotted on an xycoordinate grid Given the distribution of the 
data you would expect the X and Y variances to be about the same and a negative coariance because 
of the downward trend First rewrite your data in a table of two columns one for each feature You'll 
need to calculate mux and muy the average of each column In this case the average values of these 
columns are 8 and six Now create the matrix A minus mu in which all the values have the column 
average subtracted Now transpose that matrix and set up the matrix multiplication This table has 
eight observations So add 1 / n -1 or 17th to the front of this product Now all you need to do is 
multiply The result will be this 2x2 matrix which is C the coariance matrix And as predicted the 
X variance and the Y variance which lie on the diagonal are fairly similar and the coariance 
is negative The examples you saw were all for data sets of two variables But these same process 
works for any size data set Here the same steps are shown with a new column Z added Here's what 
you do Arrange the data with a different feature in each column Calculate the column averages 
mu Subtract the average from their columns generating the matrix A minus mu Multiply a minus 
mu transpose by a minus mu and divide by n minus one given the covariance matrix c Again this has 
been a technical look at the process underlying this very important matrix and you'll have more 
opportunities to study these statistical concepts   in more detail in the third course of this 
specialization I think it's beautiful though that a complicated process that generates the coariant 
matrix can be so succinctly expressed using matrix multiplication Now you're also ready to see the 
final incredible step that powers PCA So let's trace the steps of PCA and finally put all the 
pieces together Remember at the start you had this data set which you centered and the goal was to 
project your data onto the line that preserved the most information You learned that this best line 
is the one which preserves the most variance in your data The big question then is how do you find 
this best line this is where everything you've been learning in this lesson comes together So far 
you've learned about projections and how a simple matrix multiplication allows you to project data 
into a lower dimensional space You've also learned about values and vectors and how they capture the 
directions in which a linear transformation only stretches space but doesn't rotate it or shear 
it And in the previous video you got familiar   with the coariance matrix and how it compactly 
represents relationships between the variables in your data set PCA works by combining these 
three ideas in a clever way that helps reduce the number of dimensions in your data So let's see 
how it works I'll start again with the same data set which have centered around its mean point 
Let's begin by finding the coariance matrix C for your data points Suppose that the X variance 
is 9 and the Y variance is three The coariance is four which makes sense as there's a positive 
trend line to this data Now here's the big leap of this entire process You find the igen values 
and vectors of the coariance matrix This is how you'll find the line onto which you should project 
the data Remember that vectors and values come in pairs with the igen vector given a direction 
and the igen values giving a magnitude The first vector is 21 and it has an value of 11 The second 
vector is minus12 and it has an igen value of 1 You may have noticed these two vectors are at 
a 90° angle to one another which is also called orthogonal This is not a coincidence but it is 
true for the vectors of every matrix that is symmetric around its diagonal If you transpose 
this matrix these two fours would just change spots So C is symmetric In fact every coariance 
matrix is symmetric So the vectors you calculate will always be orthogonal Now you have two vectors 
or as they're called in PCA principal components and you want to project your data along one of 
them You can probably tell from looking that there is much more variance along the red vector than 
along the green one But how could you determine this mathematically it turns out that the vector 
with the largest value will always be the one that will give the greatest variance when you project 
your data So in this case the vector 21 has an value of 11 which is much larger than one So 21 
is the winner That is the line you'll project your data along You can meanwhile discard the second 
vector with its smaller value You can now draw the line that the vector 21 spans And all that 
is left to do is project your data onto this line Now that the points are projected along this 
vector you no longer need to graph them in   two dimensions All that matters is their position 
along the vector So I'll graph the projected data like this And just like that you have reduced 
the dimensionality of your data and preserved as much variance as possible which were the two 
goals of PCA Visually you can see your data went from two-dimensional to one-dimensional Rather 
than storing an X and a Y variable you can think of your data having been reduced to a single new 
value Z which tells you where on this vector each observation was projected to Your data has fewer 
dimensions and the maximum possible variance This process works for much larger data sets too 
Suppose you have a ndimensional data set So   a table with nine columns or features and as 
many rows or observations as you like So I'll just say there are n observations Here's how 
you'd perform PCA on this data From your data you can get the coariance matrix just like you 
saw in the earlier lecture Since you have nine   variables this will result in a 9x9 coariance 
matrix Next you would need to find the igen values and vectors of this matrix and sort 
them according to the value from biggest to smallest Imagine you want to reduce your data 
set to just two variables Then you'll simply keep the two biggest values and their associated 
vectors and discard the rest Now you have the two vectors you wish to project your data onto V1 and 
V2 To project your data create a new matrix where each column is one of the two vectors scaled by 
its own norm Finally multiply the matrices to project your data onto these two vectors giving 
you your final data set which has only two features Okay so now you might believe that this 
procedure works but I haven't explained why A rigorous proof would be a little challenging 
to show here However I can give you some of   the intuition behind this process Let's go back 
to the case of a data set with just two features which I'll still call X and Y which you can think 
of as both a table of data and points graphed in space So once you have your data you can get the 
coariance matrix C It contains information about how spread out the data is from the perspective 
of each pair of variables or maybe better yet how spread out is the data This is easier to see if 
you think of C as a change of bases How would it transform space well the fundamental vector 1 0 
would be moved to 94 and the fundamental vector 01 would be moved to 4 3 You can also check that 
the vector - 1 0 moves to - 9 - 4 and the vector 0 -1 moves to -4 - 3 Can you already see what the 
transformation looks like if you can't that's okay Let's see what happens with multiple directions 
all around the circle of radius one Let's start by adding this point and seeing its transformation 
and keep going all around the circle If you join all the dots in the transformed plane you will 
see that they are mapping out an ellipse So that the circle of radius one is transformed into the 
ellipse where you can see that all the points are stretched into different directions Note that we 
consider the circle a radius one because we are only interested in the direction of the stretches 
If you had chosen points of another bigger circle you would still get the same ellipse only bigger 
Looking at the transformed points which would you say is the direction of the biggest stretch i 
would say that is this red line right here which aligns with the major axis of the ellipse If you 
cut the ellipse in any other direction you will get a shorter line Here's where values and vectors 
come in Remember that this coariance matrix has two igen vectors 21 with igen value 11 and minus12 
with igen value 1 Together these two vectors form an basis and from the perspective of these 
basis the transformation C just stretches the plane Now it looks like the direction of greatest 
stretches along the greatest vector which is true Let me help you understand why that makes sense 
based on what you've learned about bases The whole point of getting the IG values and vectors was to 
reframe a linear transformation as two stretches Any point along the vector 21 will be stretched 
by a factor of 11 its value Any point along the vector minus12 will get stretched by a factor of 
1 which is its value And any other vector in the plane would be stretched by a factor somewhere 
between 1 and 11 Let me show you a couple of   examples to demonstrate that this is the case 
I'll calculate the norm of the vector before and after the transformation using the norm 
equation and find the ratio between them I'll start with vector 01 in teal which you already saw 
is sent to the vector 4 3 In this case the vector starts with a norm of 1 and ends up with a norm 
of five So it gets stretched out by a factor of five Now consider the vector 1 0 in orange which 
you already saw was sent to the vector 94 This vector also starts with a norm of 1 And if you use 
the norm equation you can calculate a final norm of about 9.85 So again that's how much this vector 
was stretched by This time the stretch was closer to 11 but is still less than the maximum stretch 
which will occur along the vector 21 Again this isn't a rigorous proof but hopefully this helps 
build the intuition behind choosing the Igen   vectors with the biggest values Your coariance 
matrix C characterizes the spread of your data The matrix C's vectors tells you the direction in 
which the matrix can be viewed as just stretching The largest value tells you where that stretching 
is greatest and any other direction will be stretched out less So choosing the IG vectors with 
the biggest values will give you the directions with the biggest stretch or the most variance And 
that's what you've been looking for this whole time Now that you have understood the idea behind 
PCA let's formalize the steps one last time using all the different formulas you've studied In this 
example there will be five variables but it works the same for any number You start with a data set 
of points with n observations of five variables X1 X2 X3 X4 and X5 Your goal will be to reduce your 
data from five to two dimensions First construct a matrix of your data This matrix will have five 
columns one for each variable or feature and n rows one for each observation and it will 
be called X This is the same matrix you used earlier to get the coariance matrix 
which at that point was called A We are using X to call this matrix because now all 
the variables are called X I Also this is more standard notation Next center your data 
To do this calculate the column averages and subtract them from each column giving you 
the matrix X minus mu Next calculate the coariance matrix This is just a simple matrix 
multiplication using the matrix X minus mu you just calculated In this case you end up with a 
5x5 matrix of coariances between each pair of variables Next find the values and vectors 
for the coariance matrix Once you have them sort them from smallest to largest by their 
values Now you'll create a matrix to project your data Since your goal is to reduce your 
data set to just two variables you will only   keep the first and second value vector pairs 
Create a matrix V which has two columns where each is one of the vectors you choose scaled 
by its norm Finally project the data onto the vectors you choose by multiplying the 
center data by your projection matrix   The result is a new data set XPCA which will 
have just two columns of data representing the projection of your original data set onto the 
two principal components you choose And that's it This is the step-by-step operations you need to 
perform PCA for dimensionality reduction of your data Congratulations on making it to the last 
assignment of this course In this video I want to talk about one final concept that will be helpful 
on that assignment discrete dynamical systems On the assignment you will learn to use this 
approach to model how people navigate the internet   As you'll see it's a fairly straightforward 
application of vectors Suppose today is sunny You want to know what are the chances that 
tomorrow will be sunny cloudy or rainy Suppose that if today is sunny you have a 0.8 probability 
of tomorrow being sunny 0.15 of it being cloudy and 0.05 of it being rainy What about if today 
is cloudy then the probabilities might change to 0.45 of tomorrow being sunny 0.35 of it remaining 
cloudy and 0.2 of it raining Finally if today's rainy then the chances of tomorrow being sunny is 
0.3 cloudy is 0.4 4 and rain is 0.3 You might have noticed that these values are positive although 
they could also be zero and the columns add to one If a square matrix has these properties we say 
it's a mark of matrix As you'll see in a moment mark of matrices are important because they allow 
you to infer the probability of how your system   will evolve Imagine that today is cloudy You can 
represent that with the vector 0 1 0 because with 100% certainty you know that it is cloudy right 
now This is called the state vector because it represents the state of the system Since this is 
the first you're considering I'll call it x0 To   know the probabilities of tomorrow being sunny 
cloudy or rainy you simply take the dotproduct between the matrix and the vector to get the new 
state vector 0.45 0.35 and 0.2 Notice that this is just the second column of the matrix the one 
corresponding to a cloudy day and this is the prediction of the weather one day ahead Now say 
you want the prediction for the following day So you take the dotproduct between the matrix and 
the new state vector x1 to get x2 which is the probability that the day after tomorrow will 
be sunny cloudy or rainy Knowing that today is cloudy x2 has values 0.5575 0.27 and 0.01525 
01525 You can keep going with this procedure to find the prediction for 3 days ahead This one 
has a probability of 6293 of being sunny,2421 of being cloudy and.1286 of being rainy Now repeat 
this process again to find the predictions for 4 days ahead for 5 days ahead Note that the change 
between states is getting smaller and smaller for six days ahead You now have the first two decimals 
constant between x5 and x6 And we can keep going and going and going And notice what is happening 
The difference between consecutive states vectors is now on the fourth decimal So let's do one 
more iteration What happens between x10 and x11 well it turns out that they're the same up 
to these four decimals So as you can see this   process tends to stabilize Essentially you are 
saying that the matrix dotproduct of the vector x10 which is 0.6665 0.2223 and 0.1112 is 1 times 
essentially the same vector This means that x10 is essentially an igen vector Now there's a small 
difference between x10 and x11 but imagine going pretty far along As a matter of fact imagine 
taking the limit as n goes to infinity of xn Then we definitely have an igen vector and one is 
the igen value associated with it You might have noticed that once you reach this vector state then 
you can never move anywhere else since it's an   igen vector with value one This matrix is usually 
called a transition matrix Remember that for this to be valid the transition matrix needs to be a 
mark of matrix with each column made up of non-   negative values that add up to one This matrix 
is usually called a transition matrix Remember that for this to be valid the transition matrix 
needs to be a mark of matrix with each column made up of non-gative values that add up to one 
This vector is called the equilibrium vector and   it gives you the long run probabilities that on 
a given day it will be sunny cloudy or rainy The cool thing about it is that no matter what your 
initial state was in the limit at infinity you will get to this equilibrium state As you've seen 
it is also an IM vector of the transition matrix Now you should have a better understanding of 
discrete dynamical systems and the role that I   values and vectors play in understanding their 
behavior You are now ready to give it a go with the programming assignment This brings us to the 
end of the lecture for week four After this you have a few final activities There is a graded 
quiz that covers all the topics in week four You'll also find this week's programming 
assignment which is a fascinating look   into how IM values and IM vectors are used in 
practice You'll see them used to model traffic between web pages and implement PCA to reduce 
the dimensionality of a data set of cat images I think these are some of the most fascinating 
topics in this course and a great final challenge   that will require you to apply everything you've 
learned To help you prepare for the assignment you will also find a short ungraded lab Best 
of luck and I'll see you once you're done 
in this lesson we want to start talking about the properties of matrices and specifically here we want to do an introduction to matrices all right so over the course of the last few lessons we learned how to solve linear systems with matrix methods specifically we looked at the gaussian elimination and the gauss-jordan elimination methods so at this point you had some exposure to matrices but over the course of the next few lessons what we want to do is just expand our limited knowledge and build a foundation for some other methods that can be used to solve linear systems so when we talk about a matrix in math normally we're speaking about an ordered array of numbers or you might say a rectangular array of numbers now when we talk about what's inside of a matrix these numbers here so for example in this guy right here you see if i read across i have 1 negative 7 5 and 13 and then here i have 3 negative 9 2 and negative 4 and then here i have 6 negative 2 1 and 5. so all of these numbers inside of these brackets represent the elements or the entries of the matrix okay so you might hear those two words interchangeably now when we talk about a matrix it's important to know when we're referring to a row or referring to a column so you see i have an image of a guy rolling a boat and that's meant to represent a row okay so a row goes across so you see you have a row here a row here and a row here so this matrix has three rows and you'll remember this from again our lesson on gaussian elimination and gauss-jordan elimination where we kind of labeled these rows we said this was row one this was row two and this was row three and i made that a b i don't know why so this was row three okay so three rows and then you see the image or the picture for a column okay columns are vertical so this is going up and down so this is vertical this is vertical this is vertical this is vertical so this is a column one this is a column two this is a column three and this is my column four okay so this guy has three rows and it has four columns so when we look at that information we can say it's a three by four matrix okay this is known as the order or the dimensions of the matrix so i'm just going to write here that this is a 3 by 4 matrix again that's called the order or the dimensions of the matrix now in this example what would the order be well again if i go through i can say i have a row here i can say i have a row here a row here a row here and finally let me make that better a row here so you have five rows right one two three four five again the rows are going across just picture the guy rowing the boat okay that's how you remember the columns are going up and down okay so we're going to say that we have one column here we have a column here a column here and a column here so you have five rows and four columns the rows always come first okay the rows always come first so it's a five by four okay when we're talking about the order or again the dimensions of the matrix what about this guy so here we have what we have a row here so this is row one we have another row here again the rows go across then the columns are up and down so this is a column here this is a column here this is a column here so this guy if i talk about the order it's a two by three right two rows three columns so it's a two by three all right so in a lot of cases we're going to use capital letters to name matrices and this has a lot of different uses mostly if you're going to kind of reuse a matrix over and over again you might be performing several operations with it so let's say you write capital letter a is equal to and again you have these elements you have 4 and 2 in the first row and negative 3 and negative 1 in the second row well i might have to do several things with a so it's just useful to name it that way i can keep talking about it without relisting the elements each time okay now another thing you might want to know is that if a matrix has the same number of rows as it has columns like we have here it's referred to as a square matrix right so this guy has two rows and it has two columns okay so this is a two by two otherwise known as a square matrix okay so if it's a 3 by 3 a 4 by 4 a 117 by 117 again same number of rows as columns it's a square matrix so again we have another example of a square matrix so this is matrix b and again we have three rows so one two and three again they're going across and you have three columns okay again the columns are going up and down this is a three by three matrix or again it's a square matrix all right so you also have a row matrix and a column matrix so a row matrix is a matrix with only one row and then a column matrix is a matrix with only one column so this d here is an example of a row matrix you only have one row here okay you've got several columns so column one column two column three so the order here is a one by three and again this is a row matrix because it only has a single row now e is going to be an example of a column matrix because it only has one column okay but it also has four rows so it has a row one let me make that a capital letter so a row one a row two a row 3 and a row 4. so this guy is a 4 by 1 if we're thinking about the order and again it's a column matrix because there's just one single column all right let's talk a little bit more about notation this is something you definitely need to know as we go deeper into this topic so generically speaking you'll probably see this in your book you have this capital letter a and it's equal to inside of the brackets you have all these lowercase a's and you have this subscript associated with each kind of entry or lower case a so each guy here let's say i start with this one right here so this is a sub 1 1 okay so the lower case a is just a matter of having a capital a here okay if i had a capital letter b this would be a lowercase b if i had a capital letter c lowercase c so on and so forth the one one there is meant to say where i am in the matrix okay it's like a location so the first number okay the one here the first one tells me what row i'm in and the second one tells me which column i'm in well in this one at the top i'm in the first row in the first column if i move one to the right now i have a sub 1 2. so this guy right here is in the first row second column move to the right i have a sub 1 3. again i'm staying in the same row so i'm still having a row position of 1 but my column is just increasing as i move to the right so here it was 1 here it was two now it's three okay but the row always stayed the same and i'm going to continue out till i get to a sub 1 n so i'm still in the first row but i'm in the nth column okay so then this notation as we go down you see that in this case all the way to the left the column stays the same it's always a 1 right because i'm in the first column but the row is now increasing as i move down so we end up down here with the a sub m1 and if we go all the way here to the bottom right i have a sub m n so this tells me that what for this matrix it's an m by n right it's got m rows and it's got n columns so generally speaking that's what you're going to see but you need to know how to find specific entries they might ask you hey what's the value of something like a sub 3 3 okay so what that means is to go and find this entry in the third row in the third column so i would say okay this is the third row third column so that's this guy right here okay whatever that happens to be in this case it's generic but in a normal matrix you'd have some kind of number or symbol or you'd have something there that you could say this is equal to this so generically in your textbook what they're going to write is they're going to say a sub i j and again all this means is that it's the element that's going to be in the i throw or you could say row i and the column j right or the jth column if you want to say it that way it's always row first and column second now one thing i want to call your attention to because this does cause some confusion if you see it with commas it's really the same thing so i could say that a sub 3 2 this is really the same as if i said a sub 3 and then put a comma there for the two so some people get confused by that it's just a difference in notation if you have numbers involved that have two digits you need to use a comma so that you understand what it is okay let's say i had a sub 3 and then 12. well you don't know what this is is this a sub 31 2 is this a sub 3 12 what is it so that's why you put a comma in between them to say hey this location is on the third row 12th column okay so that's when you definitely want to use a comma all right so to give a little example suppose we have uppercase b and it's equal to in our first row we have 1 negative 5 9 and our second row we have negative 7 4 and 12. so let's say i asked you to find lowercase b sub 2 3 okay sub 2 3. so again it's row first so this is the row and then this is the column okay so what is in the second row so second row is down here and what's in the third column that's here so it's going to be this guy right here so i can say this is equal to 12 okay and that's all you want to do if you get this as a question let's say i wanted to find b sub 1 2 again the first number which is this one is the row this guy is the column the second one so where's row one that's here it's on the top column two is here okay so that's going to be negative five very very easy suppose i gave you something like let me just kind of erase this one suppose i gave you b sub 3 2 b sub 3 2. what's the answer there well i know that i only have one two rows so i can't find a third row so they might give you this as a trick question right there is a second column but there's not a third row so this element doesn't exist okay so you could just write does not exist all right so another thing that you might see in this section it's pretty short and there's not a lot of questions but they're going to talk to you about how to determine if two matrices are equal so the rule is that two matrices are equal if and only if they have the same size okay so the same order in other words one is a three by three the other one's a three by three one is a three by four the other one's a three by four they've got to have the exact same order or you could say have the same size and then each corresponding element has to be the same okay has to be the same so let's say for example we have a and it's equal to we have 2 and x in the first row z and 5 in the second row and we have b and it's equal to we have y and 3 in the first row and 4 and w in the second row so we might ask what are the values for the variables that make the matrix equation true if we said that a is equal to b okay and all you'd have to do is kind of say okay well i've got a 2 here and a y here so y has to be equal to 2 okay and then you'd say okay i have a z here and a 4 here so z has to be equal to 4. okay you get the ideas it's very simple then i have an x here and a 3 here so x has to be equal to 3 and then lastly i have a w here and a 5 here so w has to be equal to 5. because these guys are only going to be equal if we kind of set this up and say okay we have 2 x again was going to be three and then z was four and we had five as w so we have this and we'll say it's equal to this so two three four five okay kind of trivial and it seems like you know why would you go through this but it is something that is important to understand so they will give you kind of questions like this so every corresponding element has to be the same and the order has to be the same so you have a 2 in row 1 column one you have a two in row one column one you have a three and row one column two a three in row one column two right so on and so forth you have a four here and a four here in row two column one and then you have a five here five here in row two column two so every entry has to be exactly the same and the order has to be the same all right one more of these and it's a little bit more complex so again find the values for the variables that would make the equation we'd say a is equal to b make that true so here all we're going to do is end up setting up an equation in each case so we would say 3x is equal to 9 and so in this case we know that x is 3. let me kind of do this off to the side so we know x would be 3 because 3x equals 9 we divide both sides by 3 x is equal to 3. that's very easy let's kind of grab a few more and let me kind of just erase that so we know we got this one done so let me just highlight the ones that we know and this one right here we'll have w minus 1 and that's going to be equal to 6. so let's solve that real quick so we have w minus one equals six add one to each side of the equation we get w is equal to seven so w equals seven and again very easy just have to go through these it's more time consuming than anything so you have z plus two and that's going to be equal to 12. so z plus 2 equals 12. subtract 2 away from each side of the equation you're going to find that z is equal to 10. so we've got those done so now we just need to do these so i've got 19 here and i've got a 5y minus 1 there so 5y minus 1 equals 19. and again just add 1 to both sides you get 5y is equal to 20. divide both sides by 5 you get y equals 4. so we have y equals 4. and let's go back up all right so two more so i've got this 15 here and we're going to set that equal to 4k plus 3. so 4k plus 3 equals 15. 4k plus 3 equals 15. let's subtract 3 away from each side of the equation 4k is equal to 12 divide both sides by 4 we get that k is equal to 3. okay so just one more now again i know this is tedious but it's just something that you might get asked so it is important to cover it so then the last one here we have that q minus 9 is going to be equal to 11. so q minus 9 q minus 9 equals 11. again add 9 to both sides we get that q is equal to 20. okay so these are the values for the variables that are going to make that equation a equals b true you're just looking at the corresponding entries we know that in each case i have three rows and two columns okay so that's the first thing the matrices can only be equal if and only if they're the same order okay in this case they're each a three by two and the corresponding elements are the same okay so that means that nine and three x have to be the same or equal so we say 3x equals 9 we find that x equals 3 right you do that for each one and then you find out the values for the variables again we found that x is 3 z is 10 w is 7 y is 4 k is 3 and q is 20. so that's what you do if you get this as a question on your homework or on a test in this lesson we want to continue to talk about the properties of matrices and specifically here we're going to talk about adding and subtracting matrices all right so we're going to begin our lesson with the addition of matrices first and foremost we can only add two matrices together if they're the same size or again as we talked about in the last lesson the same order so these are the kind of dimensions of the matrix it's how many rows you have by how many columns you have so for example if we had a three by four matrix that matrix has three rows and four columns and if you had two three by four matrices you could add those two together okay that would be perfectly fine but if you had a three by four matrix and you had a two by three matrix you couldn't combine those two with addition that operation would be said to be undefined so the first thing you would do is verify that again they're the same size or order once that's done then you can simply add the two matrices together by adding the corresponding elements it's actually a very easy process overall so suppose i have matrix a and matrix a is equal to in the first row we have the elements 3 and negative 1. in the second row we have the elements 5 and 2. then suppose i also have this matrix b and in matrix b we say this is equal to in the first row negative 4 and 3 in the second row 1 and 7. so in each case i have 2 rows and 2 columns so they're each going to be a 2 by 2 matrix so i can find the sum of a and b so to do that if i want to find the sum of a plus b it's just equal to what well i'm just going to sum the corresponding elements so all that means is i just take the element in the first row in the first column okay and the element in the first row in the first column in each case and i find the sum so what is 3 plus negative 4 okay we'll do that in a minute and then we just go through and do the same thing for kind of each case so here i would have negative 1 plus 3. so negative one plus three then down here i would have five plus over here i have a one then i would have a two plus a seven okay so that's all you're doing you're just adding the corresponding elements from the two matrices all right so let's crank this out real quick so we know that 3 plus negative 4 would give us negative 1. let me kind of slide down a little bit so i'm just going to say this is going to be negative 1 and then down here 5 plus one is going to give me six and then negative one plus three is going to give me two and then two plus seven is going to be nine okay so this is the matrix that is a result of adding a plus b in your top row you'd have negative one and 2 in the bottom row you'd have 6 and not all right let's take a look at another example so suppose now you have this as your matrix a and this as your matrix b so if i want to find the sum of a plus b okay first and foremost i've got to verify that the order is going to be the same in each case so we see that for matrix a i have three rows and three columns so it's a three by three and for matrix b it's three rows and three columns so again it's a three by three so because they're the same size or the same order or they have the same dimensions however you want to say that we can add them together right we can find the sum so again to do this it's very simple i just add the corresponding elements so if i start up here in row one column one i add those two guys together and i'm just going to do this right here i don't need to write it out so negative two plus negative one is going to be what that's going to be negative three okay then let me erase this and i'll highlight each one that i'm doing so let's do this one now so what is one plus six that's going to be seven and then what is seven plus two that's going to be nine okay so that's my first row that's done so now let's move on here and i'll just do these a row at a time now to make it a little bit faster so what is 5 plus 8 that's going to give me 13. what is 6 plus 3 that's going to give me 9 what is 9 plus 4 that's going to give me 13 and then let me erase this we'll go to this bottom row so this and this we'll say what is 1 plus 11 that's 12 what is 4 plus negative 6 that's going to be negative 2 and then lastly what is 8 plus 10 that's 18 okay so this matrix here is the sum of the matrix a plus the matrix b all right now you can also add more than two matrices together again they've all got to be the same order or the same size and in this case we have a matrix a a matrix b and a matrix c we're going to find the sum so in each case we have one two three four rows and one column so they're going to be a four by one matrix in each case and remember in the last lesson we said that these were a column matrix right because there's only one single column so to add these together we do a plus b plus c i want to just note here that the commutative property that we learned with addition of real numbers would hold here i can add these in any order also the associative property right i can group the addition however i want it won't change the sum either way because all i'm really doing is i'm adding things together and with addition we know that it's commutative we know that it's associative so nothing's really going to change the sum so let's go ahead and set this up so i'll just do this like this and again i'm just going to work here i'll do 3 plus negative 2 plus negative eight so three plus negative two is one one plus negative eight is negative seven let me move down to kind of the next row i'll do five plus nine plus fourteen five plus nine is fourteen fourteen plus fourteen is twenty eight and then let me move down to this row here and i'll erase this one up here so now i'm going to do negative 7 plus 3 which is going to give me negative 4 and then negative 4 plus 6 is going to give me 2 and then lastly i'm going to have this bottom row so i'm going to have 10 plus 1 plus 9. we know that 9 plus 1 is 10 and 10 plus 10 is 20. okay so pretty easy overall so a plus b plus c is going to give us this matrix that's still a column matrix right it's got the same dimensions as what we're adding and we've got negative 7 28 2 and 20 as that column there okay so four rows one column it's still a four by one all right so let's move on and talk about subtracting matrices now so again when you subtract matrices it's the same thing you have to have two matrices with the same order or again same size or dimensions whatever you want to say in this case we're going to start out with a which has the elements 5 1 in the first row and 9 negative 5 in the second row and then b which has the elements 7 2 in the first row and negative 9 12 in the second row or the bottom row so each of these is a 2 by 2 matrix right two rows and two columns now if we subtract it's not commutative right we have to know that a minus b is not going to be equal to b minus a you can kind of eyeball this and see that if i did let's say this guy right here so this 5 that's in row 1 column 1 and this 7 right here that's in row 1 column 1. if i did matrix a minus matrix b well 5 minus 7 would give me negative 2. if i did matrix b minus matrix a well 7 minus 5 would give me positive 2. those two elements wouldn't end up being the same so those two matrices would not be equal right so a minus b would not be equal to b minus a so for the purposes of what we're going to do here i'm just going to do a minus b again it's a very simple process i'm just going to match up corresponding elements and i'm going to do my subtraction in the correct order so if i have a minus b i'm going to take the element in matrix a and subtract away the corresponding element in matrix b so in this case i'm going to do 5 minus 7. again this is an a this is in b i'm going to do 1 minus 2 okay i'm going to do a 9 minus a negative 9. be careful there because you have a negative there okay so 9 minus negative 9 and then i'm going to do negative 5 minus 12 okay so 5 minus 7 we know that's negative 2 okay so this is going to be negative 2 and then 1 minus 2 is negative 1. 9 minus a negative 9 is 9 plus 9 that's 18. and then if we had negative 5 minus 12 that's negative 17. okay so this would be the result of a minus b now there's another way to do this and this is probably what you're going to see in your textbook i want you to remember that when we subtract integers right we learned that we could do a minus b and i'm just using a and b as any real numbers and this might get confusing so let me use p and q so p minus q p is a real number q is a real number we can say that p plus negative q is the same thing right so if i had 5 minus 3 i could write this as 5 plus negative 3. okay so we're going to use that same concept here and we can say that if i have matrix a minus matrix b i can say this is a plus the negative of b okay and we haven't talked about multiplication yet with matrices we'll get to that in the next lesson but essentially if i have the negative of b i can just go into my matrix b and i can change the sign of every element okay it's just like multiplying every element by negative 1. so let's go ahead and set this up real quick let's say that we have the negative of b and i'll say this is equal to 1. instead of 7 i'll have negative 7. instead of two i'll have negative two instead of negative nine i'll have positive nine and instead of twelve i'll have negative twelve now what i can do is i can say let me make a little border here a minus b okay so a minus b is equal to a plus the negative of b so this will be equal to let me set up this matrix i'm going to say 5 plus this guy right here because i'm working with negative b so 5 plus negative 7 okay and then i'd have 9 plus 9 so 9 plus 9 i would have 1 plus negative 2 so 1 plus negative 2 and then i would have negative 5 negative 5 plus a negative 12. now because i switched this operation from subtraction to addition once i'm adding the opposite once i have this set up i can add in any order right if i do negative 7 plus 5 or if i do 5 plus negative 7 i get the same answer but you can only do that once you've changed it into its opposite and you've set up the addition okay so don't make that mistake you can't do that with subtraction all right so let's go ahead and crank this out so 5 plus negative 7 again we know this is going to be negative 2. 9 plus 9 we know that's 18 and then 1 plus negative 2 we know that's going to be negative 1 and then negative 5 plus negative 12 again we know that's negative 17. so it's the same answer just a different way to kind of look at it this might be easier for you it might be harder for you it doesn't matter you get the same answer either way so whatever you want to use all right let's take a look at one more example with subtraction so suppose we have a and we have b and we want to subtract let's do b minus a this time so b minus a okay so i'm not going to go through and say b plus negative a you can do that if you want again it's the same answer i'm just going to save time and do b minus a okay so what i'm going to do because these guys again are the same size this is a one two by one two three so it's a two row and three column matrix in each case so it's a two by three so we can subtract them i'm gonna do each element of b minus each corresponding element from a so 5 minus a negative 1 5 minus a negative 1 and let me change colors we would have 1 minus 8 1 minus 8. we would have negative 2 minus 2 okay and then let me change colors again we'd have negative 4 minus 11 and then we'd have 9 minus 14 okay 9 minus 14 and then we would have 15 minus 6. all right so 5 minus a negative 1 is like 5 plus 1 so that's going to be 6. then 1 minus 8 is going to be negative 7 then negative 2 minus 2 is negative 4 and then negative 4 minus 11 is negative 15. so negative 15 and then 9 minus 14 is negative 5. okay so that's negative 5. then lastly 15 minus 6 is going to be 9. so this is the result of b minus a again if you wanted to you could do b plus the negative of a so to do that you would change each element inside of this matrix a into its opposite and then you could add and you would get the same result all right so now let's talk about the kind of zero matrix this is a matrix that is also referred to as the null matrix it's a matrix that contains only zeros as its elements okay so the zero matrix can be written as any size so whatever size you kind of need to work with you can make a zero matrix to kind of work with it so if you need a two by three a 30 by 30 715 by 220 you can do whatever you want is all going to be zeros now generally speaking we're going to notate this with a 0 at least that's how i'm going to notate it a lot of books do different things here you might see a 0 with kind of the size so in this case i have one two three rows and i have two columns so you might see them write a zero and then say it's a three by two okay if it's known what the size is you typically just put a zero instead of writing out all these zeros but there's other notations that you might come across they might use a letter to represent this so just stick to what your book gives you now i want you to recall that when we worked with kind of real numbers we said that 0 was the additive identity and what that means is that if i add 0 to something it's unchanged so if i use p or q again let's just use q and i add 0 to that i'm going to get q back okay q is just some real number so for example if i had 111 and i added 0 i get 111 right if i had a trillion and i add zero i get a trillion so zero is the additive identity well when we do matrix addition because a zero matrix only has zeros as its kind of entries what happens is every time you add the zero matrix to another matrix you just get the original matrix back so the zero matrix is the additive identity in matrix algebra so for example here if i add if i add a plus this 0 matrix i'm just going to put a 0 there okay i'm just going to get a back because all i would be doing is saying that i would set this up and say let me kind of do this i would do 5 plus 0 which is 5. i would do 2 plus 0 which is 2. i would do 1 plus 0 which is 1. i would do 7 plus 0 which is 7. let me make this a little bit longer i would do 8 plus 0 which is 8 and negative 4 plus 0 which is negative 4. so i get a back right this is equal to a i haven't changed anything because in each case i just added a real number to zero and so i got that real number back all right now additionally we saw that if you added a real number and its additive inverse together you got zero okay so it's going to be the same thing or the same concept when we work in matrix algebra if we have some matrix a okay and we add to it the opposite or the negative of a you're going to end up with a 0 matrix so in other words if i have this matrix a and i have these entries here 3 negative 5 8 11 and then 6 and 4 i could say the negative of a is what we talked about this earlier i'm just going to multiply each kind of entry here by negative one so this would be negative three this would be negative eight this would be negative six this would be positive five this would be negative eleven this would be negative four okay so what happens is if i add a plus negative a i'm going to end up with a 0 matrix and in this particular case i'm going to have a 0 matrix that's what it's 1 2 3 rows by 2 columns right so it's a 3 by 2. you could write that if you want or again it's going to be obvious that it's a three by two so you can just leave that zero as the notation again you can go through and kind of crank this out to prove it three plus negative three is zero negative five plus five is zero eight plus negative eight is zero eleven plus negative 11 is 0 6 plus negative 6 is 0 and 4 plus negative 4 is 0. so again you end up with this 0 matrix instead of writing all this out you just put a 0 okay to notate that you have a 0 matrix all right so let's wrap things up with an introduction to matrix equations this is something we're going to be dealing a lot with kind of in this chapter and also as we progress in math in general so with a matrix equation we have an equation where variables are used to represent matrices so at this level it's very very simple let's say i have a matrix a and a matrix b they're both a two by two matrix so we know we can add them together and we have matrix a plus this matrix x which we don't know what that matrix is and we say it's equal to this matrix b so if i want to solve for x i can use the same strategies that we talked about when we solve linear equations here okay so i can subtract matrix a away from each side of this equation and i can say that the matrix x is going to be equal to the matrix b minus the matrix a okay so you see where we're going with this we can find out what x is by just doing b minus a okay so what is let me kind of make a little border here what is b minus a that would be equal to what well i would just take each element from b and subtract away the corresponding element in a so 4 minus 2 would be 2 negative 1 minus 8 would be negative 9 okay 5 minus a negative 1 is like 5 plus 1 so that's 6 and then 9 minus 3 would be 6. so this is b minus a and it's also equal to x because we said x was equal to b minus a so let me just write here that this is equal to 2 and negative 9 for the top row and then 6 and then 6 in the bottom row and you can check this that's the good thing about kind of working with equations you can always check well if it's true that the matrix x which is this guy right here is equal to the matrix b minus the matrix a well then it's also true that a this matrix here plus x this matrix here would give me b you could check that really quickly you could say okay if i had 2 plus 2 would i get 4 yeah that's a check if i had 8 plus negative 9 would i get negative 1 that's a check if i had negative 1 plus 6 would i get 5 that's a check if i had 3 plus 6 when i get 9 that's a check so our matrix x is this guy right here right with 2 and then negative 9 in the top row and that kind of looks like 2 minus 9. so let me make that a little bit cleaner and then 6 and 6 in the bottom row all right let's try one more of these and we'll just kind of call it a day on this and the next lesson we'll look at some matrix multiplication all right so we have our matrix a our matrix b again they're the same size in each case we have a 3x3 three rows three columns so we have that a this matrix here plus x which is our unknown matrix is equal to b which is this matrix here so again i'm just going to solve for x i'm going to subtract this matrix a away from each side of the equation and again we find that x is equal to b minus a and i don't need to check this this time we'll just crank this out real quick and call it a day let me kind of slide this over a little bit and so what is b minus a so again we have 12 minus the negative 1 which is going to be 12 plus 1 which is 13 okay and then we have 11 minus 5 which is 6 we have 8 minus 9 which is negative 1. we have 10 minus 2 which is 8. we have negative 6 minus 7 which is negative 13. we have 5 minus a negative 1 which is 5 plus 1 that's 6. we have 13 minus 15 which is negative 2 we have negative 9 minus 13 which is negative 22 and lastly we have 4 minus a negative 3 which is 4 plus 3 which is 7. so x our matrix that was unknown again is equal to the matrix b minus the matrix a which gives us this matrix here again in the top row you have 13 6 and negative 1. in the middle row you have 8 negative 13 and 6 and in the bottom row you have negative 2 negative 22 and 7. in this lesson we want to talk about multiplying a matrix by a scalar all right so when we talk about multiplying matrices there's going to be two scenarios that you're going to come across the first scenario which is much simpler involves multiplying some real number called a scalar by a given matrix the second scenario is where we're going to multiply two matrices together now the second case is much more involved and we're going to look at that process in the next lesson all right so how do we multiply a matrix by a scalar well again just to be clear here when we say a scalar this just means some real number or to be more specific a real number that is not inside of a matrix okay so when we see this in matrix algebra we call it a scalar so if we take that scalar and we multiply it by a matrix this is where we get the term scalar multiplication now what you're going to see in your textbook is something like this if matrix a is equal to again this just identifies the matrix by the individual elements that it's kind of made up of so you say this lowercase a sub i j again the i here is the row the j here is the column and again this is just generic notation so if this is an m by n matrix and k is a scalar so some real number that's not in a matrix the scalar multiple of a by k is the m by n matrix given by and again this notation is pretty simple overall you have k times the matrix a so the scalar k times the matrix a gives us a matrix that's made up of these elements here which is k multiplied by each individual element of a right this lower case a sub i j and again when you see this kind of notation it may be a bit confusing but let's just jump into an example and you'll see that it's very very easy to do this process so suppose i have matrix a and it's made up of these elements here again in the first row we're gonna have three seven and eight the second row will have negative two eleven and one in the third row we'll have five two and twenty two so this is a three by three matrix three rows three columns it's a square matrix so if i asked you to find negative five times matrix a what would you do again all you want to do is multiply negative five this scalar here by each and every element of matrix a so a very simple process so let me just kind of write this over here negative 5 a is going to be equal to i'm just going to multiply negative 5 by every element over here so 3 times negative 5 is negative 15 and then if we do 7 times negative 5 that's negative 35 if we do negative 5 times 8 that's negative 40. and then if i do negative 5 times negative 2 that's positive 10. if i do negative 5 times 11 that's negative 55 and then negative 5 times 1 is negative 5. for the last row we have negative 5 times 5 which is negative 25 then we have negative 5 times 2 which is negative 10 and then negative 5 times 22 which is negative 110. so this matrix right here negative 5a is a scalar multiple of the original matrix a okay so that's all we're really saying and let's just look at another example it's pretty easy overall so for the second example we have matrix b and we have again in the first row 4 6 and 16 in the second row 2 10 12 and then the third and final row we have 10 4 and 14. so another three row and three column matrix so again it's a three by three a square matrix so if we want to find one half times b again all i would do is multiply every element in this matrix b by a half okay that's all we're doing very very simple process so one-half times four is two one-half times six is three one-half times sixteen is eight one-half times two is one one-half times ten is five one-half times twelve is six one-half times ten is five one-half times four is two and one-half times fourteen is seven okay so this would be one-half times b so now let's kind of look at a combination of some things that we've learned already so in this section when you talk about scalar multiplication you get a few problems on scalar multiplication and then they kind of combine things together so what we'll see is that we're going to do some problems with addition and subtraction with scalar multiplication involved and then we're also going to look at some equations so suppose we have matrix a which is made up of with the first row 3 1 and 5 in the second row negative 2 0 and 6. and then for matrix b we have the first row as 4 7 and 2 and the second row is zero one and eight and each of these have two rows and three columns okay so it's a two by three in each case so we know that since they're the same size they can be added together now our problem is going to be to do 3 times a and then add the result of 2 times b okay so 3a plus 2b and so what i'm going to do first is find 3a okay i want to find 3a so let me start by just rewriting a here real quick for reference and then we'll delete it so we have three one and five and we have negative two we have zero and we have six okay so if i want three times a again a very very simple process i would multiply every element in matrix a by three okay that's all i'm doing so three times three is nine three times one is three three times five is fifteen and then three times negative two is negative six three times zero is zero and three times six is 18. okay so let me erase this and let me just drag this out of the way and let me write down b here for reference so for b again we have 4 7 and 2 and then we have 0 1 and 8. and if i wanted 2 times b let me just write that here so 2 times b would be what i would multiply every element by 2. so this would be 8 this would be 14 this would be 4. zero would stay the same right because zero times anything is still zero this would be two and this would be sixteen okay so now let me kind of arrange these in a way that we can see what's going on so i know 3a equals this and 2b equals this so i want the sum of these guys again you can only add two matrices if they're the same size or order and in each case we have a two row by three column matrix so we're good to go on that so if i want 3a plus 2b i would say it's what well i'm just going to take every element from 3a and add the corresponding element from 2b so 9 plus 8 would give me 17. 3 plus 14 would give me 17 again 15 plus 4 would give me 19 and then negative 6 plus 0 is negative 6 0 plus 2 is going to give me 2 and then 18 plus 16 is going to give me 34 okay all right so this is our result 3a plus 2b so it's a very easy process to not only do scalar multiplication but to combine it with some addition or subtraction operations all right so let's look at another problem that involves kind of combining the scalar multiplication with addition or subtraction so we have 4a minus 5b we're using the same two matrices so here's our a and here's our b if you want to copy those down real quick you can i'm just going to write them out real fast so i'm going to say that a is equal to again we have 3 1 and 5 in the first row and we have negative 2 0 and 6 in the second row then for matrix b we're going to have 4 7 and 2 in the first row and we're going to have 0 1 and 8 in the second row all right so if we want to do this 4a minus 5b my suggestion to you when you have subtraction is to add the opposite so let's just do plus negative there so when i get to this scalar multiplication with matrix b i'm going to do negative 5 times b and then i can just add the matrices okay it makes it a little bit easier to not make a silly sign mistake because the worst thing with working with matrices is you go through all this work and then you get the wrong answer you basically have to rip the page up and start over okay so if i do 4 times a 4 times a this is equal to what it's just 4 times every entry in a so 4 times 3 is 12 4 times 1 is 4 4 times 5 is 20 okay and then down here 4 times negative 2 is negative 8 4 times 0 is 0 and then 4 times 6 is 24. so i can get rid of this i don't need this anymore and let me just kind of slide this down out of the way and then now i want negative 5 times b okay so negative 5 times b is going to give me 1. so it's negative 5 times every element so negative 5 times 4 is going to be negative 20. negative 5 times 7 is going to be negative 35 negative 5 times 2 is going to be negative 10 and then we have negative 5 times 0 which is 0 negative 5 times 1 which is negative 5 and lastly negative 5 times 8 which is negative 40. okay so let me erase this and i'll just drag this down here so it's out of our way and now what we want to do is this operation here so what is 4a plus negative 5b or 4a minus 5b as we originally saw it well again to add two matrices together or to subtract matrices you have to have two matrices that are the same size or the same order again in each case we have a two by three matrix so we're good to go right 2 rows 3 columns so we're just going to add corresponding entries so 12 plus negative 20 is going to be negative 8 then 4 plus negative 35 is going to be negative 31 then 20 plus negative 10 is going to be positive 10. then down here in my second row i'd have negative 8 plus 0 which is negative 8. i'd have 0 plus negative 5 which is negative 5 and i'd have 24 plus negative 40 which is going to give me negative 16. so this matrix here would be the result of doing 4a plus negative 5b or again 4a minus 5b all right so before we kind of move any further i want to go over some of the properties of scalar multiplication that you're going to see in your textbook just so that these things are clear and you don't get confused by them so let's define a and b to be these matrices of the order m by n and then k and h are going to be these scalars or again these real numbers that are not inside of a matrix so the first one is kind of the associative property we know about the associative property of multiplication with real numbers right that tells us we're multiplying three or more numbers together we can group that multiplication any way we'd like we always get the same result well the same thing is true here right so if i had k times h done first this is inside of parentheses these two scalars then the result of that is multiplied by the matrix a it's the same thing as if you did one of the scalars times the matrix a first and then took that result and multiplied it by the remaining scalar now the next two are extremely obvious if you have one as a scalar meaning you have 1 times your matrix a you get matrix a back and the reason for this is because 1 is the identity element in multiplication so if i'm going through and multiplying everything in matrix a by 1 i just get that element back so it's an exact copy of the matrix a the next one is if we have 0 as a scalar so 0 times matrix a will give us a 0 matrix and again the notation for this varies but generally speaking you'll see a zero with the kind of order we said these were m by n matrices so we'll say this is a zero matrix that is of the order m by n and let me make that m a little bit better okay and again because i'm multiplying 0 by every element of a i'm getting a matrix with all 0s as entries and it's of the order m by n so that's why we define this as a 0 matrix of the order m by n alright lastly we have a distributive property with scalar multiplication this one's pretty obvious as well so we have something like k a scalar multiplied by these two matrices added together so a plus b we could say it's the same as doing k times a plus k times b all right then the second part of this would be if we had k plus h these two scalars inside of parentheses those guys are being added together first and then the result is multiplied by this matrix a this is the same as doing k the scalar times the matrix a plus h the scalar times the matrix a all right so let's look at a little matrix equation we're going to see more and more of these as we progress through the chapter and we're going to start getting into some scenarios where we'll actually be able to kind of solve a linear system with a method other than the gaussian elimination and the gauss-jordan elimination we'll see that we can kind of use matrices in a different way with the cramer's rule and then also with kind of solving by using the inverse of a matrix but we'll get to that later on so for right now we have this matrix a which has negative 1 and 3 in the first row and 4 and 7 in the second row and matrix b which has 6 and 5 in the first row and 0 and 9 in the second row so they're each 2 by 2 square matrices so what we're saying for our equation is that 2 some scalar times some unknown matrix x plus our matrix a is equal to our matrix b so we would solve this the same way as if we had a linear equation kind of in one variable right so i want to isolate x my unknown matrix and to do that the first thing is to subtract matrix a away from each side of the equation so this would cancel and over here i'm just going to write this in line and say this is b minus a so now what i have is 2 times matrix x is equal to b minus a so how do i get rid of this 2 from over here well we can divide both sides by 2 or what i'm just going to say since we're working in terms of multiplication i'm going to multiply both sides of the equation by one half okay so i'm going to multiply this side by a half let me kind of move this down a little bit so it fits so i'm going to multiply this side by a half and i'm going to multiply this side by half again i'm going to wrap this in some parentheses to make sure that multiplies each kind of matrix there so i know that this is going to cancel and i'm left with just my matrix x which is what i want so what we have here is one half on the right times the quantity matrix b minus matrix a and remember because of our distributive property we know that we could do one half times b minus one half times a or we could do it this way it doesn't really matter so let's go ahead kind of erase all this we know what we want to find let me kind of scooch this down a little bit it's going to be hard to kind of do this on one sheet so let me just kind of copy this let's go down to a fresh sheet where we have lots of room to think and work so let's start out by just doing b minus a first okay and then we'll multiply the result by the scalar one-half i think that's a bit easier so if i do again matrix b this was we had six and five in our top row in the bottom row we had 0 and 9 and then for matrix a we had what we had negative 1 and 3 in the top row in the bottom row we had 4 and 7. so again if you want you could do plus negative or you could do minus so if i'm doing minus i would do 6 minus a negative 1. if i was doing plus negative i'd end up doing 6 plus the opposite of this which is positive so whatever you want to do it's fine i usually do plus a negative okay it makes it a little bit easier so i would change the sign of everything here so this would be plus this would be negative this would be negative this would be negative okay it just makes it easier to keep track of the signs so b plus negative a we're going to have what b plus negative a is gonna be equal to six plus one is seven five plus negative three is going to be two zero plus negative four is negative four nine plus negative seven is going to give me positive 2 okay so this is b plus negative a and then to solve for x i just want one half of this matrix okay so let's just do this real quick we'll erase this kind of scooch this up a little bit and so what i'll say is this is equal to one half times seven is seven halves we'll have one half times two which is one we'll have a half times negative four which is negative two and a half times two which is one okay so our matrix x what we're trying to find is going to have a top row of seven halves and one and a bottom row of negative two and one now let me copy this real quick and let me just kind of erase this we don't need this anymore i'm just going to say that x is equal to again this matrix is going to be 7 halves and 1 and it's going to be negative 2 and 1. now if you want to check this and i advise you to check these when you first start think about making sure that this equation makes sense okay that it's true so 2 times x is what it's 2 times every element in x so 2x is equal to 2 times 7 halves is going to be 7 2 times 1 is 2. 2 times negative 2 is negative 4 and 2 times 1 is 2. okay so this is 2 times x so if i then add this to a what i get b so again you'd add corresponding entries so 7 plus negative 1 gives me 6. 2 plus three gives me five negative four plus four gives me zero and two plus seven gives me nine so we know here that our solution for x is correct in this lesson we wanna learn about multiplying matrices all right so in our last lesson we learned about multiplying a matrix by a scalar again a scalar is just a real number that's not inside of a matrix and we found that that process was extremely straightforward right you just took the scalar and multiplied it by every entry of your matrix to get your result when you multiply two matrices together however the process is not so straightforward it's not what you would expect okay and so what we want to do is just kind of start out with a very basic very easy example and then just build up to the kind of tougher examples that you're going to get for your test or your homework or whatever you are encountering so let's say we have these two matrices here we have r which is a row matrix it's got one single row with three negative two and five as its elements again you could call this a row vector if you wanted to then c is going to be one single column with negative one zero and negative three so you could say this is a column matrix or a column vector so what we want to start out with is finding the product of r times c so what is r times c so i can write these two next to each other like this or i can put a dot in between them it doesn't really matter so what i want to do first is just write out the size of each okay this is going to be very important so i'm going to say that r is a one by three it's got one row and it's got three columns and i'm gonna say that c is what it's a three by one it's a three by one now this is very important this is something you wanna write down you can only perform the multiplication if the number of columns in the first matrix meaning the leftmost matrix the r here matches the number of rows in the second matrix or the rightmost matrix the c here so you can just circle these two inside numbers if you write it in this manner and you can say are they the same and if they're the same you can put a check mark there and say okay i can proceed with my multiplication if those two guys are not the same you have to stop the multiplication is not going to be possible okay so you can say the product doesn't exist now the second thing you want to do here is figure out the size of the product and the way you're going to get that is you're going to look at these two outside numbers so you have this the number of rows from r and this the number of columns okay from c so this matrix will be a one by one okay so this will be a one by one so once i determine if i can perform the action and then i determine what the size of the product is i go ahead and write out my matrix i know it's a one by one so it's just a matrix with one single entry at this point i can erase all this i don't need it anymore and then i'm going to go through a process that might seem a little bit weird at first but it gets easier as you kind of do it so what i'm going to do is i'm going to think about the first element in this r this first one right here the first element in r or the leftmost one is a 3. okay what i'm going to do is i'm going to multiply it by the top element or i could say the first element in c okay so this guy right here gets multiplied by this guy right here so three times negative one all right then you're going to add to this you're going to move down by one and you're going to move down by 1. so now what i'm going to do is i'm going to do negative 2 times 0. okay so negative 2 times 0 and then i'm going to add to this i'm going to do this trick one more time i'm going to move down and i'm going to move down so 5 times negative 3 so 5 times negative 3 i'm going to find this kind of result and that's going to be what i'm going to put in there okay and this might not make any sense at this point that's okay as we practice we'll start to see what's going on here so 3 times negative 1 is negative 3 plus negative 2 times 0 is obviously 0 then plus 5 times negative 3 is negative 15. so negative 3 plus negative 15 would be negative 18. so that is going to be the result of r times c okay we're going to find that we're going to need to do a row vector times a column vector pretty often so we need to make sure we understand what we did there again we would start with this guy right here from this one times this guy right here from this one okay then we would add to that this guy right here times this guy right here then we add to that this guy right here times this guy right here okay so now that we have that down what we want to do is move on to kind of something else we know that when we talk about multiplication with real numbers it's commutative if i do 3 times 2 it's 6 if i do 2 times 3 it's also 6. 3 times 2 equals 6 and then 2 times 3 also equals 6. this is the commutative property of multiplication with kind of multiplying matrices it's not commutative okay so when you flip the order generally speaking you're not going to get the same answer and sometimes you're going to find that you can't even do the multiplication okay so what happens if we change the order here what if we did c times r well again let's kind of write this out and compare our size c is what it's a one two three row by one column matrix and r is a one row by three column matrix okay so now again we're comparing the inside numbers the number of columns from c the first one has to be equal to the number of rows from r the second one okay so those match these are the same again if they're the same you can put a check mark and say okay i can proceed then the size of this guy is going to be given by the outer numbers so it's a 3 by 3. three rows by three columns so let's erase this and let me kind of slide this down a little bit so we have c times r here and this is going to be a 3 by 3. so i always write this out first now here's where it gets a little bit tricky okay i want to write this out so you understand what's going on this is row one this is row two this is row three this is column one column two and column three okay so what i want to do here if i wanna find this entry here i wanna think about okay it's in row one column one all right row one column one so what i'm gonna do is i'm gonna multiply row one from the left one or c times column one from the right one or r or you could say row one from the first one or c times column 1 from the second one or r however you want to think about that so row 1 in c is this guy right here and column 1 in r is this guy right here again because we have such a simple example it's just going to be you know 3 times negative 1 in this case as we get to the tougher ones then you're going to have to go through what we did before where you're finding a couple of different products and summing them together but here it's very simple so it's just 3 times negative 1 which is negative three okay so that's this guy right here then we move down to this one okay so now i have my row one and column two okay so i'm basically going to stay with this one because again when i think about the row it comes from the leftmost one it comes from c it comes from the first one you've got to remember that so i stay here i'm still in my row one here but i'm now moving to column two here so i'm moving down so negative one times negative two is 2 okay and then again if i'm staying in row 1 here okay i'm staying in row 1 here i'm just moving down to column 3 here so i would have negative 1 times 5 which is negative 5 okay and if this is not kind of gelling right now it's okay as we work through more examples you'll start to pick this up now i'm moving into row 2 here in my answer so what i want to do is move to row 2 again in c because i'm always getting my row from the leftmost 1. so i'm in row 2 now here and i'm going to multiply to get this whole row by column 1 to get this one by column 2 to get this one by column 3 to get this one again because i have column 1 column 2 column 3. that's all i'm doing now because 0 multiplied by anything is 0 this whole row is going to be 0. 0 times 3 is 0. 0 times negative 2 is 0. 0 times 5 is 0. then when i get to row 3 down here again i just change the row i'm in for this first one so now i'm here okay so row 3 column 1. negative 3 times this guy in column 1 so times 3 is going to be negative 9. the negative 3 times this guy in column 2 is going to be negative 2 so negative 3 times negative 2 is 6. the negative 3 times this guy in column 3 which is 5 is going to be negative 15 okay so that's how we get the entries as we get into a tougher example in a minute you'll see that you have more work involved but the process is basically the same okay so you see that r times c is negative 18 that one entry for that matrix and c times r is not the same right they're not equal to each other completely different size this guy has negative 3 2 negative 5 0 0 0 and negative 9 6 and negative 15 as its entries all right so let's go ahead and look at a tougher example now so suppose we have matrix a and matrix b so for matrix a we have two rows and one two three columns so a i'm gonna write my size down it's two by three and then for b it's what it's one two three rows okay by one two three four columns so can we find the product of a times b again you're looking at the inside numbers the columns from a the first one and the rows from b the second one so do these numbers match here yeah they're the same okay so we know we're good to go and let me make that a little bit better we know we're good to go on the multiplication here now if i flip the order of the multiplication if i said can i do b times a you're not going to be able to because if i kind of change this up and i said that let me just kind of drag this over here and let me drag them both over here so they fit so if i look at this now the two inside numbers do not match okay and this is very surprising again not being able to use this kind of commutative property that we're used to if we do a times b the product exists if we do b times a the product does not exist okay so we want to go back to a times b that's what we're going to be working with so let me just kind of slide this back over and let me just kind of slide this back over so we want to find a times b again we know that these numbers are the same so it works and we know that this guy is going to be a 2 by 4 matrix okay so the very first thing we're going to do is set up our matrix it's a two by four so a b is equal to okay and let me write this out so one two three four entries one two three four entries and another thing that i'm gonna do i did this before i'm just gonna put the columns the numbers in there just so we can keep track of what's going on this is something that really helps the students that i've tutored in the past just to keep track of where they are okay because a lot of times they go to a specific entry and they go i don't know what to do or they forget stuff or they kind of jumble it up this tells me immediately that what i'm in row 1 column 1. so where do i get that from again the row always comes from the first one the leftmost one just remember that the column comes from the second one or the rightmost one so i'm just going to start in row one in a and i'm going to highlight that or circle it or whatever you want to do just you're going to work with that for this entire row all the way across and all you're going to do is you're going to just keep going through the column so you're going to go this one times this one then this one times this one then times this one 10 times this one that's going to give your whole first row okay so for this first entry here i'm doing this top row times this first column row one column one row one column one that's all it is remember when you do this we have to go through the corresponding entries so you have the first one here and the first one here so 2 times 9 that's 18 then plus you have this guy here which is the second one times this guy here 3 times 10 is 30. then plus you have the third one here times the third one here 0 times 1 is 0. so 18 plus 30 is 48 plus 0 is still 48 okay so it's not that bad overall but it does get you know a little bit tedious especially if you have something that's kind of bigger in nature so let's erase this and move down to this one so i'm still in the first row i'm just going to the second column now okay so let me highlight that we're going to do 2 times 2 which is 4 plus 3 times 5 which is 15 plus 0 times 0 is obviously 0. so you can just leave that off 4 plus 15 is going to be 90. okay so that's how we get that one there and again we just move down so now i'm gonna be in row one column three so row one column three okay so two times zero is zero plus three times negative two is negative six and then zero times seven is zero so this is just negative six so pretty easy and let's erase this and let's go into this kind of last column here so we're going to say that we have 2 times negative 4 which is negative 8 plus 3 times 20 which is 60 and then plus we know zero times anything is zero so we don't need to worry about that and negative eight plus 60 is going to be positive 52. so we've got the first row worked out okay so now what i'm going to do since i'm going to row 2 in the answer i'm going to move down to row 2 here again the row comes from the first matrix just remember that okay so my first matrix is a so i'm just going to highlight that and then i'm going to work column by column okay that's all i'm going to do so i'm going to say ok i've got this row times this first column to give me this guy right here so we do negative 1 times 9 that's negative 9 plus 5 times 10 that's 50 plus 11 times 1 that's 11. so 50 plus 11 is 61 and then minus 9 is going to give me 52. okay so this is 52 okay so let's erase this and again i'm just shifting one over going into column two and b because i'm in column two here okay so you've got negative 1 times 2 which is negative 2 plus 5 times 5 is 25 and 11 times 0 is 0. so negative 2 plus 25 is 23 okay and again i'm just shifting down so now i'm in this column and so negative 1 times 0 is 0 plus you've got 5 times negative 2 which is negative 10 plus you've got 11 times 7 which is 77 77 plus negative 10 is 67 okay and then we have one more so we have this column here so we have negative 1 times negative 4 which is four five times twenty which is a hundred and then we have eleven times eleven which is one hundred twenty-one so if i sum all these together i know four plus 121 is 125. 125 plus 100 is 225 so there we go we found our product a b okay not so bad again i recommend writing this stuff in so you can figure out where you are row one row two column one column two column three column four again if i'm wanting this entry here i find row one times column one okay i always find the row from the left one and the column from the right one okay or you could say the first and the second one however you want to define that once you do this a few times it becomes very very simple all right let's go ahead and do another one again this is one where you need a lot a lot of practice so we have matrix a and matrix b so first and foremost let's try a b so a b the size of a it's a one two by one two three so it's a two by three the size of b is a one two three okay by a one two so again if these numbers match up you are good to go you can put a check and say yes we can go ahead and do it and the size of the product comes from these outside numbers so it's going to be a 2 by 2 matrix okay so i'm going to go ahead and set that up i'm going to say a b is equal to again it's a two by two so just draw this out you've got four entries to fill out okay and again this should help you when you start this is row one row two this is column one and column two so how do i get this entry again again again i get my row from the first one the leftmost one however you want to think about that it's coming from a so i'm just going to highlight this row here okay that's where i'm going to be for here and here and then i'm just going to go through if i want row 1 column 1 i want row 1 times this column here column 1 in b so again we go through and figure this out negative 2 times 1 is negative 2 plus you've got 0 times 2 which is 0 plus you've got 1 times 0 which is 0 so this is just going to be negative 2. okay then i'm going to slide down because i'm now in row 1 okay stay in row 1 but i'm in column 2. so row one here times column two here so that's going to be what it's going to be negative two times five which is negative ten plus zero times negative three which is zero plus one times seven which is seven this would give me negative ten plus seven is negative 3. okay so let's move on now so we're in row 2. so again now i'm going to shift from this row 1 down here to row 2. okay so since i'm starting here and it's in column 1 i'm going to multiply this row by this column okay so i'm going to have 5 times 1 which is 5 plus 9 times 2 which is 18 plus negative 5 times 0 which is 0. 5 plus 18 is going to be 23 okay 23. and then lastly what i want to do is this row times this column right here 5 times 5 is 25 plus 9 times negative 3 is negative 27 and then plus you've got negative 5 times 7 which is negative 35. so let's figure out what this is going to be well i know that 25 plus negative 27 is going to be negative 2. so basically i would have negative 2 plus negative 35 and that's going to be negative 37 okay so negative 37. so not that bad overall we found that our entries here are negative 2 negative 3 23 and negative 37. all right so let's erase this and then let's figure out if we can do b times a so b times a the size of b again is a three by two and the size of a is going to be a what it's a two by 3. so 2 here and 2 here the columns from b the first one match the rows from a the second one so we are good to go okay so we can do this multiplication the size of the product is going to be the outer numbers so 3 by three so ba b a is gonna be this three by three so i'm gonna have three numbers here three numbers here let me make this a little bigger so let me put my brackets in and again i'm just gonna do this so row one row two row three column one column two and column three okay so for the first row first column so for this entry here again i want to think about getting the rows from the first one or b so this is my first row here okay and i'm going to start with this row okay times this column here because it's column one so this first column here so let me just kind of mark this one off so 1 times negative 2 is negative 2 plus you have 5 times 5 which is 25. if i sum those two amounts together i'm going to get 23. so that's my first entry so this is going to be 23 okay and i'm just going to work through this first row so i'm going to get this one and this one and so i'm staying with this first row with b okay so that's why it's going to be highlighted so i'm just going to go through this one first so 1 times 0 is 0 plus 5 times 9 is 45 so that's going to be 45 here okay and then i'm going to move to this one okay so i'm going to do 1 times 1 which is 1 plus 5 times negative 5 is negative 25 this is going to be negative 24. so negative 24. so that top row is now done okay so what i'm going to do now since i'm in row 2 i'm just going to move this down to row 2. okay so let me erase this and let me highlight this row 2 here and we'll start out with this one so we're going to do 2 times negative 2 which is negative 4 plus we're going to do negative three times five which is negative 15. so negative four plus negative 15 is going to be negative 19. and let me erase this and let me erase this and now we're going to move into this one so we have 2 times 0 which is 0 plus negative 3 times 9 which is negative 27 this is going to give me negative 27 here so let's erase this we've got one more for this second row so let me just circle this like this 2 times 1 is 2 plus negative 3 times negative 5 is 15 so this would be 17 here okay so we just have one more kind of row down here to fill in so let me just highlight this now and we'll go through them so you basically only have to this is a shortcut if you see a zero here you know that when you did kind of this one this one and this one in each case zero times this the top one okay is going to be zero so really all i have to do is figure out what is 7 times 5 that's 35 then what is 7 times 9 that's 63 and then what is 7 times negative 5 that's negative 35 okay that's a little shortcut if you see things like that always take that opportunity because again if i did this guy right here it would be 0 times negative 2 which is 0 plus you'd have 7 times 5 which is 35 okay so i just short cutted that process to get these numbers all right so we've found ba and we've already found a b so we've got that result so now let's move on and let's talk about what happens when you multiply two matrices together that are square matrices okay of the same order so if you're multiplying a two by two and another two by two obviously you get a two by two if you multiply a three by three and another three by three you get a three by three if you multiply 117 by 117 and another 117 by 117 you get 117 by 117 okay so it's very obvious that you will always be able to kind of do the multiplication okay when you have two square matrices being multiplied together and they're of the same order so in this case a again it's a two by two and b is also a two by two so these numbers here are going to be the same and the outer numbers are going to be the same so we know that the product here a b would just be a two by two matrix okay and so we'll just do this one more time so this is row one and row two this is column one and column two so again for row one to kind of find this guy right here i go to the row one from a the first one okay so this is my row one here and for the column i find it from here so this is my column 1 here so i take the product of this row times this column to find this guy right here so 10 times negative 2 is negative 20 plus we have 11 times 5 that's 55 55 minus 20 would give me 35 and then what would i do next well i want to move into this one so same row up here but different column over here okay so i would do 10 times 9 which is 90 plus 11 times 7 which is 77 and if i do 90 plus 77 i'm going to get 167 okay so again very very easy now i'm moving into row 2 and column 1 so for row 2 again it comes from the first one so this is my row 2 here and i want my column 1 here i want the product of those two so negative 3 times negative 2 is 6 plus i'd have 1 times 5 which is 5 this gives me 11. and then i just move down one so i'm going to do this column here now so when we look at this we have negative 3 times 9 that's negative 27 plus 1 times 7 which is 7 negative 27 plus 7 is negative 20 okay so that's going to be my product a b if you want to flip this around just for practice and do ba okay again it's going to be the same size but the way that you calculate it's going to be different because b is now first so if i think about the row 1 i've got to think about the row 1 from b okay so this guy right here if i think about column 1 the column 1 is going to be for a okay so i want to find the product of this guy times this guy so this row vector times this column vector so negative 2 times 10 is negative 20 then plus 9 times negative 3 is negative 27 negative 20 plus negative 27 is going to be negative 47. again we just proceed so now i'm going to kind of move to this column here and i'm going to find the product of these two so negative 2 times 11 is negative 22 and then plus 9 times 1 which is 9. so negative 22 plus 9 is going to give me negative 13. and now what i'm doing is i'm moving into row two okay so moving into row two again that's from the first one so this guy right here i'm in row two and i'm gonna start with column one here so row two column one five times ten is fifty plus 7 times negative 3 is negative 21 50 plus negative 21 is going to be 29 and then we just have one more to do again i'm just going to shift this column here to this guy because i'm in column 2 now so i'd have 5 times 11 which is 55 plus 1 times 7 which is 7. 55 plus 7 is going to give me 62. okay so hopefully this is clear for you this is something that you do need to practice quite a few times to have it down okay you'll develop your own method your own strategy for doing this but typically i like to write out the row and the columns like i did in these examples here to keep track of where i need to be in my multiplication i've found that a lot of students that struggle with this if they just employ this kind of simple technique they end up really being able to improve their result quite substantially in this lesson we want to talk about gaussian elimination and also gauss elimination all right so over the course of the last few lessons we've been talking about how to solve systems of linear equations in two variables and also three variables we basically covered everything that you would see in a typical algebra 1 and an algebra 2 course now what we're going to do is just go a little bit further and we're going to talk about these kind of matrix methods that we can use and we'll see a lot of these as we kind of progress through this chapter today we're just going to start with gaussian elimination and gauss jordan elimination now some of you saw this in algebra 2 others didn't i just want to assure you that this is an easy process most of the difficulty involved with this is just keeping track of what's going on okay so to begin i'm just going to revisit this first system using the elimination method and then we're going to go into the matrix method from there okay so i'm going to start out with 4x plus 16y equals 28 and 5x plus 8y equals negative 1. so once again i'm just going to label this as equation 1 and this is equation 2. now with the elimination method we know that our goal is to eliminate one of the variables okay so we want to take one pair of variable terms okay and we want to make the coefficients opposites so i look at the kind of coefficients for x i have 4 and 5. then the coefficients for y i have 16 and 8. so it's easier to work with y right because i can basically multiply this 8 by negative 2 and i would have negative 16 there so this would give me a negative 16 y and a 16 y okay so what i want to do is i want to multiply equation 2 by negative 2. so i'm going to multiply 5x by negative 2 and get negative 10x i'm going to multiply 8y by negative 2 and get negative 16y i'm going to multiply negative 1 by negative 2 and get 2. okay so that's my transformed equation again we're always allowed to multiply any equation by the same non-zero number and preserve the solution now what i'm going to do i'm going to take my equation 1 i'm going to leave it as it is so 4x plus 16y is equal to 28. okay so two things here before we kind of do the elimination i want to make sure you understand that when you do the elimination method you want to make sure that your equations are lined up properly so in other words i want to write everything in standard form so ax plus b y is equal to c okay so ax plus b y equals c a x plus b y equals c and the reason for that is is i'm adding the two left sides together and setting this equal to the sum of the two right sides so i want to have like terms on each side right so when i go to add these two equations together on the left sides i have my negative 10x plus my 4x that's going to give me negative 6x then over here i have my negative 16y plus my 16y and i'm going to go ahead and just cancel this right that's zero and this equals two plus 28 which is third okay so let's erase this we already know x is going to be negative five i can complete the process by just dividing both sides by negative six and i'll just write over here that x is negative five right 30 divided by negative 6 is obviously negative 5. okay so pretty easy overall we know how to do that and we could just plug in for x in either equation one or two find out what y is right very simple so let's just take equation two because the numbers involved are smaller so 5 times negative 5 plus 8y equals negative 1. we know 5 times negative 5 is negative 25. so plus 8y equals negative 1. let me add 25 to both sides and if i add it over here this is going to cancel so let me just erase it and over here this would be 24. so let's just quickly do this we'll divide both sides by 8 and we'll find that y is equal to 3. okay so very simple very easy something you've been doing since algebra 1 and also for the last few lessons in this course as well what we want to do now is think about how we could solve this using a matrix so we're going to start out with the kind of gaussian elimination okay this is usually the first thing that you learn and for gaussian elimination the very first thing you need to do is you need to set up an augmented matrix okay so for an augmented matrix i'm just going to take the numerical information only but again i got to make sure that these equations are written in the correct format so i want to do ax plus b y equals c and i'll explain why this is important in a minute but essentially i'm just going to grab this information and i'm going to write it in the order that it appears so i'm going to take my 4 i'm going to take my 16 and i'm going to take my 28 okay so this row going across represents the numerical information from this equation 1. okay so that's all that is then the second row that i'm going to form which is going to be the 5 the 8 and the negative 1 that's the numerical information from equation 2. so the reason we want to write it in that ax plus b y equals c format is that i now know that this column here this leftmost column that numerical information corresponds to the coefficients of the x variable then i know that this column here is going to be the coefficients for the y variable and i know this column here is going to be the constants so writing it in that manner is what allows you to know that immediately okay so what i'm going to do now is i'm just going to put some brackets around this and i'm going to put a vertical line here that's going to separate the coefficients from the constants okay so this is our first step and i'm just going to copy this now and go to a fresh sheet okay so the goal is to use these what are known as elementary row operations or sometimes you'll hear them called matrix row transformations to put the matrix into what is known as row echelon form okay so row echelon form looks like this so we're going to have this let me put this and i'm just going to use this symbol here to represent a real number so what we're going to have is we're going to have ones starting at the upper left and going down on a diagonal okay and then below it's going to be a zero here okay so i want to make this a one and this a one and i want to make this a zero okay just using those row operations that i'm about to tell you now these row operations are the same things we can do when we're solving a linear system with just equations right if we didn't have a matrix involved so the first thing is we can interchange any two rows so that means i could really write this matrix here by saying i have 5 8 and negative 1 and then i have 4 16 and 28 okay so i took the kind of second row and flipped it to the first one took the first one and flipped it down to be the second one this matrix represents the same linear system okay it's just like if we had these two equations and i chose to write this equation two up on top of equation one that doesn't make any difference to the answer okay so you can always interchange any two rows the second thing we can do we can multiply or divide the elements of any row by a non-zero real number that should make sense because again if we go back up we remember that we multiplied equation two by negative two when we're solving this right we can always multiply both sides of an equation by the same non-zero number and preserve the solution so since this is just the numerical information from the equation in other words if i took this matrix and i multiplied row 2 by let's say negative 2 this would be negative 10 this would be negative 16 okay and this would be positive 2 okay this would be positive 2. and that's exactly what we saw when we were working with this kind of system right we multiplied equation 2 by negative 2 and again we got negative 10x we got minus 16y this was equal to positive 2. now the third and final elementary row operation tells us that we can replace any row of our matrix by the sum of the elements of that row and a multiple of the elements of another row so this goes back to what we did when we looked at this remember we multiplied equation 2 by negative 2 and then we added that to equation 1. well it's the same thing that we are allowed to do here okay it's just that we're working with the numerical information only okay so let me move this over here so that we can keep in mind what we're trying to do and let's just go ahead and start on this this doesn't take that long when you first start doing this you want to use a lot of scratch paper and you want to really take your time because it's easy to make very simple mistakes on this that give you the wrong answer all right so the way i'm going to do this i'm going to get a 1 for this first position here right because we need a 1 there and then once that's done i'm going to get a 0 in this position here okay so you want to finish a column at a time working from left to right okay so what i'm going to do to get a 1 there you got to think about what's legal again you can multiply a row by a non-zero real number well how can i get something there that's going to be 1 4 times what would give me 1. remember 4 times its reciprocal which is 1 4 would be 1 okay so what i can do is i can multiply this whole row row 1 okay by 1 4. now the way i'm going to show you this okay is something you might see in your textbook so you'll see this abbreviated for row 1 you'll see r with a 1 down there so r sub 1 you could just say r1 it doesn't matter and that just means row 1. so i'm going to say 1 4 times row 1 is going to replace row 1. so i'm going to multiply 4 times 1 4 okay and obviously that's going to give me 1. i'm going to multiply 16 times 1 4 and that's going to give me 4 and i'm going to multiply 28 times 1 4 and that's going to give me 7. okay so i'm going to replace the top row with what i just found so let me write that right here so i'm going to have a 1 i'm going to have a 4 and i'm going to have a 7. and then this bottom row stays the same so 5 8 negative 1. let me close that off and let's erase this and we can even erase the original one we don't need it anymore let's just move this up so that we have room to work okay so for the next step i want to get a zero here right i've already got my one here so this is done all right so now i want my zero so how can i get a zero well this is the tricky one it's always harder to get zeros but basically what you want to do is multiply row one by something and then add that result to row two so that this becomes a zero okay so in other words i've got to add something to 5 to get that to be 0. so what can i add to 5 to get a result of 0 i want to add negative 5 okay and how can i get negative 5 well i would multiply 1 by negative 5 okay and then add that to 5 to get my 0. that's all i need to do is multiply this top row or row 1 okay so row 1 times negative 5 and then we're going to add that to row 2. so that's all this is saying so we're going to say that we're replacing row 2 with negative 5 times row 1 plus row 2. so again when you first see this it's a bit confusing but after you work enough examples it becomes pretty easy so if i take negative 5 and i multiply it by everything in row 1 negative 5 times 1 is negative 5. then 4 times negative 5 is negative 20 and then we have 7 times negative 5 which is negative 35. okay so i'm going to now add these values and let me just take those results so negative 5 plus 5 is 0 negative 20 plus 8 is negative 12 and negative 35 plus negative 1 is negative 36. so i can just replace these with those numbers so i'm going to have a 0 i'm going to have a negative 12 and i'm going to have a negative 36 okay all right so we just have one more thing to do okay so i'm gonna just highlight this to show that it's done so now what i want is i want this to be a one okay so again getting a one is easy because i just multiply by the reciprocal of what i want to be a one okay so i would multiply row 2 by the reciprocal of negative 12 which is negative 1 12. okay so 0 0 times negative 112 would be what it would be zero okay so this is unchanged this equals 0. negative 12 times negative 1 12 is obviously 1 and then negative 36 times negative 1 12 we know that this cancels with this and gives me a 3 negative times angle is positive so this is three okay so this is zero this is one okay and this is three now at this point at this point we've achieved row echelon form okay you have your ones down this diagonal here okay and you have a zero below now this is enough information for us to get a solution for the system remember this column is the coefficients for x this column is the coefficients for y this is the costs okay so here's where this comes into play so i'm going to take this row 2 this 0 that's the coefficient for x plus this 1 that's the coefficient for y and this equals this 3. okay so 0x plus 1 y equals 3 what does that really tell me 0 times x is 0. so really i have 1y or just y equals 3. so y equals 3 is part of my solution right if i go back up we know y equals 3. right so we found that part and now what we can do is we can just back substitute and find x so i can take this information here this is 1x plus 4y i know y is 3 is equal to 7. 4 times 3 is 12. if i subtract 12 away from each side of the equation i'll find that x is equal to negative 5 and again if i go back up i see that x is negative 5. so again it's not really that hard it is a little bit time consuming when you first start because you have to figure out what's going on you have kind of some new notation and some new concepts to wrap your head around but once you kind of get going with this this is a very very quick process now we talked about this gaussian elimination in this row echelon form there's this other related concept which is gauss jordan elimination so what happens with gauss jordan elimination is we take this a step further instead of there just being a zero down here now i would put a 0 here okay so let's do that real quick let's transform this kind of entry here this 4 into a 0 and what we'll see is that this gives us all the information from the matrix directly right we don't have to do any substituting and this is known as reduced row echelon form okay so reduced row echelon form so how can i get this to be a zero again i can use row two here the fact that i have a one there okay and i can say okay 1 times negative 4 would be negative 4 negative 4 plus 4 is 0. so multiply row 2 by negative 4 and then add the result to row 1. okay so this is going to give me my row 1. that's what i'm going to replace it with so we know 0 times anything is 0 so this isn't going to change i don't need to do anything for that 1 times negative 4 would be negative 4 then negative 4 plus 4 would be 0 okay so that's done and then 3 times negative 4 would be negative 12 and negative 12 plus 7 is going to give us negative 5. so you see that we have our answer now directly from the matrix okay very easy to read because again this is 1x plus 0y equals negative 5 or i can say that this is x equals negative 5 okay and then this guy right here is 0x plus 1y equals 3 or i can just say y equals 3. okay so directly from this matrix i read the solution and although it took longer for us to do it this way that's just because we're getting start okay so let's go ahead and take a look at another example so the second example here we have negative 10 equals 9y minus 10x and we have negative 9x equals negative 4y plus 32. okay so the first thing we want to do again make sure this is written as ax plus b y equals c okay so for this guy over here really all i have to do is just kind of flip it i don't need to change anything i can just say this is negative 10x plus 9y equals negative 10. that's perfectly legal then for this guy right here i have my negative 9x i'm going to add 4y to both sides of the equation and this equals 32. okay so now what i want to do is just take the numerical information only and i want to make sure i write it in the order that it comes in so negative 10 9 and negative 10 then i want negative 9 4 and 32 okay so i put my bar here to separate the coefficients from the constants and we're ready to go so let me copy this we'll go to a fresh sheet and let me paste this in here and let me just rewrite this row echelon form and actually let's just use reduced row echelon form so i want ones down the diagonal and i want a zero above and below and then over here i'll just put this symbol here to represent two real numbers okay and you can use letters there if that makes you more comfortable you could say this is i and this is j it just represents two real numbers so i'll just put these symbols in here to represent that okay so we're going to start out by getting a 1 in this position here that's how i always start and then i'll get a 0 below and then i'll move to the right so again if i want to get a 1 it's really easy just think about what you're trying to make into a 1 and just multiply that row by the reciprocal of that so for negative 10 the reciprocal is negative 1 10 so i'm going to multiply the entire row 1 by negative 1 10. so negative 1 10 times row 1 that's what we're going to replace row 1 with so negative 1 10 times negative 10 would be 1 this would be 1. 9 times negative 1 10 would be negative 9 10 okay and then negative 10 times negative 1 10 this would cancel give me positive 1. okay so now that i have that done the next thing i want to do and i'm just going to mark this off and say this is done the next thing i want to do is get a 0 here okay so i want that to be a 0. again not that hard because you have a 1 here okay so i can just think about i need to add a 9 a positive 9 to make negative 9 a 0. okay so the way i do that is i think about okay if i need that 9 i just multiply 9 by this whole top row okay and then i add the result to the second row so i'm going to say that i have 9 times row 1 okay the top row plus my row 2. this is going to give me my row 2. okay that's what i'm going to replace row 2 with so let's do this down here so we can keep track of what's going on so 9 times 1 is 9 and then negative 9 10 times 9 would be negative 81 tenths so negative 81 tenths and then 9 times 1 would be 9. okay so i'm going to add these to these kind of corresponding entries so 9 plus negative 9 we know that's 0. so this is 0. let's erase this if i had negative 81 10 plus 4 that's going to be a bit of work so let's do this one for a minute 9 plus 32 we know that's 41. so let's just knock this one out real fast and then this over here i got to get a common denominator going so i know this is going to be plus 40 over 10 right if i took 4 and multiplied it by 10 over 10 i'd get 40 over 10 and now this is going to simplify to negative 41 over 10. so i'll say this is negative 41 over 10. okay so that part's done pretty easy now i'm just going to check this off and say that's done and i'll move on to this one right here i always want to get my one first okay so i want to make this into a 1 that's super easy to do again i just multiply that whole row by the reciprocal of that the reciprocal of negative 41 over 10 is negative 10 over 41. okay that has no impact on zero if i multiply this by this i get one okay i get one and if i multiply this by 41 the 41s are going to cancel and i get negative 10. okay i get negative 10. at this point we have our row echelon form right our gaussian elimination is completed but we can take it a step further again and put it in this reduced row echelon form again that's from the gauss-jordan elimination so what i'm going to do is i'm going to make this guy right here into a zero and the way i can do that again i'm gonna use this one right here to my advantage i'm gonna multiply row two by what's necessary to kind of get rid of this negative nine tenths or change it into a zero okay so if i had negative 9 10 i would want to add 9 10 to make that 0. so i'm going to multiply row 2 by 9 10 so 9 10 times row 2 add the result to row 1. again i just use this symbol to say hey i'm going to replace row 1 with 9 10 times row 2 plus row 1. okay so let's go ahead and do that so i know that 9 10 times 0 is 0 and 0 plus 1 is still 1. okay so that's no change and then 1 times 9 10 is 9 10 9 10 plus negative 9 10 is 0. so we've got that and then for this last one here i'm going to have 9 10 times negative 10 which the tens are going to cancel is going to give me a negative 1 so this would be negative 9 okay negative 9 and then i'm going to add the result to this row 1 so negative 9 plus 1 is negative 8. okay so this is negative eight and we don't need to do any substituting we already can just look at this and know our answer we can say that one x plus zero y equals negative eight or we can basically say x equals negative eight and we could say that 0x plus 1y equals negative 10 or just y is equal to negative 10. okay so again not as quick as we're used to but just because we're getting this kind of process started once you really get going with this you can do this very very quickly all right let's take a look at one more of these and then i'll show you the kind of special case scenarios and what happens with those so we have 12x equals negative 11y plus 20 and then 12y equals 15x plus 78. so again what i want to do is i want to start by writing everything in the format of ax plus b y is equal to c okay so over here i have 12x i'm just going to add 11y to both sides and this will be equal to 20. over here i'm going to subtract 15x away from both sides and then plus 12y and this will be equal to 78. now one thing you want to do if you notice that everything in an equation is divisible by some number you want to take the opportunity to simplify it because it's going to make your calculations easier so if i look at this second equation or this bottom equation everything is divisible by 3. this would be negative 5. this would be 4 and this would be 26 right i just divided every part of that equation by 3. that's something you definitely want to do because it's going to make it easier to achieve your answer okay so let's scroll down just a little bit i'll set up this matrix so again just take the numerical information so 12 and negative 5 11 and then 4 and then you have 20 and then 26. okay so let's go ahead and copy this and again what we're trying to do is get this reduced row echelon form so ones down the diagonal and a zero above and below and then over here we're just going to have real numbers okay again this is reduced row echelon form and once more if you have a number here then it's row echelon form okay so let's get started again by making this into a 1 very easy to do we're just going to multiply the top row by again the reciprocal of that number the reciprocal of 12 is 1 over 12. so again i'll write this as 1 12 times row 1 that's what we're going to replace row 1 with so this would be 1. 11 times 1 over 12 is 11 12 so 11 12 20 divided by 12 let's write that over here 20 divided by 12 each is divisible by 4 so this would be 5 and this would be 3. okay so this would be 5 3. so 5 thirds okay so now i want to get a 0 here let me kind of highlight that to show that it's done i want to get a 0 here so again if i have negative 5 i know that if i add 5 to that i'd get 0. so i'm going to multiply row 1 by 5 add the result to row 2. so 5 times row 1 plus row 2 that's what i'm going to replace row 2 with so 1 times 5 is 5 5 plus negative 5 is 0 okay and then 11 12 times five we know this would be what this would be fifty-five twelfths so fifty-five twelfths and then plus four to get a common denominator i'd say this is forty-eight twelfths so what's 55 plus forty-eight that's gonna be a hundred three okay so this would be a hundred three twelfths one hundred three twelfths okay and then for this guy right here again we have five times five thirds five times five thirds so that's 25 thirds and then i'm going to add that to 26. again 26 times 3 is 78 so i could say this is 78 thirds and 25 plus 78 is 103. so you get 103 over 3 okay so let's erase this and i'll mark this one as done and now i want to get a 1 here okay i want to get a 1 here so again that's very easy to do we're going to multiply row 2 by the reciprocal of this number so i'm going to have 12 over 103 okay times row 2. again i'm going to replace row 2 with that so i know that 0 stays unchanged because 0 times anything is just zero and then this guy would be transformed into a one and then this guy i would have what i would have 103 over three times 12 over 103. so we know this would cancel with this and this divided by this would be 4 so i can erase this and just put a 4 here so again at this point i have my row echelon form so if i wanted to go back and substitute i could find my answer at this point and i'd be done but i like to take it to this form where this guy right here is a 0 as well and i can just read my answer straight from the matrix so how can i get a 0 here again i want to think about what can i add to 11 12 to make it 0 what would be a negative 11 12 and so i would just multiply row 2 by a negative 11 12 and add the result to row 1. so i'm going to multiply negative 11 12 times row 2 add the result to row one okay that's how i'm going to get my row one okay that's what i'm going to replace it with okay so negative 11 12 times zero i know is zero zero plus one is still one no change there then negative 11 12 times 1 is negative 11 12 then plus 11 12 is 0. so that's what we want there and then negative 11 12 times 4 this cancels with this and gives me a 3. so i'd have negative 11 thirds and then i'm going to add this to 5 thirds so negative 11 plus 5 is going to be negative 6 so this would be negative 6 thirds which is negative 2. so this is negative 2 and again we found our answer so we know that at this point what x is equal to negative 2 x is negative 2 and y is 4. now let me show you what happens when you have a special case scenario again you know that you have systems that have no solution and you know you have systems with an infinite number of solutions again if you end up with kind of a false statement then you know you have no solution if you end up with just a true statement you know that you have an infinite number of solutions so let's go ahead and start this in the usual way i'm going to have a negative x i'm going to subtract 3y away from each side and i'm going to subtract 1 away from each side and then over here i'm just going to flip this and say this is 8x plus 24y is equal to 17. again that's perfectly legal so the numerical information from this i'm just going to take a negative 1 a negative 3 and a negative 1 and then an 8 a 24 and a 17. okay so we're good to go with this let me just copy this and i'll paste it in right here for us and again i'm trying to achieve this form one's down the diagonal a zero above and below again this is reduced row echelon form and then these will just be numbers over here and once more just so we don't forget if this is just a number over here and it's not a zero then this is row echelon form okay so that's the difference between the two it's something you might get asked about okay on a test they might tell you to put it in row echelon form or they might say put it in reduced row echelon form and you need to know the difference between the two okay so i want to get a 1 as the first entry right here how can i do that again we've been doing this all day now so i'm just going to multiply row 1 by the reciprocal of that number okay so negative 1 is the reciprocal of negative 1 right because if i wrote this over 1 i would say that the reciprocal of it is 1 over negative 1 again it's just negative 1. so i would say that we have negative 1 times row 1 that's what i'm going to replace row 1 with so negative 1 times negative 1 is 1 negative 1 times negative 3 is 3 negative 1 times negative 1 is 1. okay so now what i want to do is get a 0 here so how can i do that again we've been doing this all day i know i need to add a negative 8 to 8 to get 0. so i'm going to multiply row 1 by negative 8 okay so row 1 times negative 8 i'm going to add the result to row 2. again this is going to give me my row 2. all right so negative 8 times 1 is negative 8 negative 8 plus 8 is 0. then negative 8 times 3 is negative 24 negative 24 plus 24 is 0 should already see that there's a problem and then let's just do let's just keep going although you know there's no solution as a point just keep going negative 8 times 1 is negative 8 negative 8 plus 17 is 9 okay is not so let's stop for a minute and consider what we have we have 0x plus 0y equals 9. so 0x plus 0y equals 9. what is 0x it's 0. what is 0y it's 0. so basically we have 0 plus 0 which is 0 equals 9 which is always false so this is no solution okay and if we went back and we really observed this we would see that we have two lines that are parallel or two lines that have the same slope if i just look at this as three y equals you have negative x plus one again divide everything by three to put it in slope-intercept form and the slope here is going to be negative one-third okay for this one let me write this as 24y 24y is equal to negative 8x plus 17. i'm going to divide everything by 24 and you can already see that you have the same slope right you can erase this negative 8 over 24 is negative one-third okay negative one-third and then 17 over 24 you can't simplify that but you don't need to because you can see that this slope and this slope those are the same when we have parallel lines again we know that they don't ever intersect so there's no point that lies on both lines so there's no solution for the system okay so this guy right here you put the symbol for the null or empty set or again just write no solution let's look at one more problem just to kind of wrap this up and then in the next lesson we'll look at some systems with three variables so now we have negative 1 equals 2x plus 5y so i just have to flip this so 2x plus 5y equals negative 1. and then for this one i have negative 6x minus 15y equals 3. so we're good to go at this point but before we even start you can immediately notice that what if i multiply equation 1 or this top equation by negative 3 i get equation 2. okay 2x times negative 3 is negative 6x 5y times negative 3 is negative 15y negative 1 times negative 3 is positive 3. so you know that this is going to be an infinite number of solutions for the answer okay because these two equations are the same so whatever works in the first equation also works in the second equation now let's show this with a matrix we have 2 5 and negative 1 and then we have negative 6 negative 15 and 3. so let's copy this so again i'm looking to get this kind of reduced row echelon form so one's down the diagonal a zero above and below and then these are just numbers okay so i'm going to get a 1 here in this first position we already know what we're doing multiply row 1 by one half which is the reciprocal of 2. so one half times row 1. that's what i'm going to replace row 1 with so this would be 1. this would be five halves okay this would be five halves and this would be negative one half so this would be negative one half now for this guy right here i want to make this into a zero and to do that i'm just going to multiply row one by six add the result to row 2. so i'm going to multiply again row 1 by 6 add the result to row 2. so that's what i'm going to replace row 2 with so 6 times 1 is 6 6 plus negative 6 is 0. okay then five halves times six this cancels gives me three three times five is fifteen fifteen plus negative fifteen is zero negative one half times six the six cancels with the two and gives me three negative one times three would be negative three so i would have negative three plus three which is zero so again i'm going to end up with zero equals zero which is always true so that's how i know i have an infinite number of solutions so let's just write that out we have an infinite number of solutions in this lesson we want to continue to talk about gaussian elimination and also gauss jordan elimination all right so in our last lesson we learned about the gaussian elimination and also the gauss-jordan elimination and we saw that we could take a linear system in two variables we could take the kind of numerical information and set up an augmented matrix and through these elementary row operations we could kind of pound our matrix into a specific form okay now with the gaussian elimination it's this one over here on the left this is referred to as the row echelon form where you have these ones down your diagonal okay so starting at the top left and then going down at a diagonal okay and then a zero below okay and what this tells you is that you can find out in the case of where you have your x variable the coefficients are on the kind of left column and the coefficients for the y variable or kind of next to it you could say that this bottom row here is 0x plus 1y is equal to whatever this is let's just call it d for this video so let's say this is d so really you know that y is d and you can go back up and plug in and find out what x is so that's our gaussian elimination with the gauss-jordan elimination it goes a step further okay so now the matrix is put into something known as reduced row echelon form so that's where we have these ones going down the diagonal a zero above and below and so now directly from the matrix i can read my solution what we're going to do today is we're just going to kind of expand on this and we're going to look at some kind of linear systems with three variables and then in the next lesson we'll even look at some linear systems with four variables okay so i want you to see the kind of row echelon form for this so you're going to have ones down the diagonal zeros below and of course you have to back substitute to get your answer and then the reduced row echelon form you have ones down the diagonal zeros below and zero above so with the reduced row echelon form you get your answer directly from the matrix this guy on the right on the left the row echelon form you have to do some back substituting all right so let's go ahead and start this with kind of an easier example i am going to warn you that this process does get really tedious i'm going straight off of what your textbook would show you which is where you get a one in your column first and then you find your zeros okay there are faster methods to do this and you can use those if you want but i prefer to go with just what the textbook's going to show you so you don't get confused and then as we kind of progress we're going to find some easier ways to do this all right so let's go ahead and read this off we have x plus y plus 3z equals negative 1. we have 3x plus 3y plus 4z equals negative 3. and we have negative 5x minus 3y minus 60 equals 13. so again i want all my equations in the format of we have the ax plus by plus cz is equal to d okay the reason i want that is i want all my x terms to line up my y terms my z terms and my constants because when i write this augmented matrix i want all the coefficients for x to be in one column all the coefficients for y to be in one column all the coefficients for z to be in one column and then all the constants to be in one column so let's go ahead and set that up i'm just taking the numerical information only we know that x has an implied coefficient of one y has an implied coefficient of one we have a three and we have a negative 1. then for this second equation i have a 3 a 3 a 4 and a negative 3. for my third equation i have a negative 5 a negative 3 a negative 6 and a 13. so let's just copy this and let me paste that in right here we know that we want to put a vertical bar here to separate the coefficients from the constants and then i'm going to wrap this whole thing in some brackets so the very first thing you want to do if you're doing gaussian elimination you want to get ones going down the diagonal and you want zeros below this is faster right because you don't have to go through and keep manipulating the matrix to get it into reduced row echelon form from gauss-jordan elimination i'm going to do both with you i know it's going to be a lot of time i know it's going to be tedious but i want you to get a lot of practice with this so for the first one i'm just going to go through and do the gaussian elimination we're going to do row echelon form we're going to back substitute and then i'll go back into it okay and we'll we'll pound the matrix further and get it into reduced row echelon form so what i want is in the first column on the left i want the top entry to be a one it already is so that's going to save us some time so then i want this to be a zero and this to be a zero okay so the reason your textbook always wants you to get a one in your column is because it's easy to use it to get a zero it's always easy to get a one hard to get a zero so how do i get a zero remember i can multiply any row by a non-zero number okay and add the result to another row so what's going to happen is i got to think about what i can add to 3 to get a result of 0 well 3 plus its additive inverse of negative 3 would be 0 and it's convenient to have a 1 there because 1 times anything is itself so if i multiply this by negative 3 i get negative 3 and then if i add negative 3 to 3 this is going to be 0. i don't care about any of this stuff over here i'm only worried about getting a 0 here so the way we're going to do this is we're going to say okay i'm going to multiply the additive inverse of 3 which is negative 3 times this top row or row 1. let me just label these real quick so this is row 1 row 2 and row 3. so negative 3 is going to be multiplied by row 1 and then i'm going to add to row 2 and that's what i'm going to replace row 2 with okay so that's what that notation means and then let me do this one too real quick while we're kind of working on this we can do two at once so we have this negative 5 here so what would i need to add to negative 5 to get a 0 i would need to add positive 5. so i would multiply this top row here by positive 5 and it's really the same thing i'm just changing this from row two to row three and that's what i'm going to replace row three with okay so let's go through this it's a good idea to kind of write these things down on some scratch paper because one mistake here and it's gonna kill you right you're going to get the wrong answer you've got to start the whole thing over so negative 3 times row 1 that's what we're going to do first so i'm just going to write these answers kind of down here so negative 3 times 1 is negative 3 negative 3 times 1 is negative 3 again negative 3 times 3 is negative 9 and then negative 3 times negative 1 is positive 3. so now what i'm going to do is i'm going to add these kind of results it's just this part right here i'm now adding it to row 2. okay the corresponding entries all right so negative 3 plus 3 is going to give me 0 and then i'll have negative 3 plus 3 again that's going to be 0. i'm going to have negative 9 plus 4 that's going to be negative 5. and then i'm going to have 3 plus negative 3 which is going to be 0. okay so i'm done with this and i'm done with this so i'm just going to erase this and now i'm going to move on to this one so if i do 5 times 1 that's 5. 5 times 1 again that's 5. 5 times 3 is 15 and 5 times negative 1 is negative 5. so now i'm adding these results okay this part right here to row three okay if you ever get lost in what you're doing just look at your notation it's going to help you keep track of what's going on so 5 plus negative 5 is 0 and then 5 plus negative 3 is going to be 2 and then 15 plus negative 6 is going to be 9 and then negative 5 plus 13 is going to be positive 8. okay so this part's done now i've got one up here i've got zeros below so as i move to my next column i want this to be a one again if i'm doing gaussian elimination okay gaussian elimination i want ones down the diagonal and zeros below i don't care about anything else in this particular case i can't just multiply by the reciprocal okay because that's what i normally do right if i had a 2 in that position i could multiply that whole row by half and a half times 2 would give me 1. but because i have 0 and it's unique because 0 times anything is 0 what i'm going to do is i'm actually going to switch row 2 and row 3. remember we can always interchange 2 rows so i'm going to say row 2 is going to switch with row 3 and so this is going to change in b let me kind of write this up here so we keep track so i'm going to have 0 0 negative 5 and 0. so i'm just going to erase this and i'm going to copy this so 0 2 9 and 8 and then i'll write this in here what i just did so 0 0 negative 5 and 0 and it's just a good idea to use a lot of scratch paper because again one silly mistake and the whole thing is blown all right so let's go through now and make this into a one i'm going to multiply the reciprocal of 2 okay which is one half times that entire row so times row two that's going to give me my new row two and i want you to think about something i'm not affecting this to the left because zero times anything is always zero so there's no change on this this becomes what we want which is a one so i really only have to worry about this being a nine halves and then eight divided by two is going to be four okay so that takes care of that all right so now to complete this row echelon form i just need a one here and i'm done right i can go back and substitute and find my answer so i would want to multiply row 3 by the reciprocal of negative 5 which is negative 1 5 because negative 1 5 times negative 5 would give me 1 so that's what i'm going to replace row 3 with of course zero times anything is zero zero times anything is zero zero times anything is zero so i only need to change this into a one and at this point we've achieved our row echelon form so we can just go back substitute figure out our answer right so we can say that since this column here represents the coefficients for z we would have z or one z is equal to zero these two guys are zeros so those variables are eliminated you can think about this as saying you have zero x plus zero y plus one z equals zero well zero times anything is zero so this is zero plus zero plus one z which is just one z or z equals zero so now i can start back substituting if i know z equals zero i can take these coefficients here i can just say this is 0x plus 1y plus 9 halves times zz is 0 and this equals 4. well i know that this is gone right 0 times anything is 0. i basically just have this gone and y is just equal to 4 y is just equal to 4 so that's pretty easy and then in terms of x i know that 1x plus 1y okay y is 4 so just 4 plus 3z okay z is 0. so i can just say leave that off this would be equal to negative 1. well okay i can just subtract 4 away from each side of the equation and find that x is equal to negative 5. so x is going to be negative 5 y is going to be 4 and z is 0. so at this point if you're just told to find the answer and you want to use a matrix method you can just do it this way because this is a little bit quicker than going all the way through the process with the gauss jordan okay if you want to go through the remainder of this which we're going to do what you need to do is you need to get a 0 here you need to get a 0 here and you need to get a 0 here and what that does is it allows us to kind of read the answer directly from the matrix all right so let's go ahead and crank this out i'm going to start off with this guy right here in my middle column so again i'm going to use this one to my advantage i'm going to multiply row 2 i'm going to multiply row 2 by negative 1 which is the additive inverse of positive 1. i'm going to add the result to row one okay that's how i'm going to get my new row one so let's just go through and multiply negative one times everything in row two so negative one times zero is zero we have negative one times one which is negative one we have negative one times 9 halves which is negative 9 halves and then we have negative 1 times 4 which is negative 4. so what i'm going to do now is i'm going to add these results here this part right here to the corresponding entries in row 1. so we know 0 plus anything is going to leave it unchanged don't need to worry about that negative 1 plus 1 is going to be 0. i know that negative 9 halves plus 3 let's do that off to the side in a minute and then i would have negative 4 plus negative 1 which is negative 5. okay so let's work on this one right here we're trying to figure out what this is going to be so to get a common denominator i'd multiply this by 2 over 2 okay and i would have 6 let's just write this as 6 over 2 plus negative 9 over two i know that negative nine plus positive six is going to be negative three so this would end up being negative three over two so this is done so let's erase this so now what i need is a zero here and here to finish this up it's actually going to work out pretty conveniently here because we have a 0 here a 0 here and a 0 here so what does that actually mean well remember if i'm trying to work on things in column 3 i'm going to use this 1 in row 3 to kind of work off of and what makes it easy is that when i multiply by 0 i get 0 if i add 0 to something it leaves it unchanged so what happens is if i want to multiply 3 halves the additive inverse of negative three halves times my row three in every case it's going to be zero zero and zero over here so when i add those to the corresponding entries nothing's going to change so the only thing is going to change is this okay so if i add this to row 1 and i replace row 1 with this i only need to change this and i'm just going to change it into a 0. it's the same thing here so if i multiply negative okay negative 9 halves times row three okay for that one and then i add the result to row two that's what i'm going to replace row two with once again these zeros do not affect this okay because negative nine halves times zero is going to be zero adding zero to something does not change it so i'm only going to change this guy right here and it's going to be a zero all right so we can see that we got the same answer by putting our kind of augmented matrix in this reduced row echelon form we have our ones going down the diagonal we have zeros above and below so we find that x is negative five we find that y is four and we find that z is equal to zero all right so let's go ahead and take a look at another one again as we work more and more of these they get easier and easier so we have negative 6x plus y plus 7z equals negative 32. let me just write the negative 6 the 1 for the coefficient for y that's implied the 7 and then the negative 32. okay just taking the information from the first equation then from the second one you have negative 5x plus 5y plus 5z equals negative 10. again when you see something that can be simplified meaning everything here is divisible by 5 go ahead and take the opportunity to do that because it's going to mean that you're working with smaller numbers so if i divide everything by 5 this would be a negative 1 this would be a 1 this would be a 1 and this would be a negative 2. so you'd have negative 1 1 1 and negative two okay and i can erase that and then for the last one we have a negative three we have a two we have a one again that's implied to be one and then we have a one okay so let me put my vertical bar here to separate the coefficients from the constants and once again i just want to stress this this is already written in the format of ax plus b y plus cz equals d if it's not already written in that format you've got to do that first okay all right so let's set this up and let's copy this all right so let's paste that in so again i want this top left entry to be a 1. so the easiest way to do that is just to multiply this first row here by the reciprocal of this number so again i can label this as row 1 row 2 and row 3. you don't have to do this but it's just nice to kind of show what's going on so i'm going to multiply negative 1 6 which is the reciprocal of negative 6 by row 1. that's going to give me my new row 1. okay so if i do that negative 1 6 times negative 6 is going to be positive 1. negative 1 6 times 1 is going to be negative 1 6. then negative 1 6 times 7 is negative 7 6 and then negative 1 6 times negative 32 let's do that off to the side so we would have negative 1 6 times negative 32 we know that negative times negative is positive so we get rid of that sign there so we'll just erase these we don't need them and then 32 and 6 are each divisible by 2 right so 32 divided by 2 is 16 6 divided by 2 is 3. so this is going to be 16 thirds all right so the next thing i want to do is i want to get zeros below this one so i want this to be a zero and i want this to be a zero so what can i do again i just think about the fact that i have a 1 here already okay i have a 1 there the reason you get the 1 first is so that you can work with that so all i need to think about is the additive inverse of negative 1 which is positive 1 okay so i would just multiply row one by one or you could just think of it as i'm just going to add row one to row two okay so what i'm going to say is i'm going to multiply row one by one i'm going to add the result to row two that's going to give me my new row two okay so all i'm going to do is just add so 1 plus negative 1 is 0. negative 1 6 plus 1 is what i'm going to write this as 6 over 6 and 6 minus 1 would be 5. so this would be 5 6 then for this one you would have negative 7 6 plus 1. i'm going to write as 6 over 6. negative 7 plus 6 is going to be negative 1 so this would be negative 1 6 and then lastly you would have 16 thirds plus negative 2 so 16 thirds plus you'd have negative i'll go ahead and say this is 6 over 3 so 16 minus 6 is 10 so we can say this is 10 thirds okay so that's taken care of so to get a 0 here again i use the same concept i'm going to use that 1 there and again the opposite of negative 3 is 3 so i'm going to multiply row 1 by 3 add the result to row 3. so i'm going to multiply row 1 by 3 add the result to row 3. that's going to give me my new row 3. so 1 times 3 is 3. negative 1 over 6 negative 1 over 6 times 3 is 1. this will cancel with this and give me a 2. so this is negative one-half this is negative one-half and then negative 7 6 negative 7 6 times 3 is 1. this cancels with this and gives me a 2 so this is negative 7 halves and then you have 16 thirds times 3 the 3s are going to cancel so this will cancel with this and i have 16. all right so let's add now 3 plus negative 3 is 0. so let me erase this we'll have negative 1 half plus 2 so negative 1 half plus 2 so let me do plus 4 over 2. so this would be what four minus one is three so this would be three halves so three halves okay so this is gone and then negative seven halves plus one let's add two over two negative seven plus two is negative 5 so this would be negative 5 halves and then lastly you would have 16 plus 1 which is 17. okay so this is 17. all right so now we have the first column done and we're going to move into the second column here and again i always want to start by getting a 1. i get the 1 and then i can get the 0 above and below pretty easily so to get the 1 i just multiply row 2 by the reciprocal of this 5 6 which is six fifths okay so six fifths times row two that's what i'm going to replace row two with all right so we know that this guy to the left isn't going to be affected because zero times anything is just zero okay but this guy is going to be a one so i don't really need to worry about that i really just need to do the calculations for these two so i would have six-fifths times negative 1 6 okay so this would cancel with this and give me a negative one-fifth so this would be negative one-fifth let me erase that and then lastly you would have your six fifths times ten thirds we know that this would cancel with this and give me a two and this would cancel with this and give me a two two times two is four so this would be four so now we wanna get a 0 above and below this 1. so again i'm thinking about the additive inverse of negative 1 6 so that's going to be 1 6. so let me multiply row 2 let me multiply row 2 by 1 6. so 1 6 times row 2 then plus row 1. this is going to be my new row 1. okay so again to the left of this it's not going to matter because 0 times anything is 0 0 plus anything is just itself so i really need to just think about i know that this would turn into a 0. so i really just need to work on this negative 1 5 times 1 6 okay and then i'll add this result to this negative 7 6. so i know that this would be negative 1 over 30 so negative 1 over 30 and then plus negative 7 over 6. so i'll multiply this by 5 over 5. negative 7 times 5 is negative 35 so you would have negative 35 over 30 and negative 1 plus negative 35 is negative 36 so let's say this is negative 36 over 30 which is what each is divisible by 6 this would be negative 6 over 5. so i'll replace this with negative six fifths and one more okay we need to do one more so let me write that a little bit better now i also need to multiply four by one sixth so four times one sixth this would cancel with this this would be a three this would be a two so this would be two thirds okay so two thirds and then add that result to 16 thirds okay so 16 plus two is 18 18 divided by three is going to give you six and let's get this guy to be a 0 now so i'm going to multiply row 2 i'm going to multiply row 2 by what again i want the additive inverse of this so negative 3 halves then plus my row 3 that's going to give me my new row 3. i know that i don't need to really do anything this way because i'll be multiplying this by a zero and that's going to be zero and then adding it to this would leave it unchanged so really i just need to change this into a zero because i know it would be zero and i need to work on this one and this one so i would multiply negative three halves times negative one fifth and that would give us three tenths okay and then i would add three tenths to negative five halves so three tenths three tenths plus negative five halves multiply this by five over five so this would be negative 25 negative 25 over 10 okay over 10. so negative 25 plus 3 is negative 22 so you would have negative 22 over 10. so this is negative 22 over 10. and then lastly i'm going to multiply negative 3 halves times this 4 okay that's going to cancel give me 2 negative 3 times 2 is negative 6 so negative 6 and then add the result to 17 that's going to be 11. what we want to do now is get a 1 as this entry here okay so how do we do that we multiply row 3 by the reciprocal of that guy so i'd have negative 10 over 22 times row 3. again that's going to give me my new row 3. so we know that this would be a 1. we know that this would be a 1. and we can just eyeball this and say that i know that 11 would cancel with 22 right 22 divided by 11 is 2. so let's just erase this this would become a 2 and negative 10 over 2 would be negative 5. okay so at this point you have row echelon form you have ones down the diagonal okay let me highlight that so one's down the diagonal you have zeros below okay so that's row echelon form this is what we get from gaussian elimination we're going to continue with the gauss-jordan elimination and put this in reduced row echelon form so i want a zero here and here again it's not that hard because we're we're almost there okay i know this is very tedious and that's just because we're taking our time so for this guy right here this negative one-fifth i can make it into a zero by multiplying row three okay row three by one fifth add the result to row two so one fifth times row three plus row two would give me my new row two okay so i know i don't have to work this way right i don't have to worry about that because if i'm multiplying a 0 okay in each case here by this number i get 0 and if i add 0 to something it stays unchanged so i really don't even need to work on this one because i know it's going to be 0. i just have to work on this one okay so that makes it kind of quicker so it's one-fifth one-fifth times this guy right here which is negative five that becomes negative one okay and that gets added to four so that is three okay so now the last one we're going to multiply row three by six fifths okay so by six fifths and we're gonna add the result to row one and that's gonna give us our new row one okay now again when i think about this i don't need to do anything this way okay i know i'm going to replace this with a zero okay and i just need to work on this one so six fifths times negative five six fifths times negative five this would cancel give me negative one so it's negative 6. so negative 6 plus 6 would give me 0. so now i have my solution finally okay so i can say that 1z equals negative 5 or z equals negative 5. i can say that 1y equals 3 or y equals 3 and i can say that 1x equals 0 or x equals 0. all right so let's continue with the next example so now we have negative 4x plus 7y minus 3z equals 33. so negative 4 we have 7 we have negative 3 and we have 33 then we have negative 7x plus 6y minus 5z equals 19. so negative 7 6 negative 5 and then 19. and then we have negative 5x plus 2y minus 4z equals 2. so negative 5 2 negative 4 and 2. okay so let me put this bar here and then let me put these brackets and we're ready to go so let me copy this let's go down here to a nice fresh sheet and again we want one down the diagonal we want zeros above and below okay so i'm going to start by getting a 1 in this position here and to do that i'm just going to multiply the top row or row 1. let me label these real quick so row 1 row 2 and row 3. i'm just going to multiply row 1 by the reciprocal of negative 4 so i'm going to do negative 1 4 times row one okay that's what i'm going to replace row one with okay so negative one fourth times negative four is going to be one then negative one fourth times seven is going to be negative seven fourths so negative seven fourths then negative one fourth times negative three is going to be positive three fourths and then negative one fourth times 33 is going to be negative 33 fourths so negative 33 4. okay so that part's done so now i want to get a 0 here and here so let me start with row 2 and getting a 0 here so again because i have a 1 here it makes it easy to get a 0 here because i can just say okay i want to add a 7 to this negative 7 to get a 0 so i can multiply this top row or row 1 by 7. so 7 times row 1 and then i can add that result to row 2 that's going to give me my new row 2 and this is going to be a 0. okay so this would be 0 and then for this guy right here i would have 7 times negative 7 4 so 7 times negative 7 4 and then i would add that to 6. so when that's done i'm going to add that to 6. so this would be negative 49 fourths and then if i add that to 6 i would make this 24 4. so this would end up giving me negative 25 4. so i'm going to put negative 25 4. okay so now i want to do 3 4 times 7 and add the result to this negative 5. so this would end up being what it would be 21 4. so 21 4 plus negative 5 so i could make this negative 20 over four and 21 plus negative 20 would be one so this would be one-fourth okay so i'll erase this and put a one-fourth here so let's erase this and then lastly i have this negative 33 4 times 7 and then plus 19 when that's done so this would end up giving me negative 231 4. so negative 231 fourths and then i'm adding to this 19. now what i can do to get a common denominator 19 times 4 is 76 so i can say this is 76 4. so negative 231 plus 76 is going to be negative 155 and this is over four okay all right so that part's done now we're going to do something similar for this right here so i would multiply 5 by row 1 again think about that 1. five times one is five add that to negative five you get zero and i would add the result to row three okay so that's gonna give me my new row three so if i do this one would be zero then i would take negative 7 4 and multiply it by 5 and add the result to 2. so this is going to end up giving me negative 35 4 and then plus instead of 2 i'm going to write 8 4. so negative 35 plus 8 is negative 27 so this is going to be negative 27 4. that's going to go right here so negative 27 4. okay the next thing i want to do is take three fourths so three-fourths and multiply it by five and then add the result to negative four so this is going to end up being 15 fourths okay and if i add this to negative four let's write this as negative 16 over four so 15 plus negative 16 is negative one so this will end up being negative negative one fourth okay so negative one fourth and then lastly what i want to do is multiply five by negative 33 fourths so negative 33 fourths times five that's going to give me negative 165. so negative 165 over four and then plus two again i'm going to write that as eight over 4. so negative 165 plus 8 is negative 157 so this is negative 157 over 4. and let's move on now so the next thing i want to do is get a 1 here okay i always want to get a 1 first and then get my 0 below and above in this case so i can do that by just multiplying row 2 by the reciprocal of that so i would have negative 4 25 times my row 2 that's going to give me my new row 2. okay so this is not going to be affected this will turn into a 1 okay this will be a 1. and then this guy right here you'd have 1 4 times negative 4 25 so what's going to happen is this will cancel with this i'll have a negative 1 over 25 okay and then let me erase this the last thing i want to do is i want to take negative 4 over 25 and multiply it by negative 155 over four so this is going to cancel and you basically have this negative times negative which would give you a positive between 155 and 25 you have a common factor of so this would be 5 here and this would be 31 here okay so this right here is going to be 31 fifths so let's erase this so now we're done with that and now i want a 0 here and here so to get a 0 here on the bottom i would multiply row 2 by 27 4 okay so 27 4 times row 2 and then plus row 3 that's going to give me my new row 3. so again i don't need to worry about this stuff over here for this one i know this is going to end up being a 0. so i'm really going to only have to work with kind of this part right here and this part right here so i would have 27 4 times negative 1 over 25 i'm going to add that to negative 1 4. so what's going to happen is between 27 and 25 there's no common factors other than 1. so really i'm just going to say this is negative 27 over 100. so negative 27 over a hundred for this i could multiply by 25 over 25 so i'd have negative 25 over 100 and that's going to give me what this would be negative 52 over 100. okay so let me erase this and we can simplify this before we write it 52 divided by 4 is 13 and 100 divided by 4 is 25 okay so i'm going to write this as negative 13 25 and then lastly i want to do 27 4 times 31 fifths and then add that to negative 157 fourths so 27 times 31 is 837 so this would be 837 over 20. so 837 over 20. if i multiply this by 5 over 5 we get negative 785 over 20. so if i do this addition 837 plus negative 785 i'm going to get 52. okay so i would get 52 over 20. 52 over 20 which is going to simplify each is divisible by 4. 52 divided by 4 again is 13. 20 divided by 4 is 5. so this guy right here is going to be 13 fifths okay all right so let's work on this in getting a 0 here so again i want to multiply row 2 by 7 4 now and add the result to row 1 okay that's going to be my new row 1. and so i already know that this is going to be a 0 okay then you would have 7 4 times negative 1 over 25 add that to 3 4. just scroll down a little bit i'll come back up so this right here would be negative 7 over 100 plus if i multiply this by 25 over 25 i would have 75 over 100 negative 7 plus 75 is going to be 68 so this would be 68 over 100 which would simplify 68 divided by 4 is 17 so this would be 17 over 25 right because 100 divided by 4 is 25. so this would be 17 over 25. so 17 over 25 and then for this one right here again we're going to multiply 7 4 times 31 fifths and then plus negative 33 4. okay so 7 times 31 is 217 so you'd have 217 over 20. so 217 over 20 and i'll multiply this by 5 over 5. so this would be negative 165 over 20. so 217 minus 165 is going to give me 52. so i'd end up with 52 over 20. so 52 over 20 and we already know this is going to simplify to 13 over 5 right because 52 divided by 4 is 13 and 20 divided by 4 is 5 okay so i'm going to replace this with 13 fifths so now i want to move on to this column and i want to get a 1 here okay so how do i do that i multiply row 3 by the reciprocal of this so i would take 25 over 13 the negative of that multiply it by row three that's my new row three again this one and this one are not going to be affected this guy is going to be a one and then this guy right here i'd have negative 25 over 13. times 13 over 5 the 13s are going to cancel negative 25 over 5 is negative 5. so this is negative 5 and at this point again you could stop you have row echelon form okay so you could go back and substitute you know z is negative 5 at this point you could find out x and y but we're going to continue we're going to get a zero here and here okay so what i want to do to make this into a zero i want to use this one here the additive inverse would be 1 over 25 so times this row 3 and then plus row 2. okay so i know that i don't need to worry about this or this because multiplying by 0 would leave a 0 and then adding 0 to that would leave it unchanged this guy i know is going to be a zero so i really only need to work with 1 over 25 times negative 5 and then plus 31 fifths okay so i know that this would cancel with this and give me a five here so this would be negative one-fifth so this would be negative one-fifth okay so plus thirty-one fifths would be thirty over five which is six so this is 6. so we know z is negative 5 and we know y is 6. let me just write that as we're doing this so we know z is negative 5 we know y is 6. okay now let's find out x so we're going to do is get a 0 here so again i'm going to use this row 3 with this 1 in it and i'm going to multiply row 3 by the additive inverse of this so negative 17 over 25 so then plus row one that's going to give me my new row one okay so i know that this guy and this guy are not going to be affected okay this guy's going to be a zero all i need to do is figure out this guy so it is negative 5 times negative 17 over 25 and then plus 13 over 5. okay let's find out so this guy would cancel with this guy and give me a 5 negative negative is positive so let's just get rid of that and say this is 17 over 5 plus 13 over 5 that's 30 over 5 which is 6. okay so this is 6 so x is 6 y is 6 and z is negative 5. in this lesson we want to finish talking about gaussian elimination and also the gauss-jordan elimination all right so over the course of the last two lessons we learned about the gaussian elimination and the gauss-jordan elimination and we saw that we could use these two methods to kind of solve a linear system we looked at two variables and then also one with three variables and so now what we're going to do is just take the next step and look at a linear system with four variables this is very tedious i'll just tell you that before we start but i'm going to do some things that just kind of speed us up so i'm going to just take this guy real quick and set up our matrix so we can just get going right away so we know we want to take the numerical information from each equation here and you might notice that we've labeled our variables a little bit different in your book if you're taking precalculus or college algebra you might see x y z and then w okay so w will be first because it's in alphabetical order but i didn't do that because a lot of times you'll also see this with the notation of x sub 1 through x sub 4. so whatever you're comfortable with is fine if you don't like this you can replace x sub 1 with w x sub 2 with x x sub 3 with y and x sub 4 with z okay so i'm just going to follow the format of x of 1 coming first then x sub 2 then x sub 3 then x sub 4. okay that's what i'm going to do all right so we're going to start out with this first equation we'll have a 3 a negative 2 a 5 and then a negative 1 and a negative 8. then in the second equation i'm going to have a negative 1 a 3 a negative 1 a 4 and then a 9. then in the third equation i have a negative 2 a negative 1 a 4 a 9 and a negative 9 i make that a little better and let me scroll down a little bit then in this last equation i want you to notice that you don't have an x sub 2. whenever you're missing a variable you want to write a 0 as the coefficient for that variable to act as a placeholder so i'm going to put a 1 and then again for my x sub 2 i'm putting a 0 as the coefficient then for x sub 3 i put a 3 then my 2 then my negative 2. okay so this guy is now set up for us and we are ready to go so let me copy this real quick now the quickest way to solve this if you only know kind of gaussian elimination or gauss-jordan elimination if you just get this and this is where you are in your chapter i would suggest just doing the gaussian elimination putting this matrix in row echelon form meaning i'm just going to get ones down the diagonal and zeros below it's going to be really quick to do that and then kind of back substitute to get your answer if you go through to reduce row echelon form again that's from the gauss-jordan elimination it does take a little bit longer that is what we're going to do today just to get a lot of practice on these elementary row operations because i feel like you do need it it's just something that comes up it goes away and then later on if you have to go back to it it's something if you didn't practice enough you kind of forget it and you might struggle with it again all right so the very first thing i want to do is i want to get a 1 as this top position here and then i want to work below and get zeros okay every time i go to the right to a new column i want to get my one first and then my zeros so what i'm going to do i could multiply this first row which i'd label as row one okay let me label all these real quick i could multiply row one by one third but there's an easier solution remember for the elementary row operations i can swap two rows so i can say row one is going to swap with row 4 because i already have a 1 there and if i swap those rows this guy would come up here okay so let me just write this real quick i have 1 0 3 2 and negative 2. i'll just erase this from here i'll just copy this here 3 negative 2 5 negative 1 and negative 8. and then i'll just copy this right here so 1 0 3 2 and negative 2. okay so now that i have my 1 in this position here these ones going down are pretty easy to get and i'm gonna do multiple ones at once okay we're ready to kind of speed up this process so we know that if we want this to be a zero i've got to add the opposite of that number to it okay so in this case i've gotta add a one in this case i've gotta add a two and in this case i've gotta add a negative three well this one is really convenient because whatever i multiply by one it's just itself so what i'm gonna do in each case is i'm gonna multiply row one by whatever i need to add to this to make it a zero and then i'm going to add it to that row and then that's what i'm going to replace the row with okay so for the first one i'm going to say that i'm going to have 1 times row 1 plus row 2 this is what i'm going to replace row 2 with okay and when you multiply something by 1 it's just itself so really you could just say row 1 plus row 2 if you want then for the next one i'm going to end up saying that i'm going to have 2 times row 1 okay so the opposite of negative 2 is 2. so 2 times row 1 plus row 3 that's what i'm going to replace row 3 with and then for this last one here i'm going to have negative 3 okay because negative 3 plus 3 is 0. so negative 3 times row 1 plus my row 4 is what i'm going to replace row 4 with okay so let's go through and crank this out real quick so i know for this one i'm just adding row one and row two that's what i'm replacing row two with so one plus negative one is zero zero plus three is three three plus negative one is two two plus four is six and negative 2 plus 9 is 7. so this one is done then for this one 2 times row 1 plus row 3 that's what i'm going to replace row 3 with so 2 times 1 is 2 2 plus negative 2 is 0. 2 times 0 is 0 0 plus negative 1 is negative 1. then 2 times 3 is 6 6 plus 4 is 10. 2 times 2 is 4 4 plus 9 is 13. 2 times negative 2 is negative 4 negative 4 plus negative 9 is negative 13. all right so this one is done so let's do this one so we have negative 3 times row 1 plus row 4. negative 3 times 1 is negative 3 plus 3 is 0. we know that this would be still negative two negative three times three is negative nine negative nine plus five is negative four and then negative three times two is negative six negative six plus negative one is negative seven then lastly we have negative three times negative 2 which is going to be positive 6 and then positive 6 plus negative 8 is negative 2. all right so that part's done i have a 1 here and 0's below now i want to move to my next column and i want to get a 1 here okay i already have a zero above so i'm going to get that one first and then i'm going to get the zeros below okay so the easiest way to get a 1 in this position here i can multiply any row by a non-zero real number so i'm just going to multiply this whole row row 2 by the reciprocal of 3 because 3 times a third would give me 1 okay so i'm going to multiply 1 3 times row 2 that's what i'm going to replace row 2 with so this would be 1 this would be 2 two-thirds this would be six-thirds which is two and this would be seven-thirds okay so that's pretty quick and now i want to get a zero here and i want to get a zero here so i know the additive inverse of negative one is one so again i could multiply 1 times row 2 and add that to row 3. that's what i could replace row 3 with if i want this to be a 0. and again if i'm multiplying by 1 it's just itself so you can just erase that then for this one the additive inverse of negative 2 is 2. so i want to multiply 2 times row 2 and then add that to row 4 that's going to give me my new row 4. okay so for this guy right here i'm just going to add row 2 and row 3 and that's going to replace row 3. so 1 plus negative 1 is 0 then you would have 2 3 plus 10. well i know i could write this as over i could multiply this by 3 over 3 so this would be 30 thirds and 30 plus 2 is 32 so this would be 32 thirds so this would be 32 thirds then i'd have 2 plus 13 which is 15. and then lastly i would have seven thirds plus negative thirteen multiply this by three over three we get 39 there over three so negative 39 thirds so negative 39 plus 7 would be negative 32 so this would be negative 32 thirds okay so let's erase this and this this one's done so now let me work on this row here again i want that to be a zero so i'm going to multiply 2 times row 2 add the result to row 4 that's what i'm going to replace row 4 with so we know this is going to be a zero two thirds times two is four thirds then plus negative four multiply this by three over three you get negative 12 thirds 4 plus negative 12 is negative 8 so this is going to be negative 8 thirds okay then the next one is going to be 2 times 2 which is 4 4 plus negative 7 is negative 3. then the last one is 2 times 7 thirds which is 14 thirds then plus negative 2 which i'll write as negative 6 over 3. so 14 minus 6 is going to be 8 so this would be 8 thirds so i'll put this as 8 3. okay so let me erase this so far so good so now i want a one here to get started okay these two columns to the left are done i want this to be a one so you already know the deal i'm going to multiply row three by three over 32. and this gets easier as we move on because i don't have to worry about this stuff over here this is zero and this is zero so multiplying that by the reciprocal doesn't do anything right it's still zero this is going to be one we already know that so really the only calculation i have to do is 15 times 3 over 32 which is going to be 45 over 32 and then you would have your negative 32 over 3 times 3 over 32 this cancels and this cancels you get negative 1. so this is negative 1 here okay so let's erase this okay so let's think about getting a 0 here here and here okay so three different things we want to do again in each case because i've got a one there think about what the additive inverse is of what you're trying to make into a zero multiply that by that row with the one in it and then add that to the current row that you're trying to make that into a 0. i know i said a lot there but basically if you think about this for row 1 i'm trying to make this 3 into a 0. so i'm going to multiply row 3 by negative 3 the additive inverse of that add the result to row one okay so that's going to take care of that that's what i'm going to replace row 1 with same concept as we move on so for row 2 i'm going to multiply negative two-thirds which is the additive inverse of two-thirds times row three okay that one and row three plus row two that's what i'm going to replace row two with and then lastly for row four i'm gonna take positive eight thirds multiply it by row three add the result to row 4 that's what i'm going to replace row 4 with all right so let's do these one at a time so let's start with this kind of row 1 here so i want negative 3 times this row 3 okay i'm going to add the result to row 1. now because these two guys here are zeros i don't need to worry about it zero times anything is zero adding zero to something doesn't change it i know this would be zero i don't even need to worry about that i just have to do the calculation for these two so i would start here with this kind of 45 over 32 and i would multiply it by this negative 3 okay by this negative 3 and i would add to this this value of 2. okay so that's one of them that i have to do then the other one would be negative 1 so negative 1 times negative 3 and then i'm going to add to that negative 2. so let's kind of slide down here and do this and we'll come back up so 45 times negative 3 is negative 135 and this will be over 32 then plus for 2 to get a common denominator i would write it as 64 over 32. so if i sum these negative 135 plus 64 is going to be negative 71. so this would be negative 71 over 32 okay so for this one negative 1 times negative 3 is 3 right positive 3 and positive 3 plus negative 2 is positive 1. so i'm going to have negative 71 over 32 and i'm going to have positive 1. so let's erase this so we'll put this as negative 71 over 32 let me make that better so negative 71 over 32 and then again this was positive 1. okay so now i can erase this this part's done let me kind of slide this up so now let's work on this one so i know that this right here again i'm multiplying negative two thirds times row three so this and this are not affected this is going to be a zero this right here i would have negative two-thirds times so negative two-thirds times 45 over 32 and then the result of that would be added to two so let's figure out what this is so this cancels with this and gives me a 16 down here this cancels with this and gives me a 15 here so this is negative 15 over 16. so negative 15 over 16. to get a common denominator going i'm going to say this is 32 over 16 and then 32 minus 15 is 17 so this would be 17 over 16. so that's what i'm going to put right here it's going to be again 17 over 16. all right and then one more to do so i'm going to multiply negative two-thirds times negative one so negative two-thirds times negative one which would just be two-thirds then i'm gonna add the result to this seven-thirds here okay so two plus seven is nine so this would be nine over three which is three so i'm going to replace this with a three okay so let's erase this one and now we're just going to work on this one so i've got eight thirds times row three plus row four so if i go through again this one and this one it's not going to change these this guy is going to turn into a zero so what i want to do is eight thirds times 45 over 32 so this cancels with this and gives me a 4 this cancels with this and gives me a 15. so you have 15 4 plus negative 3. so 15 fourths you can go ahead and just say minus 3. 3 is 12 over four 15 minus 12 is three so this is just three fourths three fourths all right then we wanna do eight thirds times negative one which is just negative eight thirds and then we wanna go plus eight thirds which is obviously going to give us 0 so this will be 0 here all right so not much left so we want to get a 1 here now and to do that i can just multiply row 4 by 4 thirds obviously you can see that x sub 4 is going to be zero right because when i multiply this by four thirds this by four thirds this by four thirds and this by four thirds it's all going to stay zero this is going to change into a one right so four thirds times row 4 is what i'm going to replace row 4 with so this will just be 1. okay all right now i'll want a 0 here here and here so to get the 0 here i'm going to multiply row 4 by negative 45 over 32 add the result to row 3. again that's what i'm going to replace row 3 with and then i'm going to multiply negative 17 over 16 times row 4 add the result to row 2 that's what i'm going to replace row 2 with and then lastly i'm going to multiply 71 over 32 times row 4 add the result to row 1 that's what i'm going to replace row 1 with now before i do anything you might want to notice that this is a 0 this is a 0 this is a 0 and this is a 0. so in every case whatever this number is it doesn't matter when it multiplies these guys by zero and i add it to whatever row it is it doesn't change so none of these entries are going to change anywhere except for here here and here and in every case i'm adding exactly what i need to make them into a zero so i can just go ahead and put zero zero zero and say we have our solution so remember it's the coefficients for x sub one then x sub two then x sub three and x sub four so for x sub one it's going to be one for x sub 2 it's going to be 3. for x sub 3 it's going to be negative 1 and for x sub 4 it's going to be 0. so again in this particular case we just so happen to get x sub 1 through x sub 4 but if you had w x y and z remember the left most column would be w then x then y then z it goes in alphabetical order with subscripts you go 1 2 3 and 4 you go in numerical order in this lesson we want to talk about finding the determinant of an n by n matrix so when we work with a square matrix or again an n by n matrix a matrix with the same number of rows as columns we're going to have a real number that's associated with that matrix that's known as its determinant so this is a very special number for us we use it all the time when we're working with matrices and we already saw that when we work with the inverse of a two by two matrix there's a little shortcut that we can use that saves us a lot of time and it involves taking the determinant of the given two by two matrix in order to kind of find the inverse okay so that's part of the formula so what we're going to do today is just go very deep into this topic and we're going to show you how to find the determinant of any n by n matrix i'll show you the shortcut that you can use for a 3 by 3 matrix and after this in our next video i'll show you the shortcut that you can use for the 4x4 matrix okay it would be too much to put in one video so we're going to kind of break it up all right so we're going to start out with the 2x2 matrix which is the easiest scenario so let's say we start out with this matrix a and it's equal to so in the first row you have this lowercase a and this lowercase b as your entries in the second row you have the lowercase c and the lowercase d as your entries basically if i want to ask for the determinant of this matrix a i can put these vertical bars around a okay that's one way you can ask for the determinant or you could also say d e t of a like this so depending on which book you're using you'll see different notation just follow whatever your book is doing or whatever you're doing in your class okay but finding the determinant for a two by two matrix is very simple you see i already have the formula written it's a times d minus b times c so essentially you can just think about multiplying down like this and then you can multiply up like this or you can multiply down like this it doesn't really matter most books will go like this okay but realize the fact that c times b and b times c those are the same right because we're working with real numbers so multiplication is commutative so it doesn't really matter how you list that if you want to say it's a times d minus c times b or if you want to say it's a times d minus b times c it's the same result in the end so let's look at an example real quick so we have a is equal to we have 2 and negative 1 in the first row and 5 and 6 in the second row so the determinant of a okay would be equal to and another way you might see this you could put vertical bars around the entries so 2 negative 1 5 and 6. and this is how you would ask for the determinant of something if you didn't have a name for the matrix okay so that's one way you would see that so the determinant of this is what again i'm just going to multiply down so what is 2 times 6 that is 12 and i'm going to subtract away again you can multiply down or up it doesn't really matter i'm just going to go ahead and multiply down so negative 1 times 5 is negative 5. again notice if you did 5 times negative 1 it's also negative 5. right so it doesn't matter 12 minus a negative 5 is 12 plus 5 so 12 plus 5 and that's 17. so the determinant of a is 17. all right so let's look at one more of these and then we'll move on to a 3x3 so for a it's equal to we have 3 and 8 in the first row and 7 and negative 4 in the second row so the determinant of a is equal to again multiply down 3 times negative 4 is negative 12 then minus i'm going to just multiply up this time so 7 times 8 is so what is negative 12 minus 56 that's going to be negative 68 okay so that's my determinant of a all right so obviously that's a very easy process when you get into a 3x3 or higher if you use the kind of book method it can be a little bit tedious but it's really not that bad it's definitely not as bad as finding the inverse but luckily for us again we have a shortcut for 3x3 which is really easy to use it takes under a minute to calculate the determinant of a 3x3 matrix so it's a really good shortcut and again in the next lesson i'll talk about the shortcut for a 4x4 all right so first and foremost before we get into this kind of official method we got to talk about something known as a minor and a cofactor okay so these are terms that you're going to use even outside of talking about finding the determinant so it's important to understand what they are so i'm going to highlight this word right here so if a is a square matrix the minor which is notated with this capital m sub i j remember this sub i j this is the ith row j column that's just generic notation to say i'm at a given position in the matrix so then of the entry you have this lowercase a sub i j again that just tells me i'm at some given entry in this matrix a what we're saying is the determinant of the matrix obtained by deleting the ith row and jth column of a okay so remember this generic notation sub i j here just tells you you're at a given position some given row and column so all this is telling you to do if you want to find the minor of some given entry delete the row it's in delete the column it's in and find the determinant of the matrix that is kind of left after you've done that so to see this with a generic example this is something you probably want to write down because it's very important so if you have capital letter a to name this matrix and it's equal to you've got all lower case letters here as entries a b c d e f and then g h i if i wanted to find the minor of this kind of entry here this lowercase a what i would do is delete this column and i would delete this row okay so the matrix that's left is made up of these elements you have e you have f you have h and you have i okay so if i wanted to find the determinant of that again i can put these vertical bars around it okay and then i can just say it's equal to what e times i you multiply down so e times i minus you'd have f times h so f times h so this right here is the minor okay the minor of this a so i can say m capital m sub 1 1 okay some people put a comma between them and say it's row 1 column 1 is going to be equal to this right here this e times i and then minus f times h and i know this generic kind of stuff will trip some students up but i just want to give you a generic example first and then we'll get into some ones with some numbers involved so let me just do one more here okay let me do the minor of this element here so this lowercase b so what's the minor again i'm going to delete this column in this row what's left it's this d this g this f and this i okay so i'm just going to put vertical bars around this to say i want the determinant that's d times i d times i minus f times g so f times g so that's my determinant so the minor capital letter m sub again i'm in row one but now i'm in column two so you can put a comma there if you want or you can leave it as one two because it's clear this is equal to this so it's d times i minus f times g okay so very generic and now what i want to do is just talk about a cofactor real quick and then we can move on and we can talk about some examples with some numbers and i'll show you how to use this to calculate a determinant so what you're going to see in your book is that the cofactor now you have a capital c sub ij of the entry again this is just some generic entry in our matrix a so lowercase a sub i j is this negative 1 raised to this power so i plus j remember i is the row okay generically and j is the column okay so i'm just going to put col to abbreviate column so negative 1 to the power of i plus j and then it's multiplied by the minor remember this is the capital letter m sub i j okay so what you need to understand here if i go back to this example here let me just kind of erase this real quick and we already have our minor so we have our minor for what this is the minor for again this element here which is in row one column two and let me slide this down i wanna make this crystal clear okay this is row one this is row two this is row three this is column one column two column three so i'm in row one column two okay so that's what that one comma two tells me so if i find the sum of those two numbers one plus two that's three so the cofactor capital c sub one two is equal to negative 1 raised to the power of this 1 plus 2 the row number plus the column number i'm in row 1 column 2 1 plus 2 is 3. so it's raised to the power of 3 okay so we know that if we raise this to an odd power it's going to be negative 1. so it's going to change the sign of this guy right here this d times i minus f times g okay so if i get a result okay for the exponent that's odd it's going to stay negative if i get a result that's even it's going to be positive okay so for example let me erase this real quick if i go back to the earlier example where i found the kind of minor of a this lower case a that m sub 1 1 well this was equal to what it was equal to we delete this and this it was e times i right minus f times h okay it's just the determinant of what's left so this guy right here so e times i minus f times h so it's this now the cofactor okay would be equal to what it's negative 1 raised to the power of what you're in row one column one so you can just look at these numbers down here one plus one is two so raised to the second power i know this is even so this is going to be positive one so it's just going to be the same as the minor in this case okay so to make this really easy you can make yourself a little matrix with the signs involved so that you don't get confused okay this is going to come up pretty often so when you talk about cofactors you have to have the signs down so again if you just look at this i know this is confusing at first but if you really think about it it kind of starts to make some sense so this is row 1 column 1 right here 1 plus 1 is 2 right so this is an even number then this is row 1 column 2. so 1 plus 2 is 3. that's an odd number so that's why this is negative and this is 1 3 right row 1 column 3 1 plus 3 is 4 so that's why this is positive these signs are going to alternate right if this is negative this is positive this is negative this is positive this negative is positive okay so they alternate going across they also alternate going up and down right positive negative positive negative positive negative positive negative positive so really if you just remember the first one is positive you can go through and make a little matrix of sines so that you can figure out what the cofactor is all right now let's look at an example real quick and this is going to lead to something so i'm going to ask you to find the cofactor okay for every number in the kind of first row up here okay so let's start by calculating the minor first of each one and then we'll think about the cofactor okay remember that's just thinking about negative 1 raised to the power of the row plus the column okay times the minor that's all it is so what is let's start with the minor what is the minor of this element here i'm going to delete this and i'm going to delete this the matrix that's left is a 2 and a 4 in the first row and a 0 and a 1 in the second row so again the vertical bar is telling me i want the determinant so 2 times 1 is 2 minus 4 times 0 is 0. so the minor here is going to be 2 minus 0 which is 2. okay so let me just write this down here real quick we're going to say that m sub 1 1 is equal to 2. so the next thing is what's the cofactor so what is c sub 1 1 that's equal to what again it's negative 1 raised to the power of in this case this is row 1 row 2 row 3 this is column 1 okay column 2 and column 3. you really don't need that because you can just sum these numbers down here okay but it's in row 1 column 1 1 plus 1 is 2 so again this is raised to an even power so we know it's going to be positive 1. so the sine of the minor is not going to change when it becomes the cofactor in this case so this is going to be equal to 2 as well okay and we don't need the minor we're just going to keep the cofactor so let's move this up okay and let's talk about the cofactor of this one so again i'm going to find the minor first so delete the column delete the row what's left i'm left with 7 6 4 and 1. so 7 times 1 is 7 minus 4 times 6 is 24. if i do 7 minus 24 i get negative 17 okay so that's going to be my minor my minor is negative 17. so m in this case i'm looking for the minor of this guy right here it's going to be in row one and column two okay so row one column two so this guy is going to be a negative 17 but if i want the cofactor of this so c sub 1 2 this is equal to what it's negative 1 raised to the power of again use these numbers down here it's row 1 column 2 row one column two so one plus two is three this is an odd number so this is gonna stay negative one times the minor okay which is negative seventeen so this becomes positive seventeen okay so this is positive seventeen all right so let's erase this and let's drag this up all right so the last thing we want to do is find the cofactor for this entry right here so again i'm going to delete this column and this row what's left i have 7 i have 6 i have 2 and i have 0. so what's the determinant of this guy well i'm going to have 7 times 0 which is 0 and then minus i'm going to have 2 times 6 which is 12. so it's going to be 0 minus 12 which is negative 12. so let's erase this so my minor again we're in row one and now we're in column three this is going to be a negative twelve so the cofactor c sub one three is what we know that we're in row one column three okay so one plus three is four negative one to an even power would be positive one so the sign of this is not going to change okay so let's drag this up and again you can always reference the sign chart that we have here so this one's positive negative positive okay and if we look back we see that's exactly what happened this one was positive this one was negative and this one was positive right so this sign didn't change this one did and this one didn't okay so now that we have these cofactors let's talk about the method to find the determinant of any n by n matrix this would work no matter what you're working with okay so if it's a three by three a four by four a two 200 by 200 whatever you're working with you can use this method so what you want to do is you want to multiply each element in any row or column so it doesn't matter which row you use doesn't matter which column you use but you're going to just use one of them okay so you're going to multiply each element in any row or column of the matrix by its kind of cofactor then the sum of these products would give us the value of the determinant so in this particular case i already have my cofactors for this whole row okay so what i'd want to do is i'd want to multiply 5 by its cofactor so 5 times 2 then plus negative 1 times its cofactor which is 17. let me change this notation up a little bit i want to be consistent so if i use a parenthesis there i want to use a parenthesis here just so you don't get confused then plus i'm going to have 3 times its cofactor which is negative 12. okay so if we go through and do this operation we're going to end up with our determinant 5 times 2 is 10 and then plus you'd have negative 1 times 17 which is negative 17 and then plus 3 times negative 12 is negative 36 okay so what is this going to give me 10 plus negative 17 is obviously negative 7 and then negative 7 plus negative 36 is going to give me negative 43. so this is my determinant of a okay so if i erase this i can say that my determinant of a is going to be negative 43. okay let me slide it down just a little bit just right here now before we go any further let me just show you the shortcut okay and we'll do some mixed practice we'll kind of practice it each way and i'll give you a generic formula that you can memorize in case your teacher tells you that you have to do it the long way okay so for the shortcut let me kind of move this out of the way for now for the shortcut all you need to do is copy the first two columns okay so column one and column two just copy them to the right of this so 5 7 and 6 and then negative 1 2 and 0. okay so now what i want to do is i want to multiply the numbers starting at this top left okay i want to multiply down this diagonal so i'm going to do 5 times 2 which is 10 times 1 which is still 10 okay so that's 10. then i'm going to keep going so i'm going to add to this i'm going to multiply these numbers going down this diagonal so negative 1 times 4 is negative 4 times 6 is negative 24. and then i'm gonna add to this i've got one more to do three times seven times zero we know that's zero so you can just erase this okay now you have to be very careful what you're going to do next put brackets around this so you don't make a silly sign mistake you're going to now subtract away the whole thing so put brackets you're going to go up now so starting at the bottom left you're going to go up so 6 times 2 is 12 times 3 is 36 and then plus you're going to go up again we know there's a zero involved in that multiplication so that would be zero right zero times four zero zero times five is zero and then here you're going to go up one last time so 1 times 7 is going to be 7 times negative 1 is going to be negative 7. okay all right so 10 plus negative 24 is negative 14 and then 36 plus negative 7 is going to be 29 okay so what you're going to have here is negative 14 minus 29 which is also negative 43 okay so either way you find that the determinant is negative 43. so you can see how much faster that method is it does take a few tries to kind of memorize it okay and we'll practice this much more throughout this lesson let's go ahead and look at another example so looking at this example i want to do another example where we talk about the cofactors again and then we'll practice the shortcut so let's go ahead and do this row again so i'm going to take this row right here so i'm going to start with this guy and i'm going to find its cofactor so i'm going to eliminate this and this and so i'm going to have the determinant of 0 8 3 and 7. what's that going to be 0 times 7 is zero minus eight times three is twenty-four so this is negative twenty-four so if i look at that one remember this guy is in row one column one so my sign there one plus one is two that's an even number sine won't change right because negative 1 raised to the second power is going to be positive 1. so i would say c sub 1 1 is equal to a negative 24 okay and then i'm going to find this one so i'm going to delete this and this so i want c sub i want again row 1 column 2 it's equal to the determinant of this guy you would have 6 8 negative 2 and 7. so 6 times 7 is 42 and then minus you have 8 times negative 2 which is going to be negative 16. this is plus a positive right because you have minus a negative so this is going to end up being 42 plus 16 which is 58 okay so this is 58. so i'm going to put 58 here but remember i've got to multiply this minor that i just calculated by negative 1 raised to the power of 1 plus two one plus two is three three is an odd number so this is going to be negative right negative one raised to the third power is negative one so this would be negative 58 there then lastly let's look at this guy right here so i'm going to delete this column and this row so c sub 1 3 is going to be equal to we would take this determinant so we have 6 0 negative 2 and 3. so 6 times 3 is 18 and then minus you'd have 0 times negative 2 which is obviously 0. so this is going to be 18 and again the sign isn't going to change here because if i look at my numbers down here i'm in row 1 column 3 1 plus 3 is 4. 4 is an even number right negative 1 to the power of 4 is positive 1. so this will stay as 18. so now that we have our cofactors we can multiply them by their given entries okay and then we can sum those amounts so negative 24 times negative 1 and then plus we're going to have negative 58 times 2 and then plus we're going to have 18 times 4. okay so what's that going to give us so this is positive 24. let's just erase that and put that negative 58 times 2 is negative 116 so negative 116 18 times 4 is going to be 72 okay so 24 plus 72 is 96 so you'd have 96 okay minus 116 and that would be negative 20. okay so we can say our determinant is going to be negative 20. all right so the determinant of a is negative one okay let's look at our shortcut again again it's a very good shortcut so copy these columns so negative 1 6 and negative 2 your first one and then copy your second one so 2 0 and 3. again i'm just going to multiply start at the top left that's easy to remember and go down so negative 1 times 0 times 7 we know that's 0. so i don't have to do anything there then 2 times 8 times negative 2 we know that 2 times 8 is 16 16 times negative 2 is negative 32 and then plus multiply down this last diagonal 4 times 6 is going to be 24 and 24 times 3 is 72 okay so let's sum this real quick so we don't make some silly sign mistake if i do 72 plus negative 32 that's going to give me 40. so i have 40 over here remember i'm subtracting away this guy when i go up so again you're subtracting the whole thing away so if you're not doing it all in one go make sure you use brackets okay i'm just going to put a bracket there so i don't make a silly sign mistake all right so let's go up now so i'm going to start here at the bottom left and i'm going to go up i've got a 0 involved so i know i can just skip that because 0 times anything is 0. then here going up 3 times 8 is 24 times negative 1 is negative 24 and then going up one last time 7 times 6 is 42 42 times 2 is 84. so plus 84. so what is negative 24 plus 84 that's going to give me positive 60 okay so what i have now is 40 minus 60 which is obviously negative 20. so either way you do this you get a determinant that is negative 20. okay so again a very very good shortcut overall something you definitely want to use okay so i want to do one more example but before i do it i want to return to my generic example so in case you're in a class that doesn't want you to use the shortcut i've seen teachers do this i want to give you a generic formula that is very easy to remember okay and this is based on what we've done so far and i'll explain it as i go so you want to take if you want the determinant of a you want to take this guy right here your a and multiply it by the determinant of this guy right here okay so we know the determinant of this guy right here would end up being what it's e times i minus f times h but this is probably a little easier to remember so i'm just going to leave it like this then you're going to put a minus here okay and i'll explain why in a second you're going to put a b there and then times the determinant of again what would be left if i marked this out it would be d it would be f g and i okay so that's that and then lastly you would put a plus here okay times your c and let me write this down here because i won't be able to fit that all the way then times your determinant of again what would be left your d your e your g and your h okay so this is a very easy way to remember this a lot of books will actually give you this as the shortcut and they will give you different notation based on what they're using as entries but essentially you just take this guy and multiply it by the determinant of this guy okay then minus this guy times the determinant of again if i mark this out this that's left and then plus you have this guy the c times the determinant of this guy okay so very easy to remember the reason this is a minus sign here is remember you're in row one column two one plus two is three so that's where your cofactor would end up taking the minor and making it negative okay but in this case here it's one and one one plus one is two that's even in this case it's one and three one plus three is four so it's even remember your signs alternate this one's plus this one's minus this is plus if you're talking about getting the sign for the co factor right taking the minor and applying that kind of sign change if it's needed all right so let's go back and i want to just do one more example again it's just it never hurts to do a lot of examples of these things just to make sure that you have everything covered all right so let's do it both ways okay so i'm not going to use the cofactor method i'm going to use the thing that i just told you that method so i'm going to start by taking this and multiplying it by the determinant of this okay so i'm going to say that the determinant of a is equal to negative 2 times the determinant of this so let me just write that out so 1 3 0 and 6. then minus okay minus you have this so 5 times the determinant of put the 0 and the 4 the 3 and the 6 okay and again that just comes from marking this out and this out that's what's left and then lastly you have plus this guy times the determinant of this so let me fit this down here negative 1 times the determinant you have 0 1 4 and 0. okay all right so let's calculate the determinant here 1 times 6 is 6 and then minus 3 times 0 3 times 0 is 0. so this would just be 6. so negative 2 times 6 is negative 12. so that's your first one then here 0 times 6 is 0 okay you'd have 0 minus 3 times 4 is 12 so this is negative 12 so you'd have negative 5 times negative 12 which is going to be positive 60. so plus 60. so plus 60 okay and erase this and then what do you have here you have negative one times zero times zero is zero then minus one times four is four so basically what you have is negative one times negative four which is plus four okay so when you use that kind of technique that i just showed you even if you don't use the shortcut it's not that bad okay if you just kind of memorize that it doesn't take that long to do it's just a few kind of practice problems away for you and you'll have that one down as well so negative 12 plus 60 is going to be 48 right positive 48 and then plus 4 would be 52. all right so i want to show you one more time the shortcut just so you have that down as well again it never hurts to practice so copy the first two columns so negative 2 0 and 4 and then 5 1 and 0. again multiply down and let me make this a little bit better because it's not going to line up so again 0 and 4 and then 1 and 0. so multiply down so start here and go down what is negative 2 times 1 that's negative 2 times 6 is going to be negative 12 okay and then plus you're going to multiply down 5 times 3 is 15 times 4 is 60. then plus we already know that's going to be a 0. so you don't even need to write that negative 12 plus 60 is going to be 48 okay then minus again i'm going to put a bracket here so i don't make a sign mistake it's so easy to make a sign mistake so i always recommend putting a bracket start at the bottom left and go up so 4 times 1 is 4 times negative 1 is negative 4 and then go up you have a 0 there so don't even worry about it and then go up you have a 0 there so don't worry about it so what is 48 minus a negative 4 that's 48 plus 4 which is 52 so again either way you do this you find that the determinant is 52. so again this is a very good shortcut and we're going to use it throughout our course in the upcoming lessons we're going to talk about cramer's rule okay and we're going to be using this shortcut to find our determinant very quickly and we'll see that we can finally solve a system of linear equations using these kind of matrix methods that really save us a lot of time in this lesson we want to talk about finding the determinant of a four by four matrix so in the last lesson we talked about the laplace expansion method which is also known as the cofactor expansion method for finding the determinant now using this method we know the determinant for any n by n matrix any square matrix can be found as the sum of the entries in any row or you could do a column multiplied by their respective cofactors okay so we saw that for kind of working with a 3x3 this process really wasn't that bad it's a little bit tedious but it's not that bad and then we also found a shortcut for the 3x3 that made the process very very simple and quite a bit less tedious well when we start working with a 4x4 or higher we might need to kind of look at an alternative strategy because we don't have such a shortcut for that so i want to introduce a strategy that you can use in some cases it's going to be a little bit better than the kind of laplace expansion method in other cases it won't be okay but at least you have another tool in your toolkit to kind of find determinants for something that's a four by four five by five a six by six if you had to do it by hand so the first thing we need to do today is introduce kind of a new term force in our matrix algebra section and this is known as a triangular matrix so first and foremost you have a lower triangular matrix which occurs when a square matrix has all entries above the main diagonal as zeros remember when we talk about these kind of diagonal entries the row number and the column number are going to be the same so row one column one that's that entry there row two column two that's that entry there row three column three that's that entry there okay so going down those kind of diagonal entries you could say this main diagonal if i look above that the entry should be all zeros okay which is what we have here for the lower triangular matrix or you can say this matrix is in lower triangular form now similarly when we look at the upper triangular matrix this is going to occur when all the entries below the main diagonal are zeros so again here's your main diagonal and all your entries below are zeros okay so the reason we're kind of showing you this is because there's a very special property when you work with a triangular matrix okay so this could be the lower triangular form or the upper triangular form basically your determinant can be found by finding the product of those diagonal entries okay so i can just go down the main diagonal and say that my determinant for this matrix u is equal to a times b times c okay that's all it's going to be similarly if i come up here to matrix l okay i can say that the determinant let me make that a little better okay the determinant for l is going to be again a times b times c so i think you see where we're going with this we're going to give you a 4 by four kind of matrix and then essentially you're going to use some row operations to put it into kind of this upper triangular matrix format okay where you have zeros below this kind of main diagonal and then you can find the determinant by just finding the product of those entries along the main diagonal now before we can kind of jump into this process we have to talk about some of these properties of determinants there's a few of these i'm just going to focus on three of them the ones that involve what happens when you perform row operations on a matrix so let's talk about the first one the first one is where we swap two rows or you can say interchange two rows or you could also say this happens when you interchange two columns okay so first and foremost what's the determinant of this two by two matrix we know that we start by multiplying down one times four is four then we subtract the weight you can multiply up or you can multiply down it doesn't matter 2 times 3 is 6. so this gives me negative 2 okay so what happens if we swap two rows here well what's going to happen is the sign of the determinant will change the absolute value would be the same but the sign will change so instead of negative 2 you'll have positive 2. so if i put 3 and 4 in the first row and 1 and 2 in the second row again the sine will change it will be positive 2. if i multiply down 3 times 2 is 6 and then minus if i multiply up 1 times 4 is 4 so this is positive 2. so it's just a sign change okay so that's all it is the second one that you need to understand if we multiply a row okay a row by some nonzero real number then essentially you can just multiply the determinant by that same non-zero real number so let me give you an example of that let's say that i multiply to row 1 here by positive 2. so instead of 1 i have 2 instead of 2 i have 4 okay and i'm just doing this to row 1. row 2 will be unchanged so that's going to be a 3 and a 4. so the effect it's going to have is is going to change this by a factor of 2. so i can just take negative 2 the old determinant multiply by 2 to get the new determinant so this guy will be equal to negative 4 and again you can prove this if i multiply down 2 times 4 is 8 and then minus if i multiply up 3 times 4 is 12. 8 minus 12 is negative 4. now this isn't going to come up today okay but you do need to know this for future kind of lessons especially if you get into a linear algebra course now let me show you the one that we're going to use the most today and this is very important we know that when we kind of perform row operations to kind of change an entry let's say we want this to be a zero for example we can multiply one of the rows by some non-zero real number and add the result to a given row okay we can replace the row with that and the way your book will normally say this is it will say if you replace a row with the sum of that row and a non-zero constant multiple of some other row when this occurs you're going to have no change to the determinant okay so that's very important to understand so let's say for example i went through when i multiplied the top row by two so two times row one okay who would that be so we would have two and then we would have four and then i added the result to row two and that's when i replaced row 2 with so let me kind of move this over because i'm going to run out of rum it's going to put this up here for now and i forgot my plus sign and so i know that multiplying 2 times row 1 would give me a 2 and it would give me a 4. i'm going to add these two results to this bottom row and the top row will be unchanged so let me just kind of scooch this over for a second so i'm going to put 1 and 2 those are unchanged i'm not doing anything 2 plus 3 is going to be 5 and 4 plus 4 is going to be 8 okay so everyone should understand what i did there this is something we did when we did our gaussian elimination our gauss jordan elimination again i'm just multiplying a given row which was row 1 by a nonzero real number which was 2 and then i added that result to the entries that were in this row 2 that's what i replaced row 2 with okay so 2 times 1 was 2 2 plus 3 was 5. that's how i got that 2 times 2 is 4 4 plus 4 is 8 that's how i got that so what we're saying is if you perform this operation there will be no change to the determinant right so the determinant should still be negative 2 and it is if we multiply down 1 times 8 is 8 and then minus if we multiply up 5 times 2 is 10 8 minus 10 is negative 2. okay so with that being said let's take a look at an example so we have this 4 by 4 matrix again the vertical bars tell us to take the determinant so what i'm going to do is i'm just going to copy these kind of numbers we'll go to a fresh sheet so we have 3 1 4 and 10. we have 2 negative 1 6 and 3. we have 0 negative 5 3 and negative 2. we have 1 0 1 and 5. okay so let me copy everything here and let's go to this fresh sheet and we'll have plenty of room i'm going to put my vertical bars here again to tell me that i want to find the determinant again i'm thinking about kind of this main diagonal here all the entries below i want those to be zeros okay if i achieve that through these kind of row operations then what happens is the determinant will be found as the product of those kind of entries going down that main diagonal but remember if we end up swapping two rows which we're going to do here you've got to take and make it the negative of what you find as the determinant there and again we'll see that as we go on so let's start out by just kind of working on this first column what i'm going to do is i'm going to use this 1 okay to get a 0 here and here and then i'm going to swap rows okay so let's go through that i'm going to multiply my row 4 by negative 2 and i'm going to add the result to row 2. okay that's what i'm going to change row 2 with so let me just go through and do the multiplication first so negative 2 times 1 is negative 2. negative 2 times 0 is 0 negative 2 times 1 is negative 2 and negative 2 times 5 is negative 10. okay so now if i add these to the entries in row 2 negative 2 plus 2 is obviously 0 0 plus negative 1 is still negative 1. negative 2 plus 6 is going to be positive 4 and then negative 10 plus 3 is negative 7. all right so let's erase this and this so i'm going to use my 1 again to get this into a 0. so i'm going to multiply row 4 okay row 4 by negative 3 i'm going to add the result to row 1 that's what i'm going to change row 1 with so negative 3 times 1 is negative 3 negative 3 times 0 is 0 negative 3 times 1 again is negative 3 and the negative 3 times 5 is negative 15. okay so adding negative 3 plus 3 is going to be 0 and we know that 0 plus 1 is still 1 negative 3 plus 4 is 1. and then if we do negative 15 plus 10 that's negative 5. okay so good to go there let me erase this and now what i'm going to do is i'm going to swap row 4 and row 1. so row 1 will swap with row 4 okay and again if we do that we've got to change the sign of the determinant so i'm going to put a negative 1 out in front to remind me to multiply the determinant of this new kind of matrix that i'm forming by this kind of negative 1 to find the determinant of that original matrix which is up here okay so if we go back let me kind of copy this 0 1 1 and negative 5. so i can erase that and let me grab this so 1 0 1 and 5 and then i'm going to write this in here so 0 1 1 and my negative 5. okay so we're good to go and again i put that negative 1 out in front if you want to keep track of it a different way you can you can put a negative on top as long as you remember that you know when the end process we multiply down that diagonal you've got to take that and then multiply by negative 1 to find the determinant of again that original matrix you started with all right so let's look at our main diagonal now so that's this guy right here and i want to make that negative 5 into a 0 and that 1 into a 0. so let's start with that negative 5 because it's a little bit harder to do so what i'm going to do is i'm going to multiply row 4 i'm going to multiply row 4 which has a 1 in it okay by positive 5. i'm going to add the result to my row 3. that's what i'm going to change row 3 with so 5 times 0 is 0. 5 times 1 is 5. 5 times 1 is 5 again 5 times negative 5 is negative 25. so let's do our addition now 0 plus 0 is 0. 5 plus negative 5 is 0. we know that 5 plus 3 is 8. we know that negative 25 plus negative 2 is negative 27 okay and let's erase this and now i want to make this guy into a zero i can easily do that by just adding this row two to row four okay that's just like me saying that i have 1 times row 2 plus row 4 and that's what i'm going to replace row 4 with because again if i had negative 1 and i added it to 1 that would give me 0. so 0 plus 0 is obviously 0. negative one plus one is zero we have four plus one which is five okay and then we have negative seven plus negative five which is negative 12. so we're good to go there and now all i need to do again if i look down these kind of diagonal entries the last thing i need is to change the five into a zero but you might be tempted to use this kind of one here right because it's easy to say okay if i multiply row one by negative five add the result to row four this guy would be a0 but you have a problem there because this is a 1 okay so that's not going to work because you're going to end up changing this it won't be a 0 anymore so what you're going to have to do is use row 3 here so i'm going to figure out what i need to multiply row 3 by that when i add row 3 to row 4 okay this guy ends up being a 0. so in other words 8 this entry right here times what let's just say it's x is equal to negative 5 which is the opposite of that number there okay so if i divide both sides by 8 i find that x is negative 5 8. so i would say negative 5 8 times my row 3 this row right here plus my row 4 that's what i'm going to replace row 4 with so i know negative 5 8 times 0 and times 0 again would be 0 and 0. negative 5 8 times 8 would give me negative 5 right we already know that and then negative 5 8 times negative 27 we know that would be positive what is 5 times 27 well that's a 135 okay so this would be 135 over 8. okay so now let's go through and add so obviously zero plus zero in each case is going to be zero you can get rid of that negative five plus five is zero you can get rid of that so really the only thing we have to kind of work on is this so we're gonna add negative 12 and so to get a common denominator i would say this is negative 96 over eight so if i do 135 minus 96 i'm going to get 39 so this would be 39 8. okay so that's going to be that entry there so 39 8 okay so nice and easy not too bad so now finding the determinant is pretty easy because we just go down this kind of main diagonal because everything below is a zero okay we can just multiply those entries okay again this is when you're in upper triangular form which is what we have here now don't forget we swapped rows so that's why we have this negative 1 out here so i'm going to lead with that and say that my determinant would be negative 1 times 1 times you've got negative 1 times you've got 8 times you've got 39 8 okay so what's going to happen is you have a negative 1 here and a negative 1 here those will basically cancel that's positive so you can cancel this 8 with this 8 and you have 39 as your answer so the determinant is positive 39. so if we go back we could say this is equal to positive 39. in this lesson we want to talk about finding the area of a triangle using determinants all right so an application of kind of matrices and determinants is being able to find the area of a triangle whose vertices are given as points in our coordinate plane so you have these three vertices that you're going to be given and the first thing you want to do is you want to label one is x sub 1 y sub 1 one is x sub 2 y sub 2 and then kind of the last one is x sub 3 y sub 3. and the first thing you're going to ask me is does it matter which point gets labeled as which and it doesn't okay and i'll explain why in a moment so then you're going to plug into this kind of formula here for the area of a triangle so you see it's equal to you have plus or minus this one half times the determinant of this guy remember if you see these vertical bars it means to take the determinant okay so first and foremost before i explain anything just notice where you're plugging things in at you've labeled stuff as x sub 1 y sub 1 through x sub 3 y sub 3 you're plugging in x sub 1 y sub 1 in this first row so you have that there and there okay and then you always have a 1 in the third column then you're going to plug in x sub 2 y sub 2 always have a 1. x sub 3 y sub 3 always have a 1. okay so that takes care of that and then you're going to take the determinant now this is where i need to start explaining things so you have that plus or minus there that might be super confusing for you and basically what this is telling us is that we want to make sure that we get a positive area okay so if the determinant evaluates to be negative you multiply by negative one-half to get a positive area if the determinant evaluates to be positive well then i can just multiply by positive one-half and again i get a positive area now the other scenario is you end up with an area that's zero and we're going to talk about this specifically in the next lesson if your area evaluates to zero the three points that you had are actually on the same line okay so that's going to be our test for collinearity and we're going to talk about that specifically in the next lesson for today we're just going to look at two problems and neither is going to have an area of zero so let's talk about the final question and that's why am i able to label you know any of these as x sub 1 y sub 1 or x of 2 y sub 2 or x sub 3 y sub 3. well why don't have to go in a certain order well it deals with the properties of determinants that we've already talked about if i had something like 2 3 4 5 like this the determinant is what 2 times 5 is 10 minus 3 times 4 is 12 this is negative 2. okay what happens if we swap rows so let's say i put the 4 and the 5 into row 1 and the 2 and the 3 in row 2. well what happens is the sign of the determinant changes right the absolute value is the same the absolute value will be 2 in each case but now instead of getting a negative 2 i'm going to have a positive 2. so 4 times 3 is 12 and then minus 5 times 2 is 10. so this is going to be positive 2 versus the negative 2 there okay so when we swap rows okay we just change the sign now think about this here if i had a certain configuration here i've kind of labeled everything if one yields a positive and then i switch what got labeled as kind of x sub 1 y sub 1 with what got labeled as x sub 2 y sub 2 i'm basically just swapping rows so it's just going to change the sign of the determinant the absolute value will still be the same and i have this plus or minus here to take care of any situation where i get a negative so i'm good to go all right so with that being said let's jump into the first example and so we have this triangle with the following vertices we have this one comma three which is going to be this guy right here okay one units to write three units up five comma two that's this guy right here five units to the right two units up and then seven comma nine so seven units to the right nine units up so right there so those are our three vertices let's just copy them real quick so one comma three and then we have five comma two and then we have seven comma nine okay so let's copy this and let's go back up and i'm just gonna paste this in here real quick and we'll just kind of fill this out we'll go to a fresh sheet because i don't think i can fit everything so the area is equal to plus or minus one-half times the determinant of this guy so i'm just going to label these in order i'll just say this is x sub 1 y sub 1 i'll say this is x sub 2 y sub 2 and i'll say this is x sub 3 y sub 3 and again it does not matter what gets labeled as which and just to prove that to you i'll swap it after i do this okay so i'm going to plug in a 1 a 3 and then i'll always have a 1. then i'm going to plug in a 5 a 2 and again i'll always have a 1 and then a 7 a 9 and i'll always have a 1. okay so again i'm just plugging in for x sub 1 y sub 1 that's all i've got a 1 and a 3. plugging in for x sub 2 y sub 2 that's how i got a 5 and a 2 plugging in for x of 3 y sub 3 that's how i got a 7 and a 9. so let's copy this real quick so how do i find the determinant quickly let's just copy this 1 3 1 5 two one seven nine one again the shortcut is to copy the first two columns so you have one five seven three two nine and then you're gonna start at this top left and multiply down this diagonal one times 2 times 1 is 2 and then plus you're going to multiply down this diagonal 3 times 1 times 7 is 21 and then plus you're going to multiply down this diagonal 1 times 5 is 5 times 9 is 45. if i sum that amount 2 plus 21 is 23 23 plus 45 is 68 okay so the first part of this formula is 68 and then for the second part we're going to multiply up now so starting at the bottom left i'm going to go up 7 times 2 is 14 times 1 is still 14 and then i'm going to multiply up 9 times 1 times 1 is 9 then plus i'm going to multiply up 1 times 5 times 3 is 15. so 14 plus 9 is 23 plus 15 is 38. so you have 68 minus 38 which is 30. okay so the first part of this is that the determinant became 30. so the area okay the area is equal to in this case because we got a positive determinant i'm just going to use the plus one half so i'm just going to say one half times 30 which is 15. now when you work with triangles remember your formula from basic geometry one-half times the base times the height remember you have units involved with the base and the height so you've got to think about the fact that it's going to be square units so if you were working with inches it would be you know square inches or inches squared or whatever that is so you would really want to give a precise answer and say it's 15 square units so i'll just say the area and let me erase this is going to be 15 square okay square units now really quickly just to prove this to you let's say i swap these two rows so let's say i put 5 2 and 1 on the top i'm just going to erase that and i'll drag this up here so really this would correspond to what it would correspond to me choosing this point to be x sub 1 y sub 1 okay and this point to be x sub 2 y sub 2 okay and because i swapped two rows the only effect this is going to have is going to change the sign of the determinant right so now the determinant is going to be negative 30 and so to get the area i'm going to multiply by negative one-half and so i'm still going to get 15 it's going to be 15 square units but again let me prove this to you real fast because i know a lot of people say nope you have to go in a certain order you do not all right so let me erase this and let me paste this in really quickly so 5 1 seven two three nine one one one and again i'm copying the first two columns so five one seven two three nine all right so you already know the deal we're gonna start at the top left and we're gonna multiply down five times three times one is 15 and plus 2 times 1 times 7 is 14 then plus 1 times 1 times 9 is 9. so you'll notice that this is 38 now okay and you're going to be subtracting away 68 which is going to give you negative 30. but let's go through it 15 plus 14 is 29 and then 29 plus 9 again is 38 okay so this is 38 and then minus again if we go up 7 times 3 times 1 is 21 9 times 1 times 5 is 45 and then 1 times 1 times 2 is going to be 2. okay 21 plus 45 is 66 plus 2 is 68 again 38 minus 68 is negative 30. so again i've just proved this to you that swapping these two rows is basically like we've chosen different points to be x sub 1 y sub 1 and x sub 2 y sub 2. all i did was change the sign and i can now just multiply this by negative one half to get a positive 15 and so again my answer for the area is 15 square units okay let's just look at one more of these it's a very very easy concept so the vertices we're given now is six comma two negative four comma negative five and two comma seven so here's your six comma two here's your two comma seven here's your negative four comma negative five so let's copy those you have six comma two you have negative four comma negative five and you have two comma seven okay so let's copy this and let's just go to a fresh sheet should know the formula now it's pretty easy so the area is equal to one-half and again you've got that plus or minus out in front times the determinant of let's go ahead and label this so this i'm going to call x sub 1 y sub 1 this i'm going to call x sub 2 y sub 2 and this i'm going to call x sub 3 y sub 3. so we know that the first row is coming from this right the x sub 1 y sub 1. so 6 and then 2 the last column the third column is always a 1. so i can go ahead and just fill that in then this is x sub 2 y sub 2 so negative 4 and then negative 5. then this is x sub 3 y sub 3. so 2 and 7. so very easy formula to remember we don't need these points anymore you just get rid of them they're just going to take up space and let me get rid of that and slide this up out of our way and let me just copy our kind of matrix so we have 6 2 and 1 negative 4 negative 5 and 1 2 7 and 1. so copy the first two columns 6 negative 4 and 2 and then 2 negative 5 and 7. so let's multiply again starting from here and going down 6 times negative 5 is negative 30 times 1 is still negative 30. then we're going to multiply down this way 2 times 1 times 2 is 4 then this way 1 times negative 4 times 7 is negative 28. so we know that negative 30 plus negative 28 would be negative 58 and negative 58 plus 4 would be negative 54. okay so the first part of this is negative 54. then minus now we're going to go up so we're going to start here and we're going to go 2 times negative 5 times 1. 2 times negative 5 is negative 10 and then times 1 is obviously still negative 10. 7 times 1 times 6 is going to be 42 and then we have 1 times negative 4 times 2 which would be negative 8. okay so negative 10 plus negative 8 is negative 18. so what is negative 18 plus 42 that's going to give me positive 24 okay so what you have here is negative 54 minus 24 which is negative 78 okay so negative 78. so let me erase this the value of the determinant of this guy is negative 78. so our area will be equal to because it's negative you're going to use the negative one-half so negative one-half times your negative 78 okay so what is that going to give us well forget about the sign because we know it's going to be positive 78 divided by 2 is 39 so this equals 39 and again when you write your answer you want to put square units so let's just erase this and we'll say that the area is 39 square units in this lesson we want to talk about testing for collinear points using determinants so in our last lesson we talked about how we could use determinants to calculate the area of a triangle given three vertices of the triangle and really quickly i just want to review the formula because we're going to use it here today so the area is equal to you have plus or minus this one half times the determinant of this guy so you would label one of those kind of given points as x sub 1 y sub 1 another is x sub 2 y sub 2 and then the last one is x sub 3 y sub 3. we went through and proved that it didn't matter what got labeled as which okay because we have this plus or minus here so if the determinant becomes negative okay just multiply by negative one-half if it's positive multiply by positive one-half and you're good to go now what we didn't talk about in the last lesson specifically was what we do when this formula gives us a result of zero meaning there's absolutely no area well it turns out that if you have these kind of three points that you're given they're supposed to be vertices of a triangle and you plug it into this formula and you get zero those three points absolutely lie on the same line right so those three points are collinear again they lie on the same line so we can kind of simplify this formula and say that okay if this part right here evaluates to zero then we know those three points that you gave us are collinear so let's go ahead and use this real quick and look at an example so we're given these three points here we just want to test for collinearity we have four comma 1 negative 1 comma negative 2 and negative 6 comma negative 5. so do they lie on the same line so let's just say this is x sub 1 y sub 1 let's say this is x sub 2 y sub 2 and let's say this is x sub 3 y sub 3 it does not matter which gets labeled as which so let me go back up i'll give you a chance to just copy this formula real quick it's very easy to remember the third column of this matrix is all ones and everything else goes in order so you have x upon y sub 1 then you have x sub 2 y sub 2 and x sub 3 y sub 3. so the notation here the sub 1 matches that you're in row 1. the sub 2 matches that you're in row 2. the sub 3 matches that you're in row 3. right so it's very easy to remember so i'm just going to plug into that so i would have my x sub 1 y sub 1 so i would have a 4 and then a 1. this third column's always a 1 okay so that's easy to remember then you'd have your x of 2 y sub 2 so negative negative 1 and then negative 2 and then this is always a 1 and then your x at 3 y sub 3 so negative 6 and then negative 5 and this is a 1. so we need to find the value of this if this is 0 those three points are on the same line or we could say they are co-linear what's the quick way to find the determinate again what we want to do is copy the first two columns so 4 negative 1 and negative 6 we have 1 negative 2 and negative 5. and we want to multiply down okay so 4 times negative 2 times 1 would be negative 8 and then plus we want to multiply down 1 times 1 times negative 6 is negative 6 and then we want to multiply down 1 times negative 1 times negative 5 is going to give me positive 5. so negative 8 plus negative six is negative 14 then plus five would be negative not okay so this is negative nine that's the first part remember you're going to subtract away you're going to go up now so negative 6 times negative 2 is 12 times 1 is still 12. negative 5 times 1 times 4 would be negative 20. and then you would have 1 times negative 1 times 1 which is going to be negative 1. okay so if i do 12 plus negative 20 that's negative 8 okay and then negative 8 plus negative 1 is negative 9. so you see that what you have here remember you have minus a negative so that's plus a positive you have negative 9 minus a negative 9. so this is really negative 9 plus nine which equals zero so because this is zero because those kind of three vertices that you were given okay that were supposed to represent a triangle give you an area that's zero when you punch them into the formula you know that those three points lie on the same line they are co-linear points let's look at another example so now we have 3 comma 7 5 comma 10 and 6 comma 6. so again i'm just going to label these in order x sub 1 y sub 1 this is going to be x sub 2 y sub 2 this is going to be x sub 3 y sub 3. so again just plug into the formula so you already know this is going to be what x sub 1 y sub 1 so 3 7 and then there's always a 1. then x sub 2 y sub 2 5 10 always a 1. x sub 3 y sub 3 6 6 always a 1. okay copy the first two columns 3 5 and 6 7 10 and 6. we're going to multiply down 3 times 10 times 1 is 30 and plus 7 times 1 times 6 is 42 and plus 1 times 5 times 6 is going to be 30. 30 plus 30 is 60 60 plus 42 is going to give me 102. then minus for this one i'm going up so 6 times 10 times 1 is 60 and then plus we have 6 times 1 times 3 which is 18 and then plus we have 1 times 5 times 7 which is 35 so we have 60 plus 18 which is 78 plus 35 which is 113 okay so it's 113. so at this point we can stop we don't even need to do this calculation even though the result will be negative 11 it's not zero so these points are not collinear all right let's look at one more of these i think it's a very easy concept and you just need a few practice problems you basically have it down this is a much better formula to use much faster formula to use versus kind of using the distance formula that we talked about earlier in the course so this is my x sub 1 y sub 1 again i'm just going in order this is my x sub 2 y sub 2 this is my x sub 3 y sub 3. okay so just plug in so again i'm going to say i have negative 12 and negative 4 x sub 1 y sub 1 and then a 1. then i'm going to have my negative 6 my 0 and my 1 again x sub 2 y sub 2 and then a 1. and then i'm going to have 6 8 and 1 again x of 3 y sub 3 and then a 1. so find the determinant of this i'm going to put negative 12 negative 6 and 6 copy the first column then i'm going to copy the second column so negative 4 0 and 8. and let me make this negative 4 over here a little better make it line up a little better so i'm going to multiply down starting so negative 12 times 0 times 1 there's a 0 in there so you know that's 0. forget about it then let me kind of move this down so it lines up we're going to have negative 4 times 1 times 6 that's going to be negative 24 then plus you have 1 times negative 6 times 8 that's going to be negative 48 okay so negative 24 plus negative 48 is negative 72. so that's the first part so let me change colors it's negative 72 and then minus we're going to go up 6 times 0 times 1 is obviously 0. 8 times 1 times negative 12 is negative 96 and then 1 times negative 6 times negative 4 we know that negative 6 times negative 4 would be 24 so plus 24. so what is negative 96 plus 24 well that gives us negative 72 okay so again if you have this minus a negative okay if you have minus a negative it's plus a positive so you have negative 72 plus 72 which is zero so these three points lie on the same line they are co-linear in this lesson we want to talk about finding the equation of a line using determinants all right so over the course of kind of the last two lessons we've been kind of working with this formula which uses determinants to find the area of a triangle given three vertices and then we also kind of expanded on that and we looked at a formula to determine if three points were collinear so let's just recap that real quick we know that for the area of a triangle we can basically get three points or three vertices for that triangle okay and we can label one as x sub 1 y sub 1 the other is x sub 2 y sub 2 and kind of the final one is x sub 3 y sub 3. it doesn't matter what gets labeled as what okay so you'd plug into this guy right here you take the determinant and if that result was negative you'd multiply it by negative one-half so that you got a positive area if that result was positive you'd multiply it by a positive one-half so that you got again a positive area so we found that if the area ended up being zero well the only way that can be true is if those three points that you kind of started with were on the same line okay so that becomes the test for collinearity so if this part right here okay evaluates to zero so i take the determinant i get zero i know those three points that you gave me which were x sub one y sub one x sub two y sub two and x three y sub three are collinear again that just means they lie on the same line now we can expand on this even further and use this kind of formula to find the equation of a lot so we already talked about how to do this kind of earlier in the course we use kind of the typical algebra one method which is if you're given two points that lie on the same line what you do is you first calculate the slope okay and you do that with your slope formula and once you have that you can plug into the point-slope form of the line okay that y minus y sub 1 equals m the slope times the quantity x minus x sub 1 and from there you can get the equation of the line you can solve for y and put it in slope intercept form or you can put it in standard form if you want you can do whatever you need to do okay but we can do the same thing kind of using this formula here so you'll notice instead of kind of three given points now we just have two okay so we have x sub one y sub one that's one point and then x sub two y sub 2 that's another point so the only thing that's changing here is we don't have three points that are kind of known we only have two the first point this x and y okay are unknown so we just leave them as those two variables those are going to be the two variables in our equation okay when we set it up so all you need to do here is just kind of go through your process for getting a determinant and then you're going to set that equal to zero so let's just look at an example real quick this is not a hard concept so we're given these two points so we have three comma one and two comma seven and we wanna find the kind of equation of the line that goes through those points so i'm gonna i'm gonna do the equation in slope-intercept form the y equals mx plus b form okay but you can do it in standard form if you want as well so what i'm going to do is just kind of plug into my formula okay so i know this equals zero and again it's x y and then a one it's x sub one y sub one and then a 1 and then it's x sub 2 y sub 2 and then a 1. so let's just label these in order it doesn't matter you get the same result so let's say this is x sub 1 y sub 1 let's say this is x sub 2 y sub 2. and we're just going to plug in so i'm going to erase x sub 1 y sub 1 and i'm going to put a 3 and a 1. i'm going to erase x sub 2 y sub 2 and i'm going to put a 2 and then a 7. okay so from this point let me kind of copy this i want to get some room going and let me paste this in and let me just get rid of this stuff so how do we find the determinant let me kind of move this down how do we find the determinant we copy the first two columns that's the quick way to do it so we'd have x you would have three you would have two you would have y one and seven and just go through your normal procedure okay and we're gonna set that equal to zero so the first thing i would wanna do is multiply going down okay and this isn't really written too well so let me kind of scooch this down so everything kind of lines up a little better so i would start at this kind of top left and i would multiply down this diagonal so x times 1 times 1 is x then plus you'd multiply down here so you'd have y times 1 times 2 and i can make that a little better so let me try to angle that a little better so y times 1 times 2 is 2y and then plus you're going to multiply down this diagonal 1 times 3 times 7 which is 21. so that's the first part let's just put that in brackets and then we're going to go minus i'm going to erase this so we can see what we're doing so now i'm going to go up so i'm starting at the bottom left and i'm going to go up 2 times 1 times 1 is 2 and then plus we're going to go up 7 times 1 times x is 7x and then plus we're going to go up one more time 1 times 3 times y is 3y okay so let me erase all of this we don't need this information anymore i'm just going to scooch this back down okay and we'll do that like this and what i'm going to do is i'm going to say that this guy right here since this is the determinant i'm going to say it's equal to zero okay so we're just setting up an equation it's the same thing as this i'm saying this determinant is equal to zero this is the determinant just written out okay as the steps we need to take to find it it's still equal to zero so let's go down a little bit and get some room to work so i'm going to drop the brackets from kind of the first part so just x plus 2y plus 21. from the second part i'm going to be distributing that negative to each part so it would be negative 2 minus 7x and then minus 3y okay so this equals 0. that's why it's so important to use brackets so you don't make a sign mistake so now i'm just going to combine like terms we have x minus 7x which is going to give me negative 6x and then we have 2y and negative 3y which is minus y and then we have 21 minus 2 which is plus 19 and this equals 0. so from this point we can again we can put it in standard form if we want we just subtract 19 away from each side but what i want to do is i want to put it in slope intercept form so i am going to add 6x to both sides of the equation and we know that this part would cancel i would have negative y and then plus 19 is equal to 6x let me scroll down a little bit get a little bit more room i want to subtract 19 away from each side and so this cancels obviously so we have negative y is equal to 6x minus 19. so to finish this up i'm just going to multiply everything by negative 1. so this would become positive this would become negative and this is positive so i can just erase this and say that my equation in slope-intercept form for this line is y equals negative 6x plus 19. now let me copy this real quick and let me go back up and let me paste this in here real fast so we're going to use that as our reference i'm just going to go through this really quickly with the kind of old-fashioned algebra 1 method just so that you see that this does work so again if this is x sub 1 y sub 1 and this is x sub 2 y sub 2 okay the slope formula says what m is equal to y sub 2 minus y sub 1 so 7 minus 1 over x sub 2 minus x sub 1 so 2 minus 3. 7 minus 1 is 6 2 minus 3 is negative 1 so the slope is negative 6 and we already know that because it's right there so what we want to do next now that we have our slope and let me just label that i'll say m m my slope is equal to negative 6. i would use one of the points again it doesn't matter which one and plug into that point-slope form of the line so that's the y minus y sub 1 for y sub 1 i'm going to use 1 is equal to m my slope which is negative 6 times the quantity x minus x sub 1 in this case x sub 1 is 3. okay so what i would do to solve for y i would distribute this so negative 6 times x is negative 6 x and then negative 6 times negative 3 is plus 18 okay and this equals over here i have minus 1 and then my y so to solve for y what do i need to do just add 1 to both sides of the equation and i would wrap this up and say that y is equal to this cancels so you have your negative 6x plus 19 which is exactly okay exactly what we have here those are the same so it's the same thing i would argue that kind of using this method with determinants is probably a little bit faster just depending on you know your speed with kind of doing things definitely if you use the shortcut for determinants it's going to be a little faster because you don't have to go through and calculate the slope first and then plug in and kind of get your equation let's take a look at another one so now we have negative 1 comma 5 and 7 comma negative 3. again i'm just going to label this as x sub 1 y sub 1 and this as x sub 2 y sub 2. it does not matter the way that you label the points you get the same answer either way so what i want to do is plug in to my formula again the first one here the first row is just x y and 1. then the second row is x sub 1 y sub 1 and 1 and then it's x sub 2 y sub 2 and 1. so it's very easy to remember again this gets set equal to 0. for x sub 1 i have negative 1 okay negative 1 and then for y sub 1 i have 5 then for x sub 2 i have 7 and for y sub 2 i have a negative 3. all right let's copy this again we're just going to go a fresh sheet so that we don't run out of room and let me just paste this in here and i'm just going to slide this down so i'll just put this out here so it's equal 0. again i'm just going to copy the first two columns so x negative 1 and 7 y 5 and negative 3. so what we want to do is multiply down to start x times 5 times 1 is 5x then plus multiply down y times 1 times 7 is going to be 7y then plus you've got 1 times negative 1 times negative 3. you've got two negatives there which makes a positive so it's basically just 3 times 1 times 1 which is 3. so let's put this in some brackets again it's not going to change anything but we want to do that just to be consistent now this is where the brackets kind of help you because it reminds you to kind of distribute that negative to everything so let me erase all of this and now we're going to go up so 7 times 5 times 1 is 35 and then plus negative 3 times 1 times x is going to be negative 3x so let me just put minus 3x and then lastly you have 1 times negative 1 times y which is going to be negative y i'm just going to say this is equal to 0. and the reason that works is because again if i go through and kind of calculate the determinant this is what i would do and that's supposed to be equal to zero so that's why this is legal so let's just go through and erase this and we can just slide this down if you want or just erase it and kind of redo it and let's just kind of scroll down to get some room and go through and set this up so i can drop the brackets from the left side i don't need to worry about that so 5x plus 7y plus 3 no change when you drop the brackets the negative here has to be applied to each term okay so that's why the brackets are very important there so you'd have negative 35 you'd have plus 3x and you'd have plus y and of course this equals zero so all i really need to do now is just combine like terms i see that i have a 7y and a y that's basically 1y so that would be 8y so let's write that first then you have this 5x and 3x so you combine those and you're going to get plus 8x and then lastly you have this 3 minus 35 so what is 3 minus 35 that is negative 32 and of course this equals zero okay so what i want to do now if i want to solve this for y you can divide everything by 8 now or you can wait it doesn't really matter if i divide everything by 8 what i'm going to get is let me kind of scroll down a little bit i'm going to get y plus x minus 4 equals 0. 8 divided by 8 is 1 8 divided by 8 is 1 negative 32 divided by 8 is negative 4 and 0 divided by 8 is 0. so at this point all i really need to do is subtract away x and add 4 to both sides so in other words i would do minus x and plus 4 to both sides of the equation so minus x and plus 4. and so that's going to give us a final answer of y is equal to again this is all going to cancel negative x plus four okay so that's the equation of our line all right let's just look at one more it's a very easy concept once you kind of look at two or three of these you basically have the concept down so we have one point that's negative two comma negative two again let's just say this is x sub 1 y sub 1 and the other point is 3 comma 8. this will be x sub 2 y sub 2. you already know what to do we're plugging into the formula so you have x y and 1. then you have x sub 1 y sub 1 and 1 so x sub 1 is negative 2 y sub 1 is negative 2 and then you have a 1 and then lastly you have x sub 2 which is 3 y sub 2 which is 8 and then 1 again and this is equal to 0. okay always set this equal to 0 and then let's just copy this real quick and come to this sheet here that way we have lots of room to work let me kind of scooch this down a little bit so we have some room again i'm just going to copy the first two columns so x negative two and three you have y negative two and eight and again i'm just going to multiply down x times negative 2 times 1 is going to be negative 2x then plus let me put some brackets around this we have y times 1 times 3 which is 3y then plus 1 times negative 2 times 8 is negative 16. so let me just put minus 16 there close the brackets and this is minus again we're going to use some brackets so we don't make a sign mistake so let me erase this and now we're going to go up okay so we're going to go 3 times negative 2 times 1 which is negative 6 then plus we're going to do 8 times 1 times x which is 8x and then we're going to go up 1 times negative 2 times y is going to be minus 2 y okay and of course this equals 0. so let's scroll down a little bit and let's just apply this negative to everything on the left side i can just drop those brackets they don't do anything so negative 2x plus 3y minus 16. distribute the negative to each term over here so this would be plus 6 minus 8x and then plus 2y again i'm just changing the sign of each term there so this equals 0. now i can just combine some like terms so i have negative 2x and negative 8x that's going to give me negative 10x i have 3y and 2y that's going to give me plus 5y i have negative 16 plus 6 which is going to give me negative 10. so minus 10 and this equals 0. so everything's divisible by 5. so i can just go through and do that now divide this by 5 this by 5 this by 5 and this by 5. so negative 10 divided by 5 is negative 2 so that would be negative 2x and then plus 5 over 5 is 1 so this is just y negative 10 over 5 is again negative 2. and this equals we know 0 divided by anything that's not 0 is 0. right so 0 divided by 5 is 0. so at this point let me kind of scroll down i can just solve for y by adding 2x to both sides and adding 2 to both sides okay so this is going to cancel and this is going to cancel so you'll have that y is equal to two x plus two in this lesson we wanna learn how to find the inverse of a matrix so in our last lesson we talked about how to multiply two matrices together we found that although the process is quite challenging at first because it's so foreign to us once we kind of nail down the steps it's not so bad overall if you're working with kind of larger matrices then the process is a bit tedious but again once you memorize the steps it's just some simple arithmetic it's going to be the same thing when we talk about finding the inverse of a matrix it's going to be a tedious process when we work with these kind of larger matrices but again we're just doing some simple arithmetic okay we'll find that when we work with a two by two matrix specifically there's a little shortcut and i'll get to that in a little while so the first thing we need to understand when we talk about this topic is the identity matrix so that concept so if i have a square matrix an n by n matrix that's an identity matrix what happens is i have ones going down the main diagonal okay so what that means is that in my row one column one i have an entry of a one and then in my row two column two i have an entry of a one if i have a bigger matrix let's say it was a three by three then in my row three column three i'd also have an entry of a 1. so those are your diagonal entries those are going to be ones you're going to have zeros above and below okay so you'll see this and say that looks very familiar when we did our gauss-jordan elimination remember we put the left side in reduced row echelon form which is this form here one's down that main diagonal zeros above and below and then the right side that was what we got our answer from okay but here we don't have a right side we just have this that we're working with so this is our identity matrix okay now specifically here when you look at this and you notate the size because it's an n by n same number of rows as columns i can put one single number down here to identify the size this is a two row by two column matrix so i can say that this is an identity matrix or i sub two to say it's a two by two identity matrix if i had a three by three identity matrix again i can just say it's i sub 3 and notice how you have a 1 in row 1 column 1 a 1 in row 2 column 2 and a 1 in row 3 column 3. again your diagonal entries are going down that main diagonal there zero's above and below so that's your identity matrix it's a similar process if you had a four by four a five by five a six by six so on and so forth now why is this important well this is kind of building up to something and essentially we need to understand that when we worked with kind of real numbers okay outside of matrices just real numbers going back to kind of grammar school we found that if we multiplied a number by one it was unchanged right one is the multiplicative identity in multiplication with real numbers well what happens is if we take an n by n square matrix like a okay so let's say if a is an n by n square matrix and we multiply it by an identity matrix of the same size right so that's why you have i sub n there you get a back now what's interesting here we learned in the last lesson that matrix multiplication was not commutative right if you change the order around you generally speaking will not get the same product and sometimes you could do it one way but there's no product that's going to exist the other way here you can go in two different ways so you can say a times i sub n would give me a and then also i sub n times a kind of reversing that would also give me a okay so it's not a problem to kind of switch that around so let's see a quick example of this so what we have here is a two by two identity matrix again we show this as i sub two the two tells me i have a two by two and then we have a matrix a which is also a two by two we have negative one and five in the first row two and seven in the second row so based on our rule above we know that a multiplied by i sub 2 or again a 2 by 2 identity matrix should give me a back and then additionally i sub 2 the 2 by 2 identity matrix times a should give me a back so i'm just going to do this first scenario i'll let you reverse it and do that scenario as well again you get a back either way so let's go ahead and crank this out real quick i'm just going to erase this for right now so first and foremost can we do the multiplication of course we can because we're multiplying two square matrices together of the same order if we think about it this is a two by two and this is a two by two so the columns from a the left one match the rows from i okay i sub 2 if you want to be specific the right one okay so you always circle these inside numbers and check to see if they're the same if they are you can proceed again the size of the matrix comes from the outside numbers so we know it's a two by two okay so let's go ahead and set this up and i'll put my entries in here and just to stay consistent with what we did in the last lesson i'm gonna put row one here and row two here and i'm going to put column 1 here and column 2 here again if you're really good at matrix multiplication you don't need this anymore when you first start out i find that it really really helps students figure out what's going on if i look at this kind of entry here i'm thinking about okay this is row 1 column 1 in my answer okay i always get the row from the leftmost matrix so in this case that's a so i want to be in row 1 of a so this is the first row i get my column from the rightmost matrix or you could say the second matrix if you want so in this one i have column one so i want to be in the first column here okay so to get this entry all i'm going to do is find the product of this row vector times this column vector that's all it is okay so we know how to do this we go through and we say okay the first entry times the first entry negative 1 times 1 is negative 1 then we're going to add to this we're going to have the second entry times the second entry so 5 times 0 is 0 so we know this is negative 1 so this is negative 1. and you can see already that your first entry in this matrix matches what it is in a okay so as we continue now if i move into this position here i'm still in row one okay so for a i'm still going to be in row one but now i'm shifting to column two so for this guy i'm gonna be in column two okay so that's all it is same process i'm gonna do this one times this one so negative one times zero is zero then plus i'm going to do this one times this one 5 times 1 is 5. we know 0 plus 5 is 5 and again we can see this matches exactly so now what i'm going to do is shift because i'm going to be in row 2 down here now so i'm going to start in row 2 here and i'll have column 1 here okay so row 2 column 1 this guy right here so 2 times 1 is 2 plus 7 times 0 is 0. this equals 2 so i get a 2 there and again this matches perfectly with a so let me erase this now we're gonna move on to kind of this one so for the last entry down here you're in row two column two so you've got the row from a and the column from i okay so you've got two times zero which is zero plus you've got 7 times 1 which is 7 which gives me 7 so i put a 7 there so you can see that we multiplied our matrix a by our matrix i sub 2 or the 2 by 2 identity matrix and we got a back again if you want to pause the video and reverse this if you wanted to do i sub 2 times a you would get a back as well okay so now let's move into talking about how to find the inverse of a kind of square matrix so first we're going to define a word here so we have this non-singular word okay and that just means that the matrix is going to have an inverse so when a matrix has an inverse we say it's non-singular you might also see the term invertible okay so those two words kind of mean the same thing you might see non-singular in one textbook you might see invertible another just depending on where you're kind of getting your information from if you have a matrix that doesn't have an inverse and we'll see an example of this in a little while it's known as a singular matrix okay so if a is a non-singular meaning it has an inverse n by n square matrix then this property is going to be true if i take a and i multiply it by the inverse of a okay then i'm going to get an identity matrix of the same order again i just put that in there to say it's an n by n so if a was a two by two the inverse of a would be a two by two and then this identity matrix would be a two by two so let's show an example of this real quick i'm giving you a and i'm giving you a inverse okay and what we're saying is that if you multiply a by its inverse you're going to get the identity matrix in this case you would get an i sub 2 because it would be a 2 by 2. okay so let's crank out a times a inverse and you could reverse this you would still get the identity matrix okay it wouldn't matter so what this is going to give me is what let me kind of write this out i know that it's a two by two but times a two by two so the answer will be a two by two okay so this is row 1 this is row 2 column 1 and column 2. so i know for this guy right here row 1 column 1 i want this row here from the first one times this column here from the second one so this row vector times this column vector what is that so we have three times one which is three plus we have one times negative two which is negative two and we know this is going to give me one so i've got a one there already okay now let's move on so now i'm going to erase this and i'm going to move to this column here so this guy 3 times negative 1 is negative 3 then plus 1 times 3 is 3 and this is going to be equal to 0. so that's going to be this entry here okay so now we're moving into this row down here so let me kind of switch up my highlighting i know i'm going to be in this row here and this column here to start so 2 times 1 is 2 and then plus 1 times negative 2 is negative 2. this equals 0. so let me write that in here and then let me erase this one more to do and let's just highlight this now so we have 2 times negative 1 which is negative 2 plus 1 times 3 which is 3 and obviously this is 1. so we have our identity matrix we have our i sub 2 which is exactly what we were told from this rule here okay so now that we've proven that this is true with a simple little example let's talk about how we could find a inverse if we didn't know it okay so i'm going to give you the shortcut first okay and then i'm going to show you kind of the long way this shortcut that i'm going to give you is only going to work for a 2x2 matrix unfortunately when you get something larger you have a few different techniques that you can choose from the one i'm going to show you today is the one you're going to see in your textbook there's other methods that you can use they're not a big difference in time it just kind of depends on which kind of method you find less tedious i can tell you that this is not a process that is very enjoyable okay so for the 2x2 the shortcut goes as follows so i'm going to define a just generically to have these kind of entries of lowercase a lowercase b lowercase c and lowercase d okay now we haven't yet talked about the determinant of a matrix yet we'll get to that in two lessons from now but for now it's very easy for a two by two matrix it's just this guy times this guy so it is a times d then minus you're gonna go this times this so minus b times c okay so that's the determinant of a a lot of times you'll see d e t of a like this there's a lot of different notation for this and i'll talk about that when we get to that video for now you just need to know this is the determinant of a okay so let me erase this real quick and let me erase this i'm just going to drag this up here a little bit so that we know what this is okay so the way we find a inverse is that we're going to go 1 over the determinant of a and we're going to multiply this by we're going to do a little kind of swap root here so i'm going to take a and d and i'm going to swap them so i'm going to put d here and a here and then i'm going to make c and b into their opposite so i'm going to have the negative of b and the negative of c now the first time you kind of see this you're like what in the world is going on but after you do this a few times it's something you just memorize and i can tell you this comes up very often it's a very good shortcut it's going to save you a lot of time okay so let's say that i replace this with these numbers here and i know they're already copied but let's just say we have three one two and one and so i know that my a my lower case a is three my lowercase b is one my lowercase c is two and my lowercase d is one okay so let me start out by going 1 over the determinant again the determinant is 3 times 1 which is 3 minus 1 times 2 which is 2 so that's going to be 1. okay so 1 over 1 is 1. okay so that's pretty simple then times i'm gonna do this stuff here so these guys switch positions so the three and the one switch so one goes here and three goes here and then what's going to happen is this guy right here this guy right here i changed them into their opposite so this is negative one and this is negative two we already know that one times this in scalar multiplication just is itself so i know that a inverse is this and again we already know that that's the inverse right i have a 1 and a 1 a negative 1 a negative 1 a negative 2 and a negative 2 and a 3 and 3. so not too bad overall compared to what i'm about to show you so let's talk about the long way now and this is something you'll see in your textbook i'm going to show you later on at the very end of the video for those of you who want to see it where this kind of thought process comes from i'll start out with a 2x2 matrix and i'll go through the whole thing and i'll show you why this makes sense but for now if you don't want to look at that you can just take it as a given okay so if you want to find the inverse of an n by n non-singular matrix a so this works for a two by two a three by three four by four whatever you want to use it on okay you start out by basically setting up this in step one so you're going to put your augmented matrix where matrix a goes on the left and the identity matrix of the same size okay goes on the right okay then what you want to do is you want to use row operations to get this guy into the format of being the identity matrix and then this guy on the right which we'll call matrix b okay the result of doing the row operations to getting this guy into that format b is now going to be the inverse of a okay and again i'll show you exactly where this comes from at the end of the lesson the process takes a little while so i know some of you don't want to watch that all right so let's go back and i want to grab a real quick and i'll show you that you end up with the same thing so let's just take this real fast i'm going to copy it and i'm just going to paste this in here real quick and i'll show you that you get the same result either way so what i want to do is i want to take this matrix here this 3 1 2 and 1 and then i want to put a vertical bar and i want to write my identity matrix of the same size so one's going down the main diagonal a zero above and below okay so what i'm going to do is i'm going to use row operations to get this side over here to look like this over here i want this side to be an identity matrix and then this side would give me the inverse okay so what we're going to find is that we're going to end up with a right side that matches this okay so let's go ahead and do our kind of row operations we already know that this process kind of you know for lack of a better word is very tedious okay so the book usually tells you to get a one here first and then use that to get your zeros there's other methods you can use i'm gonna stick with that method i know there's kind of quicker ways but i'm going to stick with that because i don't want a confusion okay so the first thing i'm going to do is i'm going to multiply row 1 by a third so that that entry becomes a 1. so i'm going to do 1 3 times row 1. that's what i'm going to replace row one with okay so this three would become a one this guy would become a one-third this would be a one-third and this would stay zero so let me erase all of these entries and let me put these in this is 1 1 3 this is 1 3 again and this is 0. okay so that's the first step now the second step is to get a 0 here and to do that i'm going to use my 1 that's above that okay so i think about the opposite of 2 which is negative 2 and i would say 1 times negative 2 would give me negative 2. if i added negative 2 to 2 that would be 0. okay so what i want to do is i want to take negative 2 and multiply it by row 1 add the result to row 2. that's what i'm going to replace row 2 with okay so let's go ahead and do that real quick so if i did negative 2 times 1 that would give me negative 2 and then if i added that to 2 i would get 0. okay then if i did 1 3 times negative 2 that would give me negative two-thirds so negative two thirds and then i'm going to add that to positive one or three thirds and if i add those two amounts together negative two plus three is one so i'd have one third okay and my screen is going to get too busy so let me kind of fill these in as i go so this is a zero and this is also a one-third so let me erase this we don't need this anymore and then for this one right here i'd multiply negative two times a third okay so that would give me negative two-thirds and then i'm gonna add the result to row two so i'm gonna add 0 we know that would still be negative 2 3. so this is negative 2 3 and this one's not going to move at all because negative 2 times 0 is 0 0 plus 1 is 1. okay so that part is done and just a little bit more to go so now i want to make this a 1 okay so i would just multiply that whole row by 3 right the reciprocal of 1 3. so 3 times row 2 would give me my new row 2. okay so 0 times 3 is 0 1 3 times 3 is 1. if i did negative 2 3 times 3 i know that the 3s would cancel i'd have a 1 negative 2 times 1 is negative 2. so this is negative 2 and then i know that 1 times 3 is 3. okay so let me just put a 3 in there and you can already see that the kind of bottom row here works out to be this okay but we got to fix we've got to make this a 0 now okay so what i'm going to do is i'm going to multiply row 2 i'm going to multiply row 2 by the opposite of 1 3 which is negative 1 3 i'm going to add the result to row 1. okay that's what i'm going to replace row 1 with so i'm going to do 0 times negative 1 3 that's 0 zero plus one is still one one times negative one third is negative one third negative one third plus one third is zero so that's going to be zero and then negative one third times negative two would be positive two thirds and i can erase that sign i don't need it then plus one third well this would be one right two plus one is three three over three is one so this becomes a one here and then lastly okay i would do a negative one one-third a negative one-third times three and we know this cancels with this and gives me a one negative one times one is negative one okay and i would add that result to zero so that would be negative one now you can see here once the process is done this here matches this here so that's exactly what we said in our instructions right once you kind of start this way you use your row operations you get in the format of the identity matrix is on the left and on the right you're going to have the inverse of your matrix and we know that that's true already because we know that a inverse is the inverse of a because we already showed that okay so that's another way to go about doing it if you have a 2x2 you know it's not something you really want to do because it just takes too long you want to go ahead and use the shortcut so that's the only time i'm going to use that there i want to show you an example of a singular matrix okay so i'm just going to do one with a two by two and what you're going to find is that it's singular or meaning it doesn't have an inverse because the determinant is going to be zero okay the determinant is going to be zero so if you use row operations here you're not going to be able to get it into the format that you want but again we're going to use our shortcuts so i'm going to say that a inverse is equal to 1 over the determinant of a so 3 times 4 which is 12 minus 6 times 2 which is 12. so once you see that this is a 0 down here the determinant you can stop and say this is a singular matrix okay it's not going to have an inverse so you don't need to go any further i know the rest of it was to kind of multiply it by this these two guys would switch so you have 4 here and 3 here and these guys become their opposite so negative 6 here and negative 2 here but the whole thing is blown okay because you have a zero in the denominator and we know we can't divide by zero so this isn't going to work there's no inverse all right so let's take a look at an example with finding the inverse of a three by three matrix again as i said earlier when you get a three by three or four by four there are some tricks out there there are some different techniques but none of them are pleasant okay and you basically have to pick one of them and then grind through the work so we're going to look at the method that we learned today so basically i'm just going to copy this guy right here and i'm going to say i have a 1 a negative 1 and a 1 a 0 a 4 a negative 2 and then i have a 2 a 3 and a negative 1. okay so that's the first part remember you copy your matrix you're trying to find the inverse of on the left you're going to put your kind of vertical bar in here and then i'm going to write the identity matrix on the right so i'm going to put a 1 here a 0 here and a zero here then a zero a one and a zero and then a zero a zero and a one okay so again one's going down that main diagonal zeros everywhere else so you could say zero's above and below so let me go ahead and copy this because we won't be able to fit this on this page and let's go ahead and paste this in here and we'll just go through the work all right so we want to take the left side here and transform it into the identity matrix or you could say you're putting it in reduced row echelon form and again through that process the right side here is going to give us our inverse matrix of a okay so the first entry here we always want to get a 1 here if you go by what your book tells you right you get your one first in the column and then you use the one to get your zeros okay so that's already done for us so what i want to do is concentrate on getting a zero here and then getting a zero here okay so let's do those one at a time so i know that i can multiply row 1 by 1 which is just the same thing as having the row itself and then add the result to row 2 because 1 and negative 1 are opposites that's going to give me a 0 for that entry there so the way i'm going to write this is i'm going to say 1 times row 1 plus row 2 is going to give me my new row 2. you can get rid of the 1 if you want i'm just going to put it there for kind of emphasis okay so let's go through and basically just say that we're adding row 1 to row 2 and that's what we're replacing row 2 with so 1 plus negative 1 is 0. let me make that a little better and then 0 plus 4 is just 4 so that's going to stay unchanged 2 plus 3 is going to be 5 1 plus 0 would be 1. 0 plus 1 would be 1 so that stays 1. 0 plus 0 is obviously 0 so that stays as a 0. all right so that's the first step then the second step here again we want to get a 0 here so to do that i want the opposite of 1 which is negative 1 so i'm going to multiply row 1 by that so negative 1 times row 1 then i'm going to add the result to row 3 down here that's what i'm going to replace row 3 with okay so let me just go through and multiply everything by negative 1. so 1 times negative 1 is negative 1 0 times negative 1 is 0. 2 times negative 1 is negative 2 1 times negative 1 is negative 1 0 and 0. if i multiply either of those by negative 1 i get 0. okay so those are going to be your entries that you add to this row so negative 1 plus 1 is 0. zero plus negative two is negative two negative two plus negative one is negative three and let me just erase these because we're done with them okay and then negative one plus zero is negative one and then obviously if i add zero to something it's unchanged so i don't need to worry about the rest of those okay so now let's move into the next column again if you follow what your book tells you and you don't have to you can do it different ways you want to get a 1 first okay so i'm going to put a 1 right there in row 2 column 2 okay considering the left part of the matrix and to do that i'm just going to multiply row 2 by 1 4 right the reciprocal of 4. so 1 4 times row 2 is what i'm going to replace row 2 with so we know that zero times a fourth would just be zero so that's not changed four times a fourth is one and then we know this would be what it would be five fourths so five fourths this would be 1 4 and this would be 1 4. so let me just change that in each case and then lastly we know 0 times 1 4 is 0. so that's done so now conveniently i already have a 0 above i just need a 0 below okay so to do that i'm going to use my 1 and i'm going to multiply row 2 by the opposite of this negative 2 which is 2. so 2 times row 2 and then plus my row 3 that guy right there is going to give me my new row three okay so let me just multiply everything by two so two times zero is zero then we have two times one that's going to be two then we have two times five fourths so two times five fourths so this would cancel with this and give me a two down here so that's going to be five halves so five halves and then two times one fourth we're doing that twice so we know that this cancels with this and gives me a two down there so in each case i'm going to have a half so i'm going to have a half here and i'm going to have a half here okay and then lastly 0 times 2 would be 0. so that's the result in each case of doing 2 by each one of these guys okay so now we want to take these results and add them to the entries in row three zero plus zero obviously that's not going to change two plus negative two is going to give me zero this one this one and this one is going to be a little bit of work so let's do those off to the side so we have 5 halves plus a negative 3. so negative 3. so i need a common denominator i'm going to multiply this by 2 over 2. so i could say this is negative 6 over 2. so negative six over two five plus negative six is going to be negative one so this is negative one-half okay so this is negative one-half so that's what i'm going to replace this guy with negative one half okay so let's move on now to this one so what we have is one half plus negative one so one half plus for negative one i'm just going to write it as negative two over two okay and so negative two plus one would be negative one so this is negative one-half so this is negative one-half and obviously one-half plus zero is one-half so that one's easy and then zero plus one is one so you don't have to do anything there okay so now we are getting close to the end okay so we wanna get a one here as our first thing to do in this kind of column here so to do that i'm just going to multiply row three by the reciprocal of negative one-half which is going to be negative two so negative two times row three that's going to give me my new row three and so let's go ahead and do that we know that this and this would be unchanged because they're zeros zero times anything is zero this guy would obviously be a one so i really only need to work on these and i know that this one right here this one would just be negative two so let's work on this one and this one okay so negative two times negative one half we know that this would cancel with this and be one so you basically have what you have negative one times negative one which is positive one so this is going to be positive one and then in the last case you have it negative two times one half and so this cancels with this and gives me a one negative one times one is negative one so this would be negative one okay so we have the bottom row here complete and now what we want to do is get a zero here and here and we'll be done okay so i'm going to multiply row three by the opposite of five fourths which is negative five-fourths add the result to row two okay you already know what's going on so for these first two it doesn't matter because i'm multiplying by zero so anything by zero is going to be zero and if i add zero to something it's not going to change so i don't need to worry about those entries i know that if i multiply negative 5 4 by 1 i get negative 5 4. if i add it to 5 4 i get 0. so i really don't need to worry about that either okay i only have to work on these over here so let's kind of do these entries real quick so negative 5 4 times 1 is negative 5 4. then we have negative 5 4 times negative 1 which would be positive 5 4. and then negative 5 4 times negative 2 what would that give me well i know it's going to be positive so i can just erase these signs and i can cancel this with this and put a 2 down here so that's going to be 5 halves okay and let me get rid of this sign here we really don't need it it's just for emphasis let me kind of drag these up a little bit more all right so let me just kind of put commas between them so we know what's going on all right so let's start with this guy right here we would have 1 4 plus this negative 5 4. we have a common denominator already so we have negative 5 plus 1 which is negative 4 so this gives me negative 4 over 4 which is negative 1. okay so this is negative 1. so i can erase this don't need that anymore and erase this and then now i'd have 5 4 plus 1 4 okay so 5 plus 1 is 6 so you'd have 6 over 4 which reduces to 3 halves right each is divisible by 2. so this is going to be 3 halves okay and then obviously 5 halves plus 0 is just 5 halves all right let me make this better because that doesn't look so good okay so we have that completed now and we just need to get a zero in this entry here and we will finally be done all right so how can we do that we multiply row three by negative two the opposite of two so row three gets multiplied by negative two add the result to row one okay so we don't have to worry about again these entries to the left of this because you have negative 2 times 0 negative 2 times 0 it's going to be 0 add 0 to something it's not going to change i know this is going to be 0 because negative 2 times 1 is negative 2 negative 2 plus 2 is 0. so that's taken care of so i really don't have to worry about these entries over here okay so what is negative two times one it's negative two what is negative two times negative one it's positive two what is negative two times negative two it's going to be four okay so let's add these negative two plus one is going to give me negative 1 then next you're going to have 2 plus 0 which is going to give me 2 and then lastly you're going to have 4 plus 0 which is going to give me 4. okay so we've completed our process all right so just to recap what we did we started out with a on the left and the identity matrix and i sub 3 because it was a 3 by 3 on the right we used row operations to basically transform the left side into an identity matrix we'll say i sub 3 because it's a 3x3 and then the right now let's just call this b or you could just say it's a inverse right because it's the inverse of a at this point okay so that's the process that's how you do it if you wanted to say we can just copy this real quick go back up and let's just paste this in we can say that a inverse okay is going to be this part right here okay on the right so let me just kind of set this up we'd say negative 1 negative one and one you would have two you would have three halves and negative one you would have four you would have five halves and negative two and so that would be the inverse of this matrix here and again this process is going to work if you you know have a four by four or five by five or something like that obviously those problems get very very tedious and there are some tricks out there but usually in your course you're only going to be given a three by three and occasionally maybe for like a bonus question they'll give you a 4x4 these guys who are teaching these classes they understand how tedious this process is all right so let's wrap up the lesson by kind of returning to this generic formula i want to show you where this comes from some textbooks show you where it comes from and you might get lost if you're trying to read it other textbooks just skip it okay so if we find the inverse again of an n by n non-singular matrix a again the first step is to put this kind of augmented matrix there where a what you're trying to find the inverse of is on the left and your identity matrix is on the right then through your row operations the left side you change that into the identity matrix one's going down the diagonal 0s above and below and this matrix that's formed on the right let's just call it b is the inverse of a so where does this come from let's go ahead and start with a fresh example okay so we have a and it's equal to this we have 5 and negative 1 and we have 3 and 6. so let's start out by using the little shortcut to figure out what we should get so again a inverse if we use our shortcut is equal to 1 over the determinant of a which is 5 times 6 which is 30 okay minus negative 1 times 3 which is negative 3 minus a negative is plus a positive so this should be plus 3. so this would be plus 3 here so this would be 1 over 33 so it would be 1 over 33 times okay remember the little trick you have to do over here you switch these two so this becomes a 6 and this becomes a 5. these two become their negatives or you could say they're opposites so this would be negative 3 and this would be positive 1. okay so if i did let me kind of do this down here i'll continue and i need a bigger bracket so 1 over 33 times 6 would be 6 over 33 of course each is going to be divisible by 3 there 6 divided by 3 is going to give me 2 and 33 divided by 3 is going to give me 11. so that's my first entry then over here if i did negative 3 over 33 each is divisible by 3 this would be a 1 and this would be an 11 okay and then for this entry here 1 over 33 times 1 would just be 1 over 33 and then for this last entry i would have 1 over 33 times 5 which is 5 over 33 okay so that would be a inverse and let me just kind of write this over here let's kind of scooch this up now let's say that you didn't know that shortcut and you didn't know the rule that i just gave you okay so you didn't know about kind of going through the process how could you figure this out how could you derive the formula well if i just started out with the notion that a times a inverse gives me an identity matrix in this case it's going to be a 2 by 2 well i can come up with a little formula so let's kind of go down a little bit let me just copy a real fast so it's 5 3 negative 1 and 6. let me copy this real fast and i'm going to go to a fresh sheet we'll come back there and we'll check our answer eventually so we have this guy here and i know that if i multiply it by its inverse let's just say i don't know what that is we'll say it's x y w and c these are just kind of stand-ins for those entries okay we know that if we multiply these two together if they're inverses i get an identity matrix that's a two by two so one's down the diagonal and i've got zeros above and below so we know this so what i would do is i would say okay i can multiply these two together so the way i would do that i know i would get a 2x2 matrix and i need a lot of room here so let me just not put my brackets in yet i know for this entry here i would do this row times this column so 5 times x is 5x and then i would have plus negative 1 times w so minus w then for this entry i would do this row times this column so 5 times y is 5y and then negative 1 times z you'd have minus z okay then for this entry i would do this row times this column so i would have 3x plus you'd have 6w okay and then lastly i would do this row times this column 3y okay plus six z all right so so far so good now i know that this guy if i set it equal to this what has to be true let's think about what we know so far or what we've learned in kind of previous lessons we know that two matrices can only be equal if and only if their size is the same in each case here we have a 2x2 and their corresponding entries are exactly the same so what that means is that this guy right here this 5x minus w must be equal to 1 if this is true okay we also know that five y minus z must be equal to zero again corresponding entries so this guy right here this three x plus six w must be equal to zero and this guy right here this 3y plus 6z is equal to this one here okay so so far so good now let's go down a little bit and let's continue to think about what's going on i have four variables that are unknown and i can solve it here because i can split it into kind of two different systems with the same two variables each so on the left i'd have a system with x and w on the right i'd have a system with y and z okay now you can go through and solve this pretty quickly but what i'm going to do is solve it it's going to take me a little bit longer but i want to show you what your book's going to do so i would solve this using a kind of augmented matrix so in the first one i would have kind of the way this is lined up i have my x on the left and my w on the right okay of this left side of the equation so 5 and negative 1 and 3 and 6. and notice the order is important i've got to have the x lined up and the w lined up so that this column is the x's this column is the w so this is for the x's this is for the w's right then this guy is going to give me my constants my 1 and my 0. let's stop and think about this for a second i know that if i put this in reduced row echelon form this guy would be a one this would be a one this would be a zero and this would be a zero so x one x plus zero w would be this so this guy is going to give me what x is okay and similarly this guy is going to give me what w is okay so let's erase this and let's just keep that information so we keep track of what's going on now similarly i can set up this system over here 5 negative 1 3 and 6. and through a similar thought process i can say that this guy right here is going to be my answer for y when it's done and this guy right here is going to be my answer for z okay and i'm going to erase this kind of circling we don't need that and it would just get in our way okay now what do you notice here this is where people get lost in this process the left side here matches the left side here right 5 negative 1 3 6 5 negative 1 3 6. okay so what happens is if i'm going to solve this using row operations what i do on this would match what i do on this this they're the same numbers right so what your book's going to do they're just going to erase this and they're going to squeeze this down and say hey i'm going to combine these two operations because i can't and what's going to happen is i already know i've already kept track of what's going on so when i put this guy into reduced row echelon form where i make it into its identity matrix okay then i can get my answer by saying this is what x is this is what w is this is what y is and this is what c is okay so let's copy this and go to a fresh sheet and we'll paste this in and we'll finally crank this out i know this takes a while but nothing kind of worth seeing is quick okay so let's go through and use our kind of row operations we're trying to get this left side here into reduced row echelon form or again you could say the identity matrix so the first thing i would want to do is get a 1 here so how would i do that i would multiply row 1 by 1 5th that's what i'm going to replace row 1 with okay so i know that this would be a one i know that this would be a negative one-fifth i know this would be a one-fifth and this would stay a zero so that's pretty easy then the second thing i would do is i would make this into a 0. so i would multiply row 1 by negative 3 i would add the result to row 2 that's what i'm going to replace row 2 with so negative 3 times 1 is negative 3 negative 3 plus 3 is 0. then negative 3 times negative 1 5 gives me 1. negative 3 times negative 1 is 3 and then over 5. so this is three-fifths and then i'm going to add to this 6. so i'm going to have 3 fifths plus might as well say 6 is 30 over 5. okay so this would be 33 fifths this is 33 fifths all right and then we have negative 3 times a fifth that's negative three-fifths if i add that to zero it's just negative three-fifths so that's pretty easy and then lastly we have negative three times zero that's zero so we don't have to worry about that so that part is done now let me get this to be a one so i'm going to multiply row 2 by 5 over 33 that's what i'm going to replace row 2 with and so we know this is a 1. we don't even have to worry about this this right here if i did 5 over 33 times negative three-fifths the fives are going to cancel the three over the 33 we know they're each divisible by three right so we say this is a one and this is 11. so this right here is going to be negative 1 over 11. and we know that lastly i'm going to have 5 over 33 here right because 1 times anything is itself all right so one more thing to do here i just have to make this into a zero and to do that i'm going to multiply row two by one fifth and i'm going to add that result to row one that's what i'm going to replace row one with let me kind of slide that down it took a little bit too much room slide that over here all right so one-fifth times zero is zero don't have to do anything with that one-fifth times one is one-fifth one-fifth plus negative one-fifth is zero all right so now i'm gonna do one-fifth times this negative 1 over 11 okay so that's going to give me negative 1 over 55 okay so negative 1 over 55 and i'm going to add to this 1 over 5. so to get a common denominator going i'm going to multiply this by 11 over 11 okay so that would give me 11 over 55 and if i add these two amounts together i'm going to get 10 over 55. now i can simplify this because each one of these guys is going to be divisible by 5. if i go ahead and cancel a 10 with a 5 10 divided by 5 is 2 and 55 divided by 5 is 11 okay so that's what i'm going to replace this with this is going to be 2 11 and then lastly i'm going to have 1 5 five over 33 and so this cancels with this and gives me a one i'm gonna have one over 33 so 1 over 33 plus 0 is just 1 over 33 okay so let's erase all of this didn't mean to erase that bracket and let's kind of copy this real fast so let me paste this in here real quick and i'm saying that this guy right here should be 2 over 11. so this is my x it should be 2 over 11. this guy right here should be 1 over 33 and this guy right here should be negative 1 over 11 and this guy right here should be 5 over 33 okay so we already know that this is the case because we found it using the shortcut so let me copy this real quick so let me just paste this in here real quick and let me erase this so what you see is that you have a inverse right this is exactly what we found from the shortcut you have two elevenths and two elevenths one over thirty three and one over thirty three negative one over eleven and negative one over eleven five over thirty three and five over thirty three okay so it's the same and what's cool about this is that we ended up deriving kind of this formula that we see in our textbook right so we ended up saying that a times a inverse gives me the identity matrix we had to set up some kind of variables to represent our unknowns for the a inverse but what we ended up with was solving kind of a system that's set up this way we had a on the left and we had the identity matrix on the right we tried to solve this system so we try to put this guy in reduced row echelon form where we have ones down the diagonal zeros above and below so that's the identity matrix right and then once we got in this format we found that this guy right here ends up being the inverse okay of what we started with which was a so really cool to know where that comes from if you were reading that in your textbook and you got lost now you know the steps now you know how it works and you could really do this with a 3x3 a 4x4 whatever you want you can extend what we just did out to as large of a system as you wanted to work with again as long as you're working with a matrix that's invertible or again non-singular in this lesson we want to talk about solving linear systems using inverse matrices all right in the last lesson we talked about how to find the inverse of a matrix we found that only square matrices had an inverse and not all of them would have an inverse right so when a square matrix has an inverse it's called non-singular or invertible if it doesn't have a matrix it's called a singular matrix now we know that for a two by two matrix there's a very good shortcut that we can use to find the inverse when you get into a three by three or higher there are some different options you can kind of employ but all of them are tedious they all involve a good amount of work okay so what we want to do here is we want to take the next step and we want to show how we can solve a linear system using an inverse matrix but it's not necessarily going to be a good method to use okay in a lot of cases this is going to be way more work okay but it's something that you do need to learn because it's taught in every textbook and you're probably going to get tested on it now let's go ahead and start out with this kind of easy system to get started we have 8x minus 7y equals negative 17 and we have negative 6x minus 8y equals 26. so the very first thing you want to do is make sure that your equations in your system are in standard form okay just like when we try to set up an augmented matrix you want the ax plus b y equals c so everything has to line up your x's your y's your constants make sure all that lines up now the next thing you want to do in this particular case you don't have to do this this is just something that i like to do to make things simpler notice how everything there is divisible by 2 so i can really rewrite this and say this is negative 3x and then minus 4y and this is equal to 13. so i'm just going to rewrite my system real quick and say that it's 8x minus 7y is equal to negative 17. and negative 3x minus 4y is equal to 13. okay so let's copy this real quick i'm just going to go to a fresh sheet because things are going to get busy very quickly here and let me just paste that in so now what i'm going to do is going to seem a bit strange at first but i'm going to set up three different matrices okay so one's going to be for the coefficients one's going to be for the variables and one's going to be for the constants so the way your book is going to do this they're going to say that a is going to be the matrix for the coefficients so if i just grab them in order i have 8 and negative 7 and then i have negative 3 and i have negative 4 okay so that's going to be my first matrix of this kind of equation i'm going to be setting up which we'll get to in a minute then the second matrix is going to be a column vector so a matrix with one single column it's just going to have the two variables of the system so i have x and i have y so i'm going to put x and y okay so this is for the variables of the system and then i'm going to have a matrix b which is going to contain these constants okay so let me write that it's going to be another column vector so i'm going to say we have negative 17 and then we have 13. okay so the way we're going to set this up is we're going to say that a this matrix here multiplied by x this matrix here should be equal to b this matrix here so you might be saying why does that work out well what happens is if i go through the process of multiplying this matrix by this matrix i'm going to get entries that exactly match this kind of left side okay and it's going to be set equal to this right side here now let me show you that real quick let me just kind of scooch this out of the way so we have a little bit of room and i'll just move this up it's going to be very tight to do this but first and foremost can we multiply a times x we know a is a 2 by 2 and we know that x is what it's a two by one so the columns from a match the rows from x so we know we're good to go there and we know the output of this is going to be a two by one right the outer dimension so two by one so if i do ax i would get a two row okay by one column matrix so i would take this row times this column a times x is eight x and then negative seven times y would be minus seven 7y and then for this one right here i take this row times this column negative 3 times x is negative 3x and then negative 4 times y is minus 4y now again this exactly matches what we have on the left side here so let me kind of slide this down now and let me put some brackets around this and if we say this is equal to b b is down here this is negative 17 and this is 13. this is exactly what we see with our system remember two matrices can only be equal if they're the same size in each case it's a two by one okay and they have exactly the same elements okay so we're saying that this is going to be equal to this this is equal to this okay we're saying this is equal to this this is equal to this so that's where this equation comes from i just wanted to show you that so that you're not confused about why this is mathematically legal okay so now let's talk a little bit about how we could solve this equation right now we know what the kind of entries are for a and the answers are for b but we don't know the answers for x that's what we'd like to find so we can figure out what x and y are so with normal equations right linear equations in one variable it's very easy we divide both sides okay by a and we can get our solution right we would say x is equal to b over a well it's not going to work out that way for us because we can't divide a matrix by another matrix when we work with kind of matrices that operation is undefined so we have to have a different way and what we're going to do is use some of the knowledge from the kind of previous lessons and we're going to say that we know if we take a inverse and we multiply it by a i'm going to get an identity matrix of the same order okay so if i took a inverse multiplied by a i would get i sub 2 right a 2 by 2 identity matrix so let's just go ahead and do that real quick i'm going to slide this down here i'm going to say i have a inverse times this on this side and i've got to make sure the order because it's going to be very important to make sure the order is correct so i'm going to slide this in on the left and what happens is on this guy a inverse times a gives me an i sub 2 and this is times x and this equals a inverse times b now stay with me for a second we have x which is two rows by one column and we have i sub two which would look like this right it would be a one's going down the diagonal zeros above and below so this is my i sub 2. now if i multiply i sub 2 by x first off is that legal this is a 2 by 2 this is a 2 by 1 so yes it is and i would get x back okay and you can pause the video and do that real quick you can multiply this by this you will get x back so really what i can do at this point is i can just erase this because it doesn't really do anything it's just like if i had 1 times 5 and i said ok well this is just equal to 5. that's all i'm doing there so at this point i can say that x this unknown right here is going to be equal to a inverse times b okay so that's where we're kind of going with this and really quickly we know that we can do the operation this way because a inverse is going to be a two by two and b is going to be or is i would say a two by one so mathematically we can multiply these together if you flip the order though okay if i did it like this if i said i had b times a inverse you're going to run into a problem right because this is a 2 by 1 and this is a 2 by 2 you cannot do that operation okay so you've got to make sure that when you're setting these equations up you pay attention to kind of which order you're multiplying everything in all right so what i'm going to do now is just kind of copy these we're going to go to a fresh sheet because we're going to run out of room we'll paste this in here and i'm going to get rid of this for a second what i need to find right now i'm just going to get rid of this too i need to find a inverse to start this off so i need to find a inverse and then i'll multiply it by b and i'll know what x is so how do i find a inverse again if you didn't watch the last lesson let me give you the shortcut real quick so if we have a and let's define the entries to be lowercase a lowercase b lowercase c and lowercase d what happens is if i want a inverse all i need to do is put 1 over the determinant of a and we haven't talked about this yet but it's very easy to find this it's basically just a times d so these two multiplied together so a times d minus these two multiplied together b times c okay that's very easy then multiplied by what i'm going to do here might not make a whole lot of sense but we'll kind of get to this later on so a and d would swap so d and a notice how those positions swap and then b and c these guys are going to become the opposite so negative b and negative c okay so this is something you probably want to write down for your notes because we're going to use it a few times today and the main thing here is that don't be frustrated if you can't memorize this with kind of one go i've been using this forever so i already know it but it's one of those things that after you practice enough times it just gets committed to memory so write it down use it when you're solving your problems and you'll memorize it very quickly all right so let me just erase this i can do this from memory and so i'll say a inverse is equal to but it's going to be 1 over the determinant so this guy times this guy 8 times negative 4 is going to be negative 32 and then minus negative 7 times negative 3 is going to be positive 21 if i did negative 32 minus 21 that's going to give me negative 53 okay so that's the first part of the formula then we have to multiply it by remember we have all this stuff that goes on so these two switch so the position of this one and this one so this becomes up here and this one becomes down here and then these guys get changed into the opposite so instead of a negative seven i get a seven instead of a negative three i get a three so from this point i'm just multiplying a scalar by a matrix so very very simple let's just do this down here real quick and then we'll replace everything so negative 1 over 53 times negative 4 would just be 4 over 53. okay so that's going to be your first entry so it's going to be 4 over 53. so let me erase this and i'll put this over here so 4 over 53. we kind of slide that down because we need some problem so a inverse is going to be equal to this then the next one is negative 1 over 53 times 3 this is just going to be negative 3 over 53 and then the next one is going to be times 7 so obviously that's just going to be negative 7 over 53. and then the last one is going to be times 8 so that's just going to be negative 8 over 53 okay so let me erase this from here and let me try to make this a little bit more compact because we're going to run our rom so we're going to have 4 over 53 we're going to have negative 3 over 53 we're going to have negative 7 over 53 and we're going to have negative 8 over 53. all right so that's about as good as i can do now what we said and i have to go to kind of the next one i can erase a i don't even need it so what we need to do let's go back up we know from this equation and i can just erase all this we don't even need this anymore that x is equal to a inverse times b here's b let me grab this real quick and again you got to do this in the right order so a inverse is first b is second so let me put this in here okay so we're going to say that we have a inverse times b again this is a two by two and this is a two by one so we can do this these numbers match and it's going to be a two by one so a two by one so you're gonna have one there and one there so let's go ahead and crank this out real quick we want to start out with this kind of entry here and that's going to come from this row times this column so you're going to have 4 over 53 times negative 17 that would be negative 68 over 53 so negative 68 over 53 and then you'd have plus you have this guy times this guy so negative 7 over 53 times 13 so that's going to give me negative 91 okay negative 91 over 53. so if i add negative 68 and negative 91 i get negative 159 so this would be negative 159 over 53 and if i do that kind of calculation i'm going to get negative 3. okay so this first entry here is going to be negative 3. all right so now i just need this entry down here and all i'm going to do is get kind of this row times this call okay so negative 3 over 53 times negative 17. we know negative 3 times negative 17 would be positive 51. so this is positive 51 over 53. and then you're going to do this guy which is negative 8 over 53 okay times this guy which is 13. so that's going to give me negative 104 okay so negative 104 over 53 can add these two together so you're basically doing negative 104 plus 51 which gives me negative 53. so you know what this is going to be it's negative 53 over 53 which is negative 1. okay so at this point we've found our answer a inverse times b is x so let me just copy this real quick or i can copy the whole thing okay so let me paste this in here real quick and we know that a inverse times b is x so let me just kind of replace this and i can replace this and i'll just drag this up here so we have our solution right this guy represented x this guy represented y so we can say that x is negative three and y is negative one okay so let me erase all of this and we can just check this real fast i know this process is very tedious and again it's not a recommended method if you used any other method you'd be done a long time ago right but it's just something they teach so we do want to cover all right let's check this real quick if i plug in a negative 3 for x i get 8 times negative 3 which is negative 24 and then negative 7 times negative 1 would be plus 7. does this equal negative 17 yes it does and then for this one you get negative 3 times negative 3 which is 9. negative 4 times negative 1 would be 4 so plus 4 equals 13 that works as well all right let's take a look at another example so we have 3x minus 9 equals 3y we have negative 8y equals 48 plus 16x so in this case we have to do some cleaning up right we can't just start because everything we get messed up you want every equation in a x plus b y equals c and here you have some opportunities to kind of work with smaller numbers so you want to take advantage of that i'm going to for the first equation i'm going to subtract 3y away from each side and i'm going to add 9 to both sides and then i'm going to divide everything by 3. so this is going to give me x minus y equals 3. so this is going to be my first equation let me just kind of drag this up here and we're going to bring this to another sheet so don't even worry about this right now so i'm going to subtract 16x away from each side here so minus 8y and this equals 48. for this one everything is divisible by 8 right because you divide this by 8 you get negative 2 then times x you divide this by 8 you get negative y and this equals you divide this by 8 and you get 6. so you can say this is negative 2x minus y is equal to 6. okay so you can look at that and say that hey i could solve that with substitution in probably like less than a minute but we're going to do this using the kind of inverse of a matrix method which will probably take us about 10 minutes all right let's go ahead and paste this in and let's set everything up so the first thing i want is matrix a which is going to be the coefficients so you have a 1 and a negative 1. you have a negative 2 and you have a negative 1. okay then i want my matrix x which is going to be again a column vector with x and y then i want my matrix b okay let me write that here b is going to contain a 3 and a 6. okay again another column vector so we already know that this sets up to be a times x is equal to b we've already shown that that's legal and then we also know that if i multiply this side over here by a inverse and again slide this down as long as i do it on the left side i am good to go right a inverse times b because again this is a two by two this is a two by one so this way is legal if i flip it around it's going to be illegal okay it's not going to be defined so a inverse times a is the identity matrix and again it would be a two by two and this is a two by one so that's legal so i would get x back so x would be equal to this so i need to find the inverse of a multiply by b and again i'll have my solution for x so let's copy a real quick and go to the next sheet and we're going to quickly find the inverse now that we know the method so again it's 1 over the determinant multiply this by this 1 times negative 1 is negative 1 then minus this times this negative 1 times negative 2 is positive 2. if i have negative 1 minus 2 that's going to be negative 3. so this is negative 1 3 times interchange these two so what i want to do again this one and this one they change positions so this becomes negative one and this becomes positive one and then what i'm going to do for these for this one and this one i change the signs so this one becomes positive one and this one becomes positive 2. okay so again when you see that it might not make sense the first few times but then after you practice you're going to very quickly memorize this kind of formula it's a very good shortcut all right so if i multiply everything by negative 1 3 i'm going to get basically positive 1 3 here i'm going to get negative 1 3 here i'm going to get negative 2 3 here and i'm going to get negative 1 3 here so this is the inverse of a so this is a inverse and let's just move this over here and i'm going to erase a at this point i don't need it anymore it serves me no purpose i need to go back up and get b so let's copy this and let's go back down and paste this in and so now i want a inverse times b again in that order because it's only defined in that order so a inverse times b again this is a two by two and this is a two by one so the dimensions will be two by one okay so let's go ahead and set this up we do this row times this column so you have one third times three and then plus you're gonna have negative one third times six so what's that going to give me one third times three is one and then you have negative one third times six this cancels with this and gives me 2 negative 1 times 2 will be negative 2 so this is negative 2. so what's 1 plus negative 2 that's negative 1. so this is negative 1 and then for this one and i shouldn't put that bracket in there for this one i'm going to go this row times this column so negative two thirds times three plus negative one third times six and of course this is going to cancel you'll have negative two and this cancels with this and gives you two negative one times 2 is negative 2 and so this is going to give me negative 4 right negative 2 plus negative 2 is negative 4. so we found our solution we know x is negative 1 and y is negative 4. so let's go ahead and copy this real quick just go back up and prove that to you real fast so let me just paste this in here real quick so we know that a inverse times b is x so let me erase this and i'll just slide this down okay so we can record our solution we know this one is x so x is negative one we know this one is y y is negative four just going to erase everything at this point we don't need it anymore it's going to list our solution and be done so we'll say that this is negative 1 comma negative 4 as an ordered pair again if you want to check i highly recommend it when you start negative 1 minus a negative 4 so plus 4 equals 3. yeah that works out and then this one is negative 2 times negative 1 which is 2 and then minus a negative 4 is plus 4 does that equal 6 yes it does so that's the correct solution all right so let's wrap up our lesson and look at a more challenging problem or you could say a more tedious problem we're going to look at a linear system with three variables and again we're going to solve it by kind of finding the inverse of one of the matrices that we set up now again i just want to emphasize this this is definitely not the best method to do it it's just something we're showing you because you will probably be tested on it all right so the first thing you want to do if you get a three variable system again again again you've got to make sure these equations are set up correctly first if you don't it's going to give you the wrong answer right so you've got to make sure that the x's are lined up the y's are lined up the z's are lined up the constants are lined up so that when you load the information into your kind of matrices that you're setting up everything corresponds correctly okay so you're going to have a column with the coefficients for x's then a column with the coefficients for y's you know so on and so forth so in this format we have everything as ax plus by plus cz equals d so we're good to go but again if it wasn't like that you'd have to set it up correctly now what i'm going to do is i'm going to make a coefficient matrix just as i always have i'm going to call it a and so i'm grabbing all the coefficients so this has an implied coefficient of 1. this is going to be negative 3 this is going to be positive 2. so that's from the first equation that's this one that's this one and that's this one okay from the second equation i would have a two i would have a negative five and i would have a positive three okay so again just to highlight this from here here and here okay and then from the last one i've got a three a negative 6 and a 4 and again that's from this one from this one and this one okay so all i did was i set up a coefficient matrix and i'm just calling that matrix a now next i'm going to set up a matrix that's going to represent the variables of the system we're going to call that capital letter x and so that's going to have three variables x y and z so x y and z so this is just a column vector and then lastly i'm going to have my matrix b okay so i'm going to write that up here so b is going to be a column vector with my constants so again it's going to be negative 24 it's going to be negative 39 and it's going to be negative 40 not okay so we know from the previous two examples that if i do a the coefficient matrix times x the variable matrix i can set this equal to b right this kind of matrix with my constants in it and that would represent kind of our system right so you can prove that to yourself you can stop the video and you can multiply a times x and set this equal to b and you'll see that you get the same thing all right so what we want to do now is kind of set up the solution for this and we already know how to do it so let me kind of slide this down just a little bit so we have a little room to work it's going to be pretty tight so we already know this is going to set up as a inverse times a times x and you don't need those dots there but i'm just putting them there for emphasis is equal to a inverse times b okay now we know that a inverse times a is going to give me an identity matrix and in this case it's going to be a 3 by 3. so this would be an i sub 3 times x is equal to a inverse times b now we already know that if i multiply a 3 by 3 identity matrix by a 3 by 1 matrix x i'm just going to get x back okay so i can really erase this i don't need it anymore and i can just declare that x is equal to a inverse times b of course we already knew this from kind of previous examples but again it's just good to go over it again so really all i need to do here is find a inverse multiply it by b and then i'll have my values for x y and z okay so let's copy a so let's find the inverse first that's going to be the hardest part all right so what we want to do right now is find the inverse of a again we talked about this in the last lesson for a two by two matrix if it's got an inverse it's very easy to find there's a shortcut for a three by three there's a variety of methods that you can use we'll talk about another method kind of later on after we talk about determinants for now it's a little bit beyond us but the method that we know the method that we learned in the last kind of lesson that works for any kind of invertible square matrix what we do is we set up an augmented matrix and it's going to look something like this okay so let's say a is an n-by-n matrix in this case it's a 3x3 and then you have i which is the identity matrix in this case it would be a 3x3 so you're setting up an augmented matrix where a is on the left the identity matrix is on the right you're going to use row operations to put it in this form so you want the identity matrix on the left and your a inverse is going to be on the right and again we talked about why this works in the last lesson so let's go ahead and set this up real quick and we'll get our solution so i'm going to start with just this guy right here i'm just going to erase the a equals i'm going to put a vertical bar here and i'm going to put a 1 0 0 a 0 1 0 and a 0 0 1 okay so this is just the identity matrix on the right and this guy on the left is our a the matrix we're trying to find the inverse of so we're going to use our row operations to put this side you could say in reduced row echelon form or we could say it's going to become an identity matrix it's just going to match this over here so the very first thing we want to do if we follow the method that's given in every textbook is we want to get a 1 there we already have that so we don't really have to do that and then based on that 1 we're going to get our zeros so we want a 0 here and we want a 0 here okay so what i'm going to do is i'm going to multiply top row here by negative 2 and i'm going to add the result to row 2. so i'm going to notate that by saying i have negative 2 times row 1 and i'm going to add the result to row 2 that's what i'm going to replace row 2 with so let me just go through and multiply negative 2 by everything in row 1 so negative 2 times 1 is negative 2 negative 2 times negative 3 is positive 6. negative 2 times 2 is negative 4 negative 2 times 1 is negative 2 and then negative 2 times 0 is going to be 0 in each case so for both of those okay now what i'd want to do is add each of these to the kind of entries in row 2 that's what i'm going to replace row 2 with so negative 2 plus 2 is 0. then 6 plus negative 5 is 1 then negative 4 plus 3 is negative 1 then negative 2 plus 0 is obviously negative 2 and then in each case i'm adding 0 to something so it's not going to change so that part's done so now i move to this row right here and i want this to be a 0. so what i do is i multiply row 1 okay row 1 by negative 3 okay and then i add the result to row 3 that's what i'm going to change row 3 with okay so let me just multiply everything by negative three first get that out of the way negative three times one is negative three negative three times negative three is positive nine negative three times two is negative six negative three times one is negative three and of course negative three times 0 in each case is 0. okay so now we're going to add each of these to the entries in row 3. so this is going to be a 0 9 plus negative 6 would be 3. this is going to be a negative 2 right 4 plus negative 6 would be negative two and then in each case here and here i'm adding to zero so it's just going to be negative three and zero and then here zero plus one is just going to be one right so this is going to be my row three so now we're going to move to this column here and conveniently we already have a 1 again so we don't have to do anything there we just need to use that 1 to get our 0 here and our 0 here okay so i'm going to multiply row 2 by i'm going to get rid of this one first i'm going to make this a 0. so i'm going to multiply row 2 by positive 3 and i'm going to add the result to row 1. that's what i'm going to replace row 1 with okay all right so let me multiply everything by 3 so i'm going to go ahead and say 0 times 3 is 0 and then 1 times 3 is 3 and then 3 times negative 1 is negative 3 3 times negative 2 is negative 6 3 times 1 is 3 and then 3 times 0 is 0. so let's add everything so zero plus one is still one three plus negative three is zero negative three plus two is negative one negative six plus one is negative five and then we have three plus zero which is three and then obviously zero plus zero is zero so that takes care of that now we move into kind of making this into a zero so multiply row two by negative three add the result to row three that's what i'm going to change row three with so negative three times zero is zero negative three times one is negative three negative three times negative one is three negative three times negative two is six negative three times one is negative three and negative three times zero is zero so let's add now zero plus zero is zero so i don't have to do anything there negative three plus three is zero we have three plus negative two which is positive one now we just have six plus negative three which is positive three we have negative three plus zero which is negative three and of course 0 plus 1 is 1. alright so looking pretty good so far as we move to the next column again conveniently we already have a 1 so we just need to get a 0 here and here now we have the same number in each case we have a negative 1 and a negative one so whatever i do to make this into a zero we'll also make this into a zero so we can kind of do this as one step i'm gonna break it up into two just so you don't get lost but you could do it as one step so really what i need to do is just multiply row three by one okay or you could just say you have row three and add the result to row two that's what i'm going to replace row two with and again it's going to be the same thing i'm gonna multiply row three by one and add the result to row 1 that's what i'm going to change row 1 with again because in each case 1 plus negative 1 would give me 0 there now if you want to write a 1 there just for emphasis that's fine but it's the same thing either way so let's start with kind of this one i'm just going to add the entries in row 3 to row 2 that's what i'm going to replace row 2 with so 0 plus 0 is 0 no change there 0 plus 1 is 1 no change there 1 plus negative 1 is 0 okay and then 3 plus negative two is going to be one negative three plus one is going to be negative two and then one plus zero is one okay so that one's done and then lastly we're going to have zero plus one which is one zero plus zero which is zero one plus negative one which is zero and then we have three plus negative five which is negative two we have negative three plus three which is zero and we have one plus zero which is one okay so now we're finished and essentially you see that you have the identity matrix you have an i sub 3 here on the left and you have your a inverse on the right okay so we can just copy that real quick a inverse is going to be equal to you're going to have this negative 2 0 and 1 then you're going to have 1 negative 2 and 1 and then to make my bracket big enough and then 3 negative 3 and 1. so let me copy this real quick and we'll go to a fresh sheet and let's paste this in here real fast all right so let's get rid of all this now we need to let me kind of go back up we need to multiply a inverse which we just found times b and that's going to give me x these three entries here that are going to give me the solutions for my system so let me copy b real quick and let me paste this in and remember we've got to do this in the correct order if i do a inverse times b it's defined if i do b times a inverse it's not defined okay it's not going to work so a inverse times b is going to be equal to what well again i know that this is a three by three and this is a three by one so it's going to end up being a three by one so you can have one two three entries here let me make that a little better so for the first entry let me kind of slide this up just a little bit for the first entry what i'm going to have is this first row times this column so in each case let me just kind of set this up so we can blow through this in each case when i'm multiplying a row by a column this isn't going to change so in other words i'm going to have negative 24 i'm going to have negative 39 and i'm going to have negative 49 okay so let me just kind of wrap these and i'm going to copy this real quick and i'm going to paste it twice right there and i'm going to paste that again just to save us a little bit of time okay so what's going to happen is i'm going to multiply again this row by this column so you'd have negative 2 times negative 24. so let's put negative 2 here and plus you're going to have negative 39 times 0 and then negative 49 is going to be multiplied by 1 okay so we can do that real quick negative 2 times negative 24 is 48 this is 0 okay so you have 48 here this is going to be negative 49 so this is going to be negative 1. so that is your first entry so this is negative 1. then for this one again you're doing this row times this column so 1 times negative 24 is negative 24 then plus you have negative 2 times negative 39. we know that's positive and 2 times 39 is 78. so let's just say this is 78 and then you have this 1 times this negative 49. so that's just negative 49. so what does this come out to if i do negative 24 plus negative 49 i get negative 73. if i add that to 78 i get positive 5. so this will be 5. and kind of get rid of these bars at this point i just have this one left so this row times this column so you have 3 times negative 24 that's negative 72 okay so negative 72 then plus you'd have negative 3 times negative 39 which is positive 117 so let's put 117 and then lastly you have 1 times negative 49 so let's put plus negative 49. okay so what is negative 72 plus negative 49 that's going to give me negative 121. so i'm going to put plus negative 121. and if i do 117 plus negative 121 i get negative 4. okay all right so again it takes a lot longer to do it this way i think it's kind of cool if you do this just a few times just to kind of get the concept down but then again it's not an efficient way if somebody gave you the inverse to start then maybe it'd be pretty quick to use this because all i'm doing is some very simple matrix multiplication to get my answer but if you don't have the inverse if you have to calculate it even with the shortcut that we're going to learn kind of in a few lessons it's still going to be a very tedious process and it's going to be easier to use other methods like cramer's rule which we'll be talking about here shortly all right so let me kind of copy this and let's go back up and i don't know where i'm going to fit this let's just kind of paste this in here for a second and let me just erase some stuff here we know that x is equal to this so i'm just going to erase this and just kind of slide this down we know that this first entry was x this was y and this was z okay so x is negative one y is five and z is negative four so let's erase everything now we need anything anymore we've found our solution so as an order triple i'm going to say it's negative 1 it's 5 and it's negative 4. okay and we can just check this real quick just to make sure we didn't make a mistake so in this first one you'd have negative 1 you would have minus 3 times 5 then plus 2 times negative 4 should be equal to negative 24. well if i do my multiplication first this is 15 and this is negative 8 so you can say that you have negative 1 minus 15 which is negative 16 minus 8 which is negative 24. so this one's good all right for the next one we have what we have 2 times negative 1 which is negative 2 minus you have 5 times 5 which is 25 plus you have 3 times negative 4 which is negative 12. this should equal negative 39 negative 2 minus 25 is negative 27 and if you add negative 27 and negative 12 you do get negative 39. this one's good to go so for the last one you have 3 times negative 1 which is negative 3 and then minus 6 times 5 which is 30 and then plus 4 times negative 4 which is negative 16. this should equal negative 49 and of course it does right if you do negative 3 plus negative 30 you get negative 33 and negative 33 plus negative 16 is negative 49. so it works in all of them again not the fastest way to kind of go about this but it's good to have the concept down it's good to understand how you can kind of solve something using this method again if somebody gave you an inverse to start it would be a decent kind of technique to use but if they didn't give it to you it would probably be a much slower method to use in this lesson we want to talk about how to find the transpose of a matrix alright so finding the transpose of a matrix is a very very simple concept we're just going to jump right into an example so suppose i have matrix a and it's a 2 by 2 matrix 2 rows and 2 columns with the first row being negative 9 and 8 and the second row being 4 and 2. so if i want to find the transpose of this matrix a i would take that name a and i would put a superscript t up here and it's going to be equal to all we need to do is take the first row which again consists of negative nine and eight and make that the first column in the transpose so this will be negative nine and eight like this and then similarly the second row will become the second column so four and then two so basically when you see the definition for this in your textbook what they're trying to tell you is that the transpose of a matrix is formed by kind of swapping or you could say interchanging the rows and columns so in other words if i kind of go back through this this first row became this first column this second row became this second column okay or you could also say this first column became this first row this second column became the second row so it's very very easy to do this let's take a look at another example so here we have our matrix a and we want to find the transpose now this is a good point to stop in the last example we saw a two by two matrix which is a square matrix so when you look at the order of the transpose and you look at the order of kind of the matrix you started with it's going to be the same but if you look at kind of the order of the transpose of a non-square matrix it's going to be reversed so what do i mean by that well this is a two row and three column matrix it's a two by three well what happens is when i find the transpose the a with the superscript t it's going to be a three by two because this row will now become a column and this row will now become a column so it's going to have three rows and two columns so let's see this real quick so i'm going to take this guy right here and make it a column so negative 1 5 and negative 7. so the first row became the first column and then i'm going to take this second row and make it my second column so it's going to be 0 8 and 9. okay it's just that simple and now let me kind of move this over here so this is a 2 by 3 and this guy again the order is reversed so it's a three by two so again just to go through this this first row became this first column this second row became the second column or you could also say that this first column became this first row this second column became the second row and this third column became this third row let's look at one more for the final one again we have our matrix a and this guy is going to be a three by four right it's a three by four we've got three rows and four columns so again when i find the transpose that order is going to be flipped it's going to be a 4 by 3. so the transpose of a again we're going to put a with a superscript t here is going to be equal to again the way i always do it is they just take the row the first row and make it the first call that's how i go about it i take the second row make it the second column third row make it the third column but again it's just as valid we'll just kind of do this in the opposite way now let's take the first column and make it the first row so it would be 5 11 and then 5 okay and then take the second column make it the second row so 9 3 and 6 and then take the third column make it the third row so 0 negative 2 and 0. let me kind of slide this up just a little bit and then for the last kind of column we have here the fourth column it'll be the fourth row so you're going to have negative 4 you're going to have 44 and you're going to have 19. okay so that would be your transpose of your matrix a okay so remember the first row became the first column the second row became the second column and the third row became the third column or again you could say the first column became the first row the second column became the second row the third column became the third row and the fourth column became the fourth row in this lesson we want to talk about how to find the adjoint of a matrix so when we talk about the adjoint of a matrix which is also referred to as the classical adjoint if you're in a linear algebra course or the adject it's found using kind of two easy but i would say tedious steps so the very first thing you want to do is make a matrix of cofactors now we talked about cofactors when we did our lesson on determinants and specifically when we looked at our method known as the kind of laplace expansion method or the kind of expansion by cofactors method now after you've kind of got your matrix of cofactors the next thing you want to do or the second part of the process is to find the transpose of that cofactor matrix so like i said it's two easy steps because you're basically just doing some arithmetic but it's quite tedious especially if you get into something like a three by three a four by four five by five these are kind of matrices that when you find the adjoint for them it takes a little bit of time so without further ado let's just look at an example real quick and we're just going to start out with something that's very generic and i'm going to show you that you've seen the adjoint of a matrix already you just didn't know it so first we have our matrix a and it has a and b in the first row and c and d in the second row now we already said that the inverse of a the little shortcut method was 1 over the determinant of a okay so that's why i have those vertical bars surrounding that a there times it's going to be the adjoint of a okay so this is the adjoint of our matrix a what we're going to do is we're going to swap these two so i'm going to swap a and d so d comes up here a goes down here and then these two the c and the b i'm going to make them into their opposite so i'm going to put a negative b and a negative c just to say hey whatever we have in those positions we're going to take the negative of it or just change the sign now this is the a d j of a or again the adjoint of a or the adjective of a however you want to think about that so where did this kind of come from well again what we could do is we could start out with this guy right here we could make a matrix of cofactors and then we can take those cofactors that matrix and we can find the transpose of that guy and it will give us this exactly so to see that real quick let me just kind of copy this let's go to a fresh sheet and i'm just going to paste this in and let me just kind of clean this up a little bit so what we're going to have here i'm going to put a d j of a is equal to this okay so our original matrix a was what it's a b c and d and the reason i'm going through this generically is that once you kind of look at this you'll know the shortcut for two by two to find the kind of adjective or again the adjoint and you can quickly do it with a 2x2 just from that shortcut when you get to a 3x3 or higher you're going to have to do more work and i'll explain why in a little while so the very first thing with this guy is to make a matrix of cofactors now when we talk about a co-factor remember there's kind of two parts to it the first part is you have to calculate a minor okay so how do we find the minor well let's say i wanted the minor of this element here i would delete the row and i would delete the column and i would look at the determinant of the matrix that's left well if you do this with a two by two the determinant of the matrix that's left is just the entry that's left because the determinant of a one by one matrix if i wanted the determinant of this it would just be this itself so when you work with these you're just grabbing the entry that's left that's all you have to do but if you go into a three by three or higher you're going to end up having to actually go through the work of finding a determinant because it's not just going to be the single entry that's left okay so what we want to do is say that the minor and i'm going to notate this as m sub 1 1 because again this is row 1 this is row 2 this is column 1 this is column two this is telling me i have the minor element for row one column one so it's this entry right here and what is it equal to again if i mark out this row mark out this column it's the determinant of what's left in this case it's just d okay so that is how i find the minor but that is not the cofactor there is an extra step and there's a few different ways you can do this let me show you the long way and then i'll show you how to make like a little sign table so you can speed this up so the cofactor c sub 1 1 okay so of this entry right here you take the minor and you multiply it by negative 1 raised to the power of the row plus the column so in this case you can get that information from here so 1 plus 1 is 2. now notice how if you're raising negative 1 to an even power you're going to get 1 and the sine of the minor is not going to change so this guy is equal to positive 1 times d which is just d so the cofactor for this kind of entry in row one column one is just going to be d so let's erase all of this and let's put that c is equal to okay we're just going to put a d here to start now for the rest of these if you want to kind of make it a little bit quicker for yourself when you set up the kind of cofactor matrix instead of having to go through and multiply negative 1 raised to the power of the row plus the column each time you can predetermine the signs you'll see a lot of different sign charts out there and basically they alternate so again if i look at the kind of row and the column i'm in in each case this is a row number of one and a column number of 1 right here 1 plus 1 is 2 2 is an even number so we know negative 1 raised to an even number okay is going to be positive 1 so the sign is going to be positive or just stay what it is okay that's what that means then if i go here it's going to be row 1 column 2 1 plus 2 is 3. it's negative see how it alternates then if i come down here it's row 2 and column 1 so it's 2 plus 1 which is 3 which is odd okay so again that's going to be negative and this last one is row 2 column two so two plus two is four that's even so it's positive so if you have this as a little reference you can find your minor and just attach the sign that way so you have to go through multiply by negative one raised to kind of the power of the row plus the column each time so let's just keep this here for reference and let's just burn through the rest of this so if i want the minor for this guy again all i do is i delete the row and the column it's in and i take the determinant of what's left in this case it's going to be c and for this one let's delete the row in the column and i'm left with b and for this one i'm going to delete the column and the row it's in and i'm going to be left with a now we already did the sign change for this one there was no sign change but for this one notice how again there's a sign change so i'm going to put a negative here for this one there's a sign change so i'm going to put a negative here and for this one right here there is no sign change so when you have a plus you don't change the sign because the minor and the cofactor are the same but when you have a minus you do change the sign so you want the negative of whatever's there so this is the cofactor matrix now we've taken all the minors and applied these sign changes when they were necessary now to get the adjoint you'll notice that this is still not the same the entries along the diagonal are same but these two are not so what you have to do is take the transpose of this guy so the transpose of c of c and we put that capital letter t the superscript above our kind of letter c there to show this all we need to do this is a very simple process we take the first row and make it the first column so we'll take d and negative c then we take the second row and make it the second column so negative b and a so basically if you've never taken a transpose before you're just interchanging the rows and columns so i took the first row and made it the first column the second row became the second column now another way you could do this you could say that the first column became the first row and the second column became the second row so however you want to do that so i'm done with this matrix here and basically at this point i can say that the adjective or the adjoint of our matrix a is equal to the transpose of our matrix c which is this guy right here and you'll notice that these two match up perfectly you've got a d and a d you've got a negative b and a negative b a negative c and a negative c and an a in a so what happens is when you work with the two by two matrix you can always use this little shortcut by just remembering the generic entries here and remembering the adjective or again the adjoint turns into this one so these two switch and these two on the kind of opposite diagonal are going to be their opposite so let's look at an example real quick so we have our matrix a and it's equal to we have negative 1 and 3 in the first row and 5 and 2 in the second row so what we can do here again if it's a 2 by 2 it's very easy we can just use our shortcut so i can switch these two so i can say this is 2 and this is negative 1 and i can say that these two are going to be their opposite so this is negative 3 and this is negative five so look how fast and easy that is using that shortcut unfortunately in a moment when we start looking at three by threes we've got a lot more work to do but let's just do this the slow way real quick just so we get a little bit of practice before we get into the three by three and we have something more difficult so if i want the adjective of this guy the slow way i would find a cofactor matrix first and again let me just set up the signs so in this kind of first row first column here again it's row one column one so one plus one is two that's even so there'll be no sign change and then it alternates so this guy's gonna be negative and then down here it will be negative and here will be poly again just look at the sum of the row number and the column number row one column one one plus one is two two is even so that's why it's positive if it's row one column two one plus two is three it's odd so that's why it's negative okay so you can just look at that sum the row number and the column number if it's odd it's going to be negative okay if it's even it's going to be positive meaning there's no sign change to your minor all right so let's go through this real quick if i want the minor of this guy delete this row delete this column we want the determinant of this which is just this number itself so it's going to be 2. so let's write our 2 in there and i'll put it in a different color so let's move on to this one now so i'm going to delete the row and the column i'm left with just 5. again you take the determinant of that it's just that but notice how i put the negative 5 there i want the opposite of this because there's a negative sign there okay i'm basically just multiplying it by negative 1. then as i move down here i'm going to delete the row and the column i'm left with just 3. the determinant of that would obviously be 3 but again there's a negative there so i want negative 3 the opposite of that number then lastly for this entry here i want to delete the row and the column i'm left with negative 1. so obviously the determinant of that is negative 1. okay so we had a plus there so there's no sign change again again again i can't stress this enough because this is what confuses people if you end up with kind of a negative sign there if you're looking at a sine table take the opposite or you could say multiply by negative 1 by what you get for your minor if it's a plus there leave the minor alone there's no sign change needed all right so now that we have this we're not quite there yet because we have to take the transpose so the transpose of c is equal to the adjective or again the adjoint of a which is equal to what well let me kind of move this out of the way because we're going to run out of room so i'm going to define this down here so what we would do is take this first row and make it our first column so 2 and negative 5 the first row became the first column and then the second row will become the second column so negative 3 and negative 1. so you'll notice this matches our shortcut exactly 2 and negative 3 in the first row negative 5 and negative 1 in the second row okay let's look at an example that's a lot more tedious but again the concept is the same so we have a 3x3 here and obviously what we want to do is start out by making a matrix of cofactors then find the transpose of that and that's going to give us our adjoint okay so c is going to be equal to that's my cofactor matrix i'm just going to go through and put some signs in so we don't have to deal with them so this guy is row 1 column 1 so it's plus and then all the signs are going to alternate so it's going to be minus and then plus okay and you can go through and see that for yourself but then going down here this is going to be minus plus and minus and then plus minus plus so the signs always alternate okay so now what i want to do is go through and find those entries so i'm going to start with this one right here i'm going to mark out the row and the column and take the determinant of what's left okay so here's where it's more work because before with the two by two you really didn't have to you do take the determinate but the determinant is just the number that's presented because you only have one number here i actually have a two by two matrix that i have to find the determinant of so it's much more work so i'm going to do two times zero which is 0 minus you have 3 times negative 1 which is negative 3 0 minus a negative 3 is 0 plus 3 which is 3. okay so no sign change here so this will just be 3. so let's go through the next one so we want this one so i'm going to mark out this column and this row so i would have 5 and negative 4 and negative 1 and 0. and i'm just writing this out because it's kind of hard to see this with kind of this highlighting in between so the determinant here would be 5 times 0 which is 0 minus you have negative 1 times negative 4 which is 4. so this is negative 4 but you have a sign change there so this is positive 4. okay all right so let's go to the next one so now we're here we're going to mark out this column in this row so what we want to do is find the determinant of this 5 times 3 is 15 then minus you have 2 times negative 4 which is negative 8. 15 minus a negative 8 is going to give me positive 23 and there's no sign change here so we'll put just 23. okay all right let's erase this and let's do this one so i'm going to delete this row and i'm going to delete this column so what we're going to have here is 1 and negative 6 and then 3 and 0. so we want the determinant of this 1 times 0 is 0 minus 3 times negative 6 is negative 18 again minus a negative is plus a positive so this would be positive 18. so we're doing a sign change here because it's right here where there's a negative so this is going to end up being negative 18. so let me write that in negative 18. all right let's go to the next one now so we're going to do this one so i'm going to delete this column in this row so what do we have we have 3 and negative 6 then negative 4 and 0. so the determinant of that 3 times 0 is 0 minus negative 6 times negative 4 is 24. so we have 0 minus 24 which is negative 24 and there's no sign change there so it's just negative 24. all right let's go to the next one so now we have this one right here so delete the column delete the row what's left we have 3 and 1 negative 4 and 3. so the determinant of this guy 3 times 3 is 9 minus 1 times negative 4 is negative 4. 9 minus a negative 4 is the same as 9 plus 4 which is 13. but again you've got that negative there so you've got to do the sign change so this is negative 13. all right let's look at this final row so we have this guy right here so i'm going to delete the column i'm going to delete the row so what i have left is this guy right here so the determinant of that 1 times negative 1 is negative 1 then minus you have negative 6 times 2 which is negative 12. again minus a negative is plus a positive so it's negative 1 plus 12 which equals 11. no sign change so it's just going to be 11. and let me change colors there so now we're here so we're going to delete this column in this row so we would have 3 and 5 negative 6 and negative 1. so the determinant of this guy 3 times negative 1 is negative 3 minus negative 6 times 5 is negative 30 and this equals we would have negative 3 plus 30 again minus the negatives plus a positive so this is going to be positive 27. now when we look at this we have to do a sign change so this will be negative 27. all right last one so we want to find the cofactor of this guy so delete this row and this column so what do i have i would take the determinant of this matrix 3 times 2 is 6 minus 1 times 5 is 5. so this is 1 and there's no sign change here so this is just going to be 1. all right so now i have my cofactor matrix or my matrix of cofactors at that point it's pretty easy i'm going to say that the transpose of this cofactor matrix is equal to the adjoint of a which is going to be equal to what we're going to transpose this guy so i'm taking this row and making it this column so let me use a different color we're going to have 3 4 and 23 again first row becomes first column then the second row becomes the second column so negative 18 negative 24 and negative 13 then the third row becomes the third column so 11 negative 27 and then 1. okay now if you want you can delete this notation because you're done with it if you're asked for the adjoint just give them this you don't need to tell them it's the kind of transpose of the cofactors because they already know that okay so this would be your answer your adjoint or your adjective of a all right let's go ahead and take a look at another one it never hurts to practice a little bit more if this is something you're already comfortable with you can already start doing your practice problems otherwise let's go ahead and knock this guy out so we want to find the adjective or again the adjoint of a so the first thing i want to do is find the kind of cofactor matrix and the first thing i'm going to do again is just make a matrix of sines because i know that that's going to help me right i don't have to go through and do negative 1 raised to the power of the row plus the column each time so again if you can just remember that this first one is positive because it's in the first row and first column 1 plus 1 is 2 raising negative 1 to that even power gives you positive 1 you just alternate from there so this is negative this is positive then you would come down here and alternate so this is negative positive negative come down here and alternate positive negative positive very easy to remember that this first one's positive and that they alternate so if you remember that then you're basically good to go make your first row then drop down here and say okay if i'm alternating it was positive now it's going to be negative and you just alternate that's how you can keep going so once we have this kind of set up we just go through and get our minors and then apply the sign change if we need to find the transpose of that matrix and we're good to go all right so the first one here to find the cofactor delete the row delete the column what's the determinant of what's left negative 2 times 9 is negative 18 and then minus 0 times 1 is 0. so this is just negative 18. there's no sign change so let's just put negative 18 in there and then let's move on to the next one so now i'm going to be here i'm going to delete the row and the column and let's just do this from where it is we would take the determinant of this matrix here it would be 6 times 9 which is 54 minus you have 0 times 0 which is 0. so this is 54 but again you have that sign change so this is going to be a negative 54. all right let's move on to this one so i'm going to delete the column and the row and the determinant of this guy 6 times 1 is 6 then minus negative 2 times 0 is 0. so this is just 6 no sign change because it's positive so let's just put a 6 there so let's start right here now we'll be in the second row so i'm going to delete this row and i'm going to delete this column so what do i have left i have this matrix with negative 1 and 3 and 1 and 9. so the determinant negative 1 times 9 is negative 9 then minus 3 times 1 is 3. negative 9 minus 3 is negative 12. you've got a sign change there so this would be positive 12. so now i'm going to be here so i'm going to delete this column and this row so the determinant of this 1 times 9 would be 9 then minus 3 times 0 would be 0. so this is 9 no sign change so it's just going to be 9. then let's move on to the next one so i'm going to be here delete this row delete this column and take the determinant 1 times 1 is 1 minus you have negative 1 times 0 which is 0. so this is 1 but again you have a sign change there so it's going to be negative 1. all right let's erase this and let's highlight this guy right here so we're going to delete this row and this column okay so now what we're going to have is the determinant of this so negative 1 times 0 is 0 then minus 3 times negative 2 is negative 6. 0 minus a negative 6 is 0 plus 6 which is 6. so this is 6. all right now let's look at this one right here so i'm going to delete the row it's in and the column it's in and i'm going to take the determinant of this guy so it'd be 1 times 0 which is 0 then minus you have 3 times 6 which is 18 0 minus 18 is negative 18 but you have your sign change there so it's going to be minus a negative 18 or you could say negative 1 times a negative 18 however you want to think about that it's going to be positive 18. let me just change the color of this guy real quick i'd like for my rows to have different colors so let's do the last one we're almost there now so this guy right here i'm going to delete the row and the column and so the matrix that i have left the determinant of it 1 times negative 2 is negative 2 then minus negative 1 times 6 is negative 6 minus a negative is plus a positive so negative 2 plus 6 which is 4. so this will just be 4 here so let's erase this so now we have a matrix of cofactors and what we want to do is find the transpose of this guy and that's going to give us our adjoint so you can say c transpose is equal to the adjoint of your matrix a and this is equal to again you can just take the first row and make it the first column or you can take the first column and make it the first row let's do it a little differently so let's take the first column and make it the first row so negative 18 12 and 6. again all i did was i took the first column made the first row now i'm going to take the second column and make it the second row so negative 54 9 and 18 and then the last thing i'm going to do is i'm going to take the third column and make it the third row so i'm going to have 6 negative 1 and 4. okay again it's equally valid to take the first row and make it the first column it's the same you can take the second row make it the second column take the third row make it the third column you are interchanging the rows and the columns when you find the transpose so either way we found our adjoint for a and i can erase this notation we really need it this guy right here is the adjoint of our matrix a in this lesson we want to talk about how to find the inverse of a matrix using the determinant and its adjoint so at this point we're kind of finally ready to talk about an alternative approach that can be used to find the inverse of a non-singular or you can say invertible square matrix so let's suppose we have our matrix a and it's an n by n again that just tells you have a square matrix same number of rows as columns invertible matrix again invertible or non-singular just means that you can find an inverse okay if you can't find an inverse that's a singular matrix so if this is true then a inverse can be found using this formula so we say that it's 1 over the determinant of a times the adjoint of a now we already know how to find the determinant if you didn't watch kind of the last two lessons okay you probably don't know what the adjoint of a is but i'm going to cover that again here okay i'm not going to cover it in as much detail but we're going to go through it really quickly so let's go down and let's just start off with an example and the very first thing i want to do we know for the 2x2 we have a nice little shortcut and you're going to see where this shortcut comes from in a minute but for now let's just use the shortcut so we know that if i generically define a to be equal to let's just say it's a b c and d then a inverse is found as what it's equal to 1 over the determinant of a times what you do is you take a and d these guys along the main diagonal and you swap them so d goes up here let me change colors real quick d goes up here and a goes down here and then these other guys the b and the c you just change the sign so this becomes negative b and negative c okay so following this format if i want a inverse really quickly what i can do is i can say it's 1 over the determinant of a now let's just stop and get that for a minute it would be what it would be zero times three which is zero minus you'd have one times negative one which is negative one be careful there it's minus a negative one so that's plus one so the determinant is one so one over one is just one so you can just get rid of that you don't even need it so really all i need to work on is this part of the formula and again what do i want to do i want to swap a and d so i want to swap this one and this one so 3 goes up here 0 goes down here and then these two i want to make them into their opposite so negative 1 and positive 1. so this is the inverse of a we already know how to do that it's really easy and cool but where did it come from well let's talk about that for a minute if you go back to that formula let me just go back up here this is something you probably want to write down in your notes because we're going to use it all day again a inverse is 1 over the determinant times the adjoint of a now we did this but we really didn't know we were doing it okay so let's go back and i'm just going to copy this i'm going to need a fresh sheet to go through all this so 0 1 negative 1 and 3 and let me just copy this real quick and let's go to a fresh sheet so we have some room to work and we're going to see that we're going to get the same answer so let's paste this in okay so the first thing is we know the determinant is 1. so let me just kind of write that off here the determinant of a is 1. so we don't really need to work on that how do we find the adjoint of a well we talked about this over the course of the kind of last two lessons first you need to find a matrix of cofactors okay and then you need to take the transpose of that it sounds kind of fancy but it's really not that hard it's just more tedious than it is hard we know that finding a cofactor is kind of a two-step process the first thing is you have to find the minor and then you have to apply a sign change in some cases so really what we would say is a given entry in our cofactor matrix which i'll call c let's just say that c sub i j this is just some generic entry is equal to negative 1 raised to the power of the row plus the column so i plus j okay and then times your minor element so m sub i j if this doesn't make any sense if this doesn't ring a bell from our lesson on determinants or the last two lessons it's okay it's very easy to do when you see generic notation sometimes it's a bit confusing what this means is let's first start out by calculating a minor okay to calculate a minor let's say i want the minor of this guy right here well all i do is i mark out the row and the column what's left i take the determinant in this case i'm just left with a kind of one by one matrix just the number three is involved so the determinant of that is just three okay so i can say that if i'm building a minor my m sub one one because in the first row first column remember i'm taking the minor of that guy right there well that's going to be equal to 3. now if i want the cofactor if i want c sub 1 1 well then i just multiply by negative 1 raised to the power of the row plus the call so we know this is row one row two this is column one and column two so if it's row one column one one plus one is two if i take negative one and raise it to an even power it's positive one so there's no sign change so in this case the minor and the cofactor are the same let me erase this we don't really need this anymore and let's just say the cofactor c sub 1 1 is 3 and i'm just going to put a capital letter c to be my matrix of cofactors and the first one up here is going to be a 3. okay so let's put that in and now i want to find c sub 1 2 okay so the first thing i look at when i look at this i say okay this is row 1 column 2 okay so 1 plus 2 is 3. i know that 3 is an odd number so whatever i get for the minor i want to make it negative and you can go through and do that and apply your signs before you start a lot of people like to have a table of signs just to refer to so they don't have to go through this each time okay so we have c sub 1 2 and again this is equal to negative of the minor okay and the minor will be found by what let me put that 0 back in we would look at this guy right here in row 1 column 2 i would mark out the row i would mark out the column what's left it's a negative 1. so the negative of negative 1 is what you're going to have there and i put a negative out in front but i can just erase that now because i have the negative of negative 1 which would be 1. all right so let's look at the other two entries so moving into this bottom row you're going to have your row number as 2 your column number as 1. so c sub 2 1 okay and so 2 plus 1 is 3 that's an odd number so we know this is going to be negative and again if i mark out the column in the row what's left it's the number 1 okay so it's just negative 1 there so c sub 2 1 would be negative 1 and then lastly i want c sub 2 2 so let me erase all this and let me highlight this guy so that's what i want 2 plus 2 is 4 again negative 1 to the power of 4 negative 1 to the to an even power is positive 1. so we don't need to change the sign whatever the minor is that's what we're going to keep mark out the row mark out the column you get 0. so that's what we have there okay and i can just erase this i don't even need it so that's my matrix of coal factors now i'm not done i don't have my adjoint yet because what i need to do so c and then i'm going to put a t up here is going to be equal to remember to find the transpose of a matrix i've got to take the kind of rows and interchange them with the columns and all that means is that i would take this first row here it will become my first column here so 3 1 would go like this you would have 3 1 that would now be my column it was a row now it's a column and then for this guy negative 1 0 it would now be negative 1 0 it would be a column there so this is my transpose of c which is my adjoint of a so we found the adjoint of a and that's equal to this guy right here which is 3 negative 1 1 and 0. now remember the formula let me just go back up it's 1 over the determinant of a in this case the determinant of a is 1 times the adjoint of a if we go back the adjoint of a is this so if i just copy this real quick i'm just going to copy it and go back up and i'm just going to paste this in and you'll see that we got the same thing the adjoint of a here is 3 negative 1 1 0 3 negative 1 1 0. so because i don't have to multiply by anything because the determinant is 1 so 1 over 1 is 1 it just becomes this guy by itself but normally it doesn't work out that way in this case we just happen to have an easy example so again we could say a inverse is this guy right here so let's go ahead and look at a harder example now so we're going to have this matrix a it's going to be a 3x3 matrix we're going to use the same procedure unfortunately there's no shortcut for this you just have to kind of grind through the work so we know we need to find the determinant of a and we need to find the adjoint of a and then we're going to multiply kind of one over the determinant of a by the adjoint of a so let's just start off by finding the kind of cofactor matrix i think that's the one that takes the longest so let's go through and figure this out so if i start with this one first off let me just go through and get the signs so we can just take care of that right away you already know that these alternate in signs so this is row one column one so it's a plus then as you move it's row one column two so this is a minus right the signs are going to alternate so this is plus this is minus plus minus and this is plus minus plus okay so this is what we're going to have when i get the minor i'm just going to apply the sign and i'll erase this okay that's one thing you can do to just kind of speed things up there's a lot of kind of tricks out there but that's a good one that you can use so if i want the kind of cofactor for this one right here i would delete the row and the column and i would take the determinant of what's left so this is what's left so we would multiply down here negative 1 times negative 1 is positive 1 and then minus i would do 4 times negative 2 which is going to be negative 8. so 1 minus a negative 8 is going to be 1 plus 8 which is 9. so again it's positive so i'm not going to change the sign so it's just not all right let's move on now so for the next one let's just go to this one if i again highlight this row and this column what's left i'm going to have 4 negative 2 negative 2 and negative 1 and i'm going to write this out here because it's kind of hard to see so again if you want the determinant of that 4 times negative 1 is going to be negative 4 and then minus negative 2 times negative 2 is positive 4 negative 4 minus 4 is going to be negative 8. now when you put this up here remember you got to change the sign so this is minus a negative 8 which is going to be positive 8. all right let's move on so the next one is this guy right here again delete the column and the row what am i left with i want the determinant of this 4 times 4 is 16 minus negative 1 times negative 2 is positive 2 16 minus 2 is 14. again i don't need to change the sign here because that's already a plus all right let's erase this and let's move on to this one right here so i want to delete this row and i want to delete this column so what's left and let me write this one out because it's a little hard to see so negative 3 1 4 and negative 1. so i want the determinant of that negative 3 times negative 1 is 3. 4 times 1 is four so three minus four is negative one and of course in this particular case we're changing the sign right we have minus the negative one so this will be plus one so now let's move on to the next one so we have this one now and so i'm going to delete this column and this row so what's left again i'm going to write this out because it's kind of hard to see so you have 1 and you have 1 you have negative 2 and negative 1. so if i take the determinant of this 1 times negative 1 is negative 1 and then minus if you do negative 2 times 1 it's going to be negative 2. so negative 1 minus a negative 2 is the same thing as negative 1 plus 2 which is positive 1. so again there's no sign change here so i'm just going to put this as positive 1. so now we want this guy right here so i'm going to delete this column and this row so what i have is 1 negative 3 negative 2 and 4. again if i want the determinant of this 1 times 4 is 4 minus negative 3 times negative 2 is 6 4 minus 6 is negative 2 but i've got a negative here so i've got to change the sign so this becomes positive 2. all right let's move to the kind of final row and so what we want to do here is we want to delete this row and this column so what am i left with i'm left with this matrix here so negative 3 times negative 2 is 6 and then minus 1 times negative 1 is negative 1. 6 minus a negative 1 is the same thing as 6 plus 1 which is 7. there's no sign change there so let's just put a 7. all right just two more and i know this gets very tedious but sometimes it's faster sometimes it's not so let's put this guy and we'll kind of highlight this column in this row so the matrix that's left is a 1 and a 4 in the first column and a 1 and a negative 2 in the second one so let's go ahead and say 1 times negative 2 is negative 2 minus 1 times 4 is 4. negative 2 minus 4 is negative 6. you've got a negative there so you've got to change the sign so this would be positive 6. all right one more of these not too bad so i want this one so again i'm going to highlight this column and this row if i deleted that this is the matrix that's left so 1 times negative 1 is negative 1 then minus negative 3 times 4 is negative 12. again negative 1 minus a negative 12 is negative 1 plus 12 which is 11. so there's no sign change here so i'm just going to put an 11 in so now i have my cofactor matrix okay and when i take the transpose of this guy that's the adjoint of a so c and then i'm going to put a t up here a superscript t okay and that's equal to again you take the row and you make it into the column so row one becomes column one so i'm going to write that we have 9 8 and 14 for the first column because it was the first row the second row becomes the second column so 1 1 and 2. 1 1 and 2 and then the third row becomes the third column so 7 6 and 11. so now we have this guy so i can erase this i don't even need it anymore and i can say that the adjoint of a is going to be equal to this guy right here so we have our 9 our 8 and our 14. we have our 1 our 1 and our 2 we have our 7 our 6 and our 11 okay so that's my adjoint of a now the other thing i need is the determinant of a because i have to take this guy and multiply it by 1 over the determinant or you could say i could take this guy and divide it by the determinant however you want to think about that so the fast way to get the determinant let me kind of scooch this out of the way we want to copy the first two columns so 1 4 and negative 2 and then negative 3 negative 1 and 4. so let me multiply down 1 times negative 1 times negative 1 would be 1. then plus you'd multiply down again negative 3 times negative 2 times negative 2. we know that's going to be negative okay 3 times 2 is 6 times 2 again is 12. and then you're going to multiply down one more time 1 times 4 times 4 is going to be 16. so let me just find the sum real quick we know 16 plus negative 12 is 4 4 plus 1 is 5. so the first part of this formula is 5 and then you're subtracting away the second part of this formula so you use brackets or parentheses something to let you know that you need to kind of make sure you respect that sign so now i'm going to start at the bottom left and go up so negative 2 times negative 1 times 1 is going to be positive 2 then plus we have 4 times negative 2 times 1 that's going to be negative 8. then lastly plus negative 1 times 4 times negative 3. we know that's positive and it's basically just 4 times 3 which is 12. so inside here 2 plus negative 8 is negative 6 negative 6 plus 12 is positive 6. so you would have 5 minus 6 which is negative 1. so that's going to be your determinant okay that's going to be negative 1. so the determinant of a is equal to negative 1. so now we're ready to find the inverse again the formula let me just show you one more time a inverse is equal to 1 over the determinant of a times the adjoint of a so if i know this is negative 1 1 over negative 1 is just negative 1 right so really all i have to do is say i'm going to take this guy right here and i'm going to multiply everything by negative 1 because that's all you're really doing so i can say that a inverse is equal to i'm just multiplying this guy by negative one multiplying by a scalar so every entry i'm just going to put a negative in front of it they're all positive so they'll all become negative so negative nine negative one and negative seven you have negative 8 you have negative 1 and negative 6 and then you have negative 14 you have negative 2 and negative 11. so that is your inverse of a again is it quicker than if we had used kind of row operations maybe maybe not depending on how fast you are at that i'm kind of quicker at using row operations the times i go through it with you i go slow when i use row operations i can usually go pretty quickly so for me this method is a little bit more time consuming but not by a great deal so just another method you can use all right so suppose we have matrix a and again we want to find the inverse of this matrix kind of using this new method so again you've got to find the determinant of a and you've got to find the adjoint of a whichever one you'd want to do first is fine last time we found the adjoint first i think that's a little bit harder to do so let's just do that first again so i'm going to start by finding a matrix of cofactors so again the signs if you think about it they start out in this one as positive right because it would be negative 1 raised to the power of you have row 1 column 1 1 plus 1 is 2 that's even then it alternates so this would be minus this would be plus this would be minus this would be plus this would be minus this would be plus minus plus if you could just remember that this one's plus and that they alternate going this way and this way you're good to go right because you basically go plus minus plus then when you come down here you know it's minus plus minus then when you come down here you know it's plus minus plus again it's not hard to do because you're just looking at the row number and the column number you sum those numbers okay and if that result is even it's going to be a plus because negative 1 to an even power is going to give you a positive 1. if that result is kind of odd then we know it's going to be negative or negative 1 because negative 1 to an odd power is negative 1. all right so with that being said let's find the minors apply the sign changes that's going to give us the kind of cofactors and then we'll find the transpose and from there we'll have our adjoint so the first thing is we're going to start right here i'm going to mark out that and that right the row and the column i'm left with this guy right here so i find the determinant 3 times negative 3 is negative 9 minus 1 times negative 1 is negative 1. negative 9 minus a negative 1 is negative nine plus one which is negative eight there's no sign change applied so this is just negative eight all right now we move to this one so again i'm going to mark out this column and this row and what's left and you can write this one out because these are kind of hard to see so 0 4 1 and negative 3. i want the determinant of this 0 times negative 3 is 0 the minus 1 times 4 is 4. 0 minus 4 is negative 4. there's a sign change there because there's a negative so let's just say this is positive 4. okay so let's go ahead and go to the next one now so again if i go on this one i'm marking out this column i'm marking out this row so what's left is this guy right here zero times negative one is obviously zero the minus three times four is twelve so this is negative twelve there's no sign change so negative twelve all right let's move down to the second row now so i'm gonna mark out this row and this column and what's left you'll have negative 4 negative 3 negative 1 and negative 3. let's find the determinant of that guy negative 4 times negative 3 is 12 then minus negative 3 times negative 1 is 3. 12 minus 3 is 9. we're going to put a negative on that because again there's a sign change there so this would be a negative 9. and so i'm here now so i'm going to delete this row and this column let me write this one out again so you have 4 and 4 negative 3 and negative 3. so we want the determinant of that 4 times negative 3 is negative 12 minus negative 3 times 4 is negative 12. we know that minus a negative is plus a positive so negative 12 plus 12 is 0. so this guy's going to be 0 and of course you don't put a plus or minus with 0 it's just 0. so now i'm going to move to this guy so i'm going to highlight this row and this column so i want the determinant of this matrix so we have 4 and negative 4 and then 4 and negative 1. so 4 times negative 1 is negative 4 and then minus negative 4 times 4 is going to be negative 16. we know minus a negative again again again is plus a positive so this is negative 4 plus 16. which is going to give me 12. now we have a sign change here so i'm going to put this as negative 12. so now we're going to start with this one so i'm going to highlight this row and this column so the matrix that's formed is this guy right here so negative 4 times 1 is negative 4 and then minus negative 3 times 3 is negative 9. again minus a negative is plus a positive negative 4 plus 9 is going to give me 5. so no sign change here this is just going to be 5. all right let's erase this two more to do so now i have this guy right here so i'm going to highlight this column in this row and what do we see we see that we would have 4 and 0 and then negative 3 and 1. so if i take the determinant of this guy 4 times 1 is 4 then minus negative 3 times 0 is 0. so this is just 4 but then i want to apply that sign change so this is going to be negative 4. all right then lastly let me kind of erase this we have this guy right here so it's this column and this row that i'm going to block out and then determine it here 4 times 3 is 12 and then minus negative 4 times 0 is 0. so this is 12 and there's no sign change here so this is just 12. all right so we have our matrix of cofactors now and what we want to do is find the transpose of this so c and then we're going to put that superscript t up here and this equals again this is very easy to do take the first row make it the first column so you have negative 8 4 and negative 12. take the second row make it the second column so you have negative 9 0 and negative 12. take the third row make it the third column so 5 negative 4 and 12. so this is your transpose of the cofactor matrix which is the adjoint of a so let's put that the kind of adjoint of a is equal to this guy right here so let me kind of slide this down so i have some room and let me just rewrite this so it's negative 8 negative 9 and 5. we have 4 0 and negative 4 and we have negative 12 negative 12 again and positive 12. okay put my brackets around this and i'm just going to erase this at this point i don't need it anymore and i'll just kind of drag this down here so now let's find the determinant so we have 4 0 and 4. again i'm copying the first two columns negative 4 3 and negative 1. i'm going to start by multiplying down this diagonal 4 times 3 times negative 3 is the same thing as 4 times negative 9 which is negative 36 so that's the first part here then plus we're going to go down this diagonal negative 4 times 1 times 4 is going to be negative 16 then plus we're going to go down this diagonal notice there's a zero in the multiplication so you don't even need to do that because it's going to be zero so what's negative 36 plus negative 16 that's going to give me negative 52. so we'll have negative 52 minus again i'm going to put some brackets there so i don't make a sign mistake so what i want to do now is just start here and multiply up so 4 times 3 times negative 3 is again 4 times negative 9 which is negative 36 and then plus you would do negative 1 times 1 times 4 that's just going to change the sign of 4 so it would be negative 4. and then this one you know you have a 0 involved so you can just leave that off 0 times anything is always 0. so negative 36 plus negative 4 is negative 40 okay so what we see here is we have minus a negative so this is going to be negative 52 plus 40 which is negative 12. so that's my determinant for a so let's erase all of this and we'll say that the determinant of a is equal to negative 12. now the formula tells us that a inverse is equal to 1 over the determinant of a which in this case is negative 12 times the adjoint of a okay which we have down there so let's go back so basically what i'm going to do is just take 1 over negative 12 and multiply by each term here that's how i'm going to get my inverse so let me erase this and say a inverse is equal to and let me just erase this notation we don't really need it we'll say this is negative 1 over 12. again this is just multiplying by a scalar so everything in here is going to get multiplied by negative 1 over 12. so let's start out with negative 8 over negative 12 and we know that this would be positive and eight divided by four would be two twelve divided by four would be three so the first entry is going to be two-thirds let me erase that so then moving over here i'd have negative nine over negative 12 and so everything there is divisible by 3 so this divided by 3 would be 3 this divided by 3 would be 4 of course negative over negative is positive so the second entry would be 3 4. then when i look at 5 there's nothing really to simplify it would just be negative 5 12. so negative 5 12. so let's move on to this next row so you have 4 0 and negative 4. so let's figure out 4 first so 4 over negative 12 we know each is divisible by 4 4 divided by 4 is 1. 12 divided by 4 is 3 so this would be negative 1 3. let me erase this so it'll fit so again negative 1 3 we know zero would be unchanged because zero times negative one twelfth is still zero and then this guy is just going to be different by the sign right it's the same thing as this it'd just be positive one-third so looking at this final row here we have negative 12 negative 12 and 12. well we know that negative 1 12 times negative 12 would just be 1. and you have that again here and then for this one it would just be negative 1 right because basically if you had 12 times negative 1 12 of course these cancel and give you 1 but you still have the negative so it's negative 1. so this is going to be the inverse of a again it can be faster to do it this way or it might not just depending on your kind of speed of doing kind of row operations but i will say that this is another tool that you can use to find the inverse of a matrix and this lesson we want to talk about cramer's rule for a 2x2 linear system all right so up to this point in our course we've learned kind of a variety of methods that can be used to solve linear systems with kind of these matrices that we've set up but probably the best or the quickest method that we can use is known as kind of cramer's rule okay so not in every scenario but in most scenarios this will be true so what i want to do today is just show you kramer's rule i'll show you how easy and effective it is and then at the end of the lesson i'll show you where this comes from a lot of students will kind of read how it's derived in their book and get a little bit lost because you use a lot of generic notation so i'll take the time to show you where it comes from at the end of the lesson for those of you who want to see that so i'm just going to start out today with a simple system we have two variables and we have two equations right so we have this negative six x plus y equals nineteen this negative three x minus two y equals seven so again the very first thing you wanna do is put the equations in standard form okay so that's something that we've been doing for a while now so the ax plus by equals c and a b and c could just be any real number that you want them to be okay so after that you're going to start out by making a kind of matrix so the matrix is just going to contain the coefficients for the system your book's going to call this capital letter d okay so this is going to be equal to and we're going to be looking for the determinant of this matrix so we're going to be putting our kind of vertical bars here so i'm going to take my negative 6 which is the coefficient for x this has an implied coefficient of 1. this has a coefficient of negative 3 and this has a coefficient of negative 2. okay so notice how everything is in order so i have my negative 6 and my negative 3 on the left right those are the coefficients for x i have my 1 and my negative 2 on the right okay those are my coefficients for y so you've got to make sure that you set this up correctly and that's why it's important to put this in standard form all i need to do let me just kind of drag this over here we're going to say d okay and then sub x and this is going to be a determinant okay of this matrix we're going to form so again i'm going to use my vertical bars what you do is you think about the columns you've set up so this is for the x's right the coefficients and this is for the y's right the coefficients so i'm going to replace if this is an x i'm going to replace the column with the x's those coefficients with these constants okay so i'm going to put a 19 here and a 7 here okay and then the y's those coefficients stay the same so a 1 and a negative 2. and if the first time you see that you're like how am i going to remember that just trust me after you do this a few times it's really really quick and easy then similarly if i do this kind of d sub y okay it's going to be the determinant of this kind of matrix we're going to set up it's the same thing so the x's will now stay so negative 6 and negative 3. so you're going to take the coefficients from that y variable okay so the 1 and the negative 2 replace it with the constants so 19 and then 7. okay so what we want to do at this point is kind of calculate these three so let me copy these i'm going to go to a fresh sheet and then i'll give you the actual formula which is really easy so i'll paste this in let's just do this one at a time so for my capital letter d what's what's the determinant what is d equal to so to calculate d which again is the determinant of this guy d is going to be equal to you're going to multiply down so negative 6 times negative 2 is 12 then you're going to subtract the way you can multiply up or you can go down it doesn't matter negative 3 times 1 is negative 3. so we all know at this point that minus a negative 3 is plus a positive 3. so this is 12 plus 3 which is 15. okay so that's my value for d let me go back up and i'm going to erase this now and i'm just going to put that the value is 15 okay because we're going to use that in a moment now let me calculate this d sub y in this d sub x so let's go back i'm just going to erase this because i don't really need it anymore okay and i'm going to erase this i don't need it anymore so for this guy let me just kind of drag this over here my d sub x again i'm going to multiply down 19 times negative 2 is negative 38 and then minus 7 times 1 if i go up that's 7. so negative 38 minus 7 would be negative 45. okay so let me go back up and my d sub x would be negative 45 okay and let me just kind of drag this over here so that everything's kind of in line and now i need my d sub y okay so let me go down here we'll erase this we don't need it anymore my d sub y again i'm just going to multiply down negative 6 times 7 is negative 42 and then minus negative 3 times 19 is negative 57 so remember minus a negative is plus a positive so this is negative 42 plus 57 which is going to give me 15. okay so that's the value of d sub y all right so let's go back up and i'll show you how easy this is now so d sub y is 15. okay so you can get your solution once you have these kind of values calculated these determinants so your x will be equal to you're going to take your d sub x so this always matches this here okay over your d okay and then if you want your y it's equal to your d sub y again this always matches this right here over your d okay so it's very simple very easy we already know that d sub x is negative 45 okay and we already know that d is 15 okay and then for d sub y we already know that that is 15 and we know that d is 15. so that tells me that x is negative 3 negative 45 divided by 15 is negative 3 and that tells me that y is 1. so really quick overall and let's check this so negative 3 comma 1 would be our ordered pair solution so negative 3 comma 1. we don't need this anymore let's get rid of it and let's plug in so negative 6 times negative 3 would be 18 then plus we'd have 1 times 1 which is just 1. and this equals 19 of course it does that works then the next one we have negative 3 times negative 3 which is 9 then minus you have 2 times 1 which is just 2 this equals 7 and of course it does so really really easy to kind of calculate this especially with the 2x2 when you get into a 3x3 obviously it's harder to find a determinant but we have the shortcut for that so it's not that bad but if you start getting into a 4x4 or 5x5 no matter what you do those are kind of hard systems to solve let's look at another example all right so for this one we have 5x minus 3y equals negative 5 and we have 2x minus 6y equals negative 26. so again it takes you a few times to kind of remember what's going on the first thing is to make sure the equations are set up in standard form okay again if you don't do that you're going to not set up the correct matrices and you're going to end up with the wrong answer okay so once that's done which it's already done for us here okay we want to set up our capital letter d okay and that's the determinant of the kind of coefficients from your system setting up that matrix so you would have five and you would have two again these kind of coefficients correspond to that x variable right the x's and then for the y's again you're grabbing these so you have negative 3 and you have negative 6 okay so you take the determinant of that so we can go ahead and kind of set this up over here get out of the way and then you want to have your d sub x again how do we find that well we look at the column where the coefficients from the x variable came from so that's the 5 and the 2 and we replace it with the constants in the system so this guy and this guy okay so you're going to say that you have your negative 5 and your negative 26 there and then this part stays the same so the negative 3 and the negative 6 those don't change and then for your d sub y again it's the same thought process but now you're taking the column with the y coefficients right the negative 3 and the negative 6 you're replacing them with the constants so the x part stays the same the 5 and the 2 and then these get switched so it's going to be a negative 5 and a negative 26 okay and again the first few times you do this it is a little bit confusing takes a little while to kind of remember what's going on but after you do it a few times it becomes something you just commit to memory okay so let's copy these just as we did before and let's go to another sheet and i'm just going to paste these in we'll calculate the values real quick and then we'll use them to get our solution so for this one for capital letter d this is equal to what 5 times negative 6 is negative 30 and then minus you have negative 3 times 2 which is negative 6 minus a negative 6 is plus 6 so this would be negative 24 okay negative 24. let's erase this and now let's do this one so multiply going down negative 5 times negative 6 is positive 30 and then minus negative 26 times negative 3 is positive 78 30 minus 78 is negative 48. so this is negative 48. so let's erase this and just say this is negative 48 and let's erase this so now 5 times negative 26 is negative 130 and then minus if we go up 2 times negative 5 is negative 10 again minus a negative 10 would be plus 10 so this would be negative 120 okay so negative 120 so let me drag this over here and let me kind of drag this down and we'll just copy these and we'll bring them to the next sheet okay so we don't need any of these things anymore we'll just erase all of it again once you've calculated your determinants you've set everything up and calculated you don't need that stuff anymore we just need the values from them because again if i want the solution for x it's what it's d sub x it's always this guy matches this notation here so d sub x over your d okay this guy's always in the denominator okay and then for y it's what it's d sub y over d okay so again you do this a few times it becomes very easy d sub x is negative 48 and d okay is going to be negative 24 obviously that's positive two and then d sub y is negative 120 and then d is negative 24 and that's going to give us positive 5 okay so the solution here is 2 comma 5. so let's erase this now and we'll check it real quick it's always good to check things when you're first starting out because again you might make some silly mistake and then you turn in your test and you got it wrong and it's something you could prevent right just by checking so 5 times 2 is 10 and then minus 3 times 5 is 15 does this equal negative 5 yes it does 2 times 2 is 4 then minus 6 times 5 is 30. is this equal to negative 26 yes it is ok so 2 comma 5 is the correct kind of ordered pair solution again x is 2 y is 5. all right let's look at one more of these and then i want to talk to you about what happens for like a special case scenario what if you have no solution or an infinite number of solutions what do you do there and then i'll go through the process of kind of deriving the formula for you okay so what we have here is x plus 2y equals 7 and negative 2x minus 7y equals negative 35. again everything's in standard form already so we don't need to do anything there we just want to set up our again capital letter d it's the determinant of again take the coefficients so you have 1 you have 2 you have negative 2 and you have negative 7. again this is for the x's this is for the y's okay you've got to know where kind of everything is because you're going to be replacing stuff so when i do my d sub x again what i do is i replace the column with the kind of coefficients for x with the constants so 7 and then negative 35. okay just grab that from there and then i'm going to keep my y kind of coefficients the same so my 2 and my negative 7 there's no change there okay and then for d sub y it's the same thought process but now i take these y coefficients and i swap them with the kind of constants in the system so you're going to have your 1 and your negative 2 you know from the x's that's going to stay the same and then this part right here is going to change it's going to be the 7 and the negative 35 okay so now that we have this again let's just calculate these determinants and i'll just paste this in right here real quick and this was a d it didn't copy but we can just write it all right so for our kind of determinant for this d we have 1 times negative 7 which is negative 7 minus you have negative two times two which is negative four again minus the negative four is plus four so this is negative seven plus four which is negative three okay so this is negative three and then the next we wanna do let's do d sub x so 7 times negative 7 is negative 49 then minus you've got 2 times negative 35 which is negative 70. again again again minus the negative is plus a positive so negative 49 plus 70 which is going to give us 21. okay so this is going to be 21. okay and then let's find d sub y so d sub y we're going to go 1 times negative 35 which is negative 35 minus you have negative 2 times 7 which is negative 14. right minus a negative is plus positive so you have negative 35 plus 14 which equals negative 21. okay so let's erase all of this and we'll put negative 21 here and let me just kind of line these up and then we'll go back we'll kick our formula off and we'll have our solution really quickly so let's go up and we'll erase all this we don't need any of it anymore again once you've calculated all those determinants you're done with that information okay so let's just paste this in we have the values and so we can say that again x is what it's d sub x okay that part matches over d d is always in the denominator then for y it's what it's d sub y over d so d sub x we know is 21 and d we know is negative three okay so this is going to be negative seven right 21 divided by negative three is negative 7. then for d sub y it's negative 21 okay and this is over d which is negative 3 so this is going to be positive 7. so the solution here is that x is negative 7 and y is positive 7 or the ordered pair negative 7 comma 7. all right so let's go ahead and check this it's always good to check stuff just in case again you made a silly mistake so for the first one you'd have negative 7 plus 2 times 7 which is 14 equals 7 which is true right so this one works for the second one you'd have negative 2 times negative 7 which is 14 minus 7 times 7 which is 49 equals negative 35 that's true as well all right so let's take a little bit of time and talk about the scenario where you have kind of a special case scenario right so you have an infinite number of solutions or you have an inconsistent system meaning you don't have a solution at all so you won't be able to use kramer's rule to kind of identify which scenario you have okay unfortunately what's going to happen is you're going to have that kind of capital letter d that determine it for that matrix you set up with the coefficients is going to end up being 0 okay so let's go ahead and see that real quick everything's already in standard form if i set up my d okay again this is the determinant of we have our four and our negative two just grabbing the coefficients and then our six and our negative three if we multiply down four times negative 3 is negative 12 and then minus if you do negative 2 times 6 that's negative 12 as well minus a negative is plus a positive so you might as well just say this is plus 12 which is 0. okay so why is that a problem let's think about the formula one more time if this is zero well when i try to find x it's d sub x whatever that is over d well d is zero i can't divide by zero that's undefined okay so once you see that you've got to stop and say okay i'm dealing with a special case scenario and i can't use cramer's rule to kind of identify which one it is do we have an inconsistent system meaning there's no solution or do i have an infinite number of solutions i can't figure that out with kramer's rule so i've got to use some other method right i can use kind of gaussian elimination or gauss-jordan elimination if i want to do kind of matrix methods or i can go through and just kind of use simple substitution or elimination or whatever you want to do in this particular case i would probably just use elimination right i would multiply this first equation by let's say positive 3 so that would give me 12x minus 6y is equal to negative 60 and then the second equation i would multiply it by negative 2. so that would give me negative 12x that would give me positive or plus 6y and that would be equal to negative 36. now looking at this we don't need to go any further we can just say there's no solution right that's very obvious that there's no solution because again if you go through this this is going to cancel become zero it's equal to something that's not going to be zero okay so you know it's not going to have a solution negative 60 minus 36 would be negative 96 but again because this ends up being a false statement okay you can say there's no solution if you go through this and it ends up being a true statement you know there's an infinite number of solutions all right so what i want to do now is just kind of wrap up the lesson and show you where this kind of cramer's rule comes from it's very easy to do i'm not going to go through the whole thing because although it's easy to do it is a little bit tedious and time consuming so i'm just going to show you where we get that x is equal to that d sub x over d okay where that comes from and to do that i want to start out by just talking about the setup in general so you have two equations let's just label this top one as equation one and this bottom one as equation two you'll notice that this system just contains x and y and each equation is already written in standard form for us okay now the coefficients look a little bit weird you have a sub 1 and a sub 2 as the coefficients for x what we're doing here is we're saying hey this is just something generic we don't have numbers involved so we're using a as a stand in and to kind of tell the difference between this coefficient here in the first equation and this coefficient here in the second equation we have the a sub 1 and the a sub 2. okay so that's what that notation is for similarly you have the b sub 1 and b sub 2 as the coefficients for y and then c sub 1 and c sub 2 those are your constants okay so let's start out before we even get into anything let's just write what we know and we'll compare it to how we could get a general solution so we know already that d okay is equal to the determinant of if i set up a matrix of just the coefficients so in this case everything's already in standard form so i'd want my a sub 1 my b sub 1 my a sub 2 and my b sub 2. okay so that's my capital letter d that's this matrix here and again i'm taking the determinant of that and if you wanted to make this super clear for you remember this leftmost column corresponds to the kind of coefficients for the x variable so that's why i'm putting x's and then this is going to be for the y's these are the coefficients for the y variable and so you want to make sure you know what's going on with that because when you do your d sub x remember what you want to do since this is sub x i take this column that has the coefficients for the x's and i replace it with the constants so what i want here is c sub 1 and c sub 2 and then this part would stay the same so b sub 1 and b sub 2. and then similarly if we do d sub y then what i'm going to replace the column with the coefficients for y with the constants so c sub 1 and c sub 2 goes there and then on the left it's the same as over here so a sub 1 and a sub 2. so generally speaking if you are setting these things up again if you have d sub x you just take d and you take the column with this guy in it so the coefficients for that variable and you replace it with the constants okay and so the same thing for d sub y i take the column with the coefficients for that kind of variable in this case again it's y so i take that and replace it with the constants so very easy to remember once you do this a few times let me copy this real fast i'm going to bring it to another sheet and let me put this over here i'm just going to paste this in and really quickly i'm just going to drag this over here and generically we want to say that this is equal to what this is a sub 1 times b sub 2 again i'm just multiplying down and then minus if i multiply up i'll do a sub 2 times my b sub 1 okay and this is equal to what it's equal to c sub 1 times b sub 2 and then minus you're going to do c sub 2 times b sub 1. and then lastly this one's equal to a sub 1 times c sub 2 and then minus you're going to have your a sub 2 times your c sub 1. so this is our value for d this is our value for d sub x and this is our value for d sub y okay so we have that let me go to another sheet so i just want to write really quickly that x is equal to d sub x over d and y is equal to d sub y over d now i'm not going to show the y one i'm just gonna do the x one okay but you could do the y one on your own so you can kind of continue this process so if we go back and i'm just going to copy this real fast and trust me once we get to the other parts it's going to go really quickly so let me paste this in so this is d in each case let me put equals here and let me put equals here let me kind of paste this in and all i'm doing is i'm just pasting in a value for d we already figured that out that it was a sub 1 times b sub 2 minus a sub 2 times b sub 1. okay then for d sub x i can grab that from here let me copy that and i will paste that in and just get rid of that extra equal sign that's not necessary and then let's go back up and we'll grab this d sub y okay copy that and we'll come back down and paste this in okay so now we're going to come back to this in a moment i'm going to again show you how you can get this result so let's go up let's say i wanted to solve this using elimination and what i'm going to do is i'm going to eliminate the variable y again you could use a similar thought process to eliminate x and kind of go through the other scenario i'm only going to do one okay so what i'm going to do is i'm going to first start out by multiplying equation 1 by b sub 2. so that would be a sub 1 times b sub 2 then times my x then plus i would have b sub 1 times b sub 2 then times my y and this equals c sub 1 times b sub 2. so all i did was i multiplied equation 1 by b sub 2. so every term has a factor of b sub 2. okay so now what i'd want to do again if i'm trying to eliminate the y variable i want this to be the opposite of this so negative b sub 1 times b sub 2 times y so how could i accomplish that well i would multiply equation 2 by negative b sub 1. so this one right here would be negative a sub 2 times b sub 1 times x okay so again i'm just multiplying by negative b sub 1. that's what i did there this is already done so you can skip that and then it's equal to it's going to be negative c sub 2 times the b sub 1 okay so now if i use my elimination process we know on the left sides if i add those together this is going to be gone okay so all i'm going to have let me kind of scroll down so we have enough room on the left i have my a sub 1 times b sub 2 times x then minus my a sub 2 times my b sub 1 times x and this equals you're going to have your c sub 1 times b sub 2 minus your c sub 2 times b sub 1 okay now at this point it probably looks like a lot of nonsense but i want you to realize that you can solve for x you have x here and you have x here so to solve for x we want to factor that out okay so if i factor that out inside the parentheses that have what's left so i'd have that so a sub 1 times b sub 2 then minus my a sub 2 times b sub 1 okay that guy right there and then let's just close the parentheses and say this is equal to c sub 1 times b sub 2 minus c sub 2 times b sub 1. okay so how can we solve for x well all i'm going to do is i'm just going to divide both sides of the equation by what's multiplying x and in this case it's this a sub 1 times b sub 2 minus a sub 2 times b sub 1 okay i'm going to do that on both sides so a sub 1 times b sub 2 minus your a sub 2 times b sub 1 okay so notice that this would cancel with this it's just a complicated form of 1 and i'm left with a solution to say that it's x is equal to your c sub 1 times b sub 2 minus your c sub 2 times b sub 1 over your a sub 1 times b sub 2 minus your a sub 2 times b sub 1. okay so let's copy this real quick and let's go back to this page here and let me paste this in right down here so i want you to notice something and what you're going to notice is that this formula right here for x and this is the same okay so you have c sub 1 times b sub 2 c sub 1 times b sub 2 minus c sub 2 times b sub 1 minus c sub 2 times b sub 1. then down here it's the same as well it's the exact same thing so that's where this comes from you just realize that all you really need to do is find kind of this determinant and this determinant and basically you've found your solution for x okay now if you wanted to prove this for y you could do it in the same way you can go back to the original kind of set up here and now you would want to make kind of these two terms here opposites you could eliminate the x variable and you could solve this guy for y and all you're going to find is that your solution is this guy right here okay it's just going to turn out that way so y is equal to your d sub y over d in this lesson we want to talk about cramer's rule for a 3x3 linear system so in the last lesson we talked about using cramer's rule to solve a two by two linear system now we're just going to take the next step and we're going to look at some examples with kind of a three by three linear system this is no more difficult it's just a little bit more tedious okay because it takes more time to kind of calculate things so let's just start out with this example here we have our system and notice how every equation is already written in standard form force again this is something that's very important to check okay if you have a three variable system okay and you have three equations you want to make sure every equation is ax plus b y plus cz equals t okay and you might get different notation you might have x sub 1 through x sub 3 but you want to make sure that everything lines up right so the x's line up the y's line up the z's line up the constants line up because when you load information into your kind of matrices it's got to make sense okay all the columns have to correspond to kind of the same thing whether it's you know the coefficients for x or y or z or the constants it's all got to line up and make sense okay so with that being said let's go ahead and get started with this problem the very first thing we want to do as we saw in kind of the last lesson we want to set up our kind of d okay and this is the determinant of the coefficient matrix so if i take my coefficients from the system in the first equation i'd have negative 3 at f2 and i'd have a negative one okay so that's an implied coefficient of negative one so let's just write that in the top so negative three we'd have two and then negative one okay and then for my second kind of row here i'd have a negative one and you could write that in if you want okay let me make that a little bit better so again a negative 1 we would have a 3 and a negative 3. so a negative 1 a 3 and a negative 3. okay then we're going to have a negative 1 again let me write that in so negative 1 you're going to have a 2 and a negative 3. so negative 1 2 and negative 3 okay so this is finding the determinant of the coefficient matrix now we need three other kind of setups like this we need d sub x d sub y and d sub z so let me kind of write d sub x first and each time i write one of these i'm going to go ahead and just move it to another page because we're going to very quickly run out of room okay so d sub x would be equal to what well essentially what i do is i take this guy right here and i take the column where the kind of coefficients correspond to this variable that i'm looking at so in this case it's x you know if it was y if it was z you've got to look at that kind of column that corresponds to the coefficients for that variable in this case i'm looking at this guy because this column represents the coefficients for the kind of x variable what i want to do is i want to replace that column and that column only with the constants of the system okay so what i'm going to do is i'm going to take and put negative 15 negative 9 and negative 13. so i just took these values put them here in this place here okay and this the rest of it's going to stay the same so you have 2 3 2 negative 1 negative 3 and negative 3. so that part's the same and i'm just going to cut this away i'm going to go to a fresh sheet and i'm going to paste this in and i'm going to put that right there okay so that's going to stay there and we'll come back to this in a little while let's get the rest of it going so i'm going to erase this highlighting here and now what i want to do is i want to find d sub y well again it's the same thought process except now the kind of column that corresponds to the coefficients for the y variable that's going to be this middle column here so everything else would be the same so negative 3 negative 1 and negative 1. again this guy is the coefficients for the x variable this is what's going to change in the middle so i'm taking the constants so negative 15 negative 9 and negative 13 write that in and then this this guy right here the coefficients for the kind of z variable that's going to stay the same so negative 1 negative 3 and negative 3. so let me cut this away and then grab all of it so let me cut this away okay so let me go back up so now we just need to find d sub z okay so that's the last one so again it's the same thought process so these first two columns will be the same so negative 3 negative 1 negative 1 2 3 and 2 and again i'm looking at the kind of column here that corresponds to the coefficients for this variable okay so i'm going to replace this with the constants of the system it's just that easy so negative 15 negative 9 and negative 13. okay so let me copy this and get rid of it from here and let me paste that in so each one has its own page okay so let me go back up and the very first thing we want to do is copy this and figure out what this kind of d is going to be equal to so let's calculate the determinant of this first because it's going to be used in every formula and again if you don't remember that i'll talk about that in a second but the first thing is we need to calculate the value of d okay so i'm going to use the shortcut here i'm going to copy the first two columns so negative 3 negative 1 negative 1 we have 2 we have 3 and we have 2 okay so what i want to do is multiply going down first so i want to start here and i want to multiply down so you'd have negative 3 times 3 times negative 3. so you know that would be positive and 3 cubed is 27. so i would say this is 27 then plus i'm going to multiply down this diagonal here so 2 times negative 3 times negative 1 that's going to be 6 then plus i'm going to multiply down this diagonal negative 1 times negative 1 times 2 is 2. okay so i'm going to put some brackets around that or you can do the operation now if you want but basically you're going to subtract away put some brackets around this now i'm going to multiply up so this is getting kind of busy to where we can't see stuff so let me erase these arrows and we'll get rid of that so we can see what's going on so negative 1 times 3 times negative 1 is going to be positive 3 okay then plus we're going to go up 2 times negative 3 times negative 3 i know that would be 9 times 2 which is 18 then plus if i do negative 3 times negative 1 times 2 that's going to be positive 3 times 2 is 6. okay so what's this going to be equal to well 27 plus 6 is going to be 33 and then plus 2 is 35 and then minus 3 plus 18 is 21 plus 6 is 27. if i do 35 minus 27 i get 8 okay so d here is equal to 8. so let's just erase all of this okay i'm going to erase this and i'm just going to say that the value here is going to be 8 and again this is going to be used in every formula so let me go back up to this part right here i'm just going to erase all this highlighting we don't need it anymore and i'm just going to put that d is equal to 8 okay now i also need to know what d sub x is i need to know what d sub y is and i need to know what d sub z is because in each case we're going to be using this to get our answers so let's go down and let's calculate these guys so again i'm going to use my shortcut i'm just going to write this as negative 15 negative 9 and negative 13. just copy the first two columns so 2 3 and 2. so i'm going to start by multiplying down okay so i'm going to multiply down so negative 15 times 3 times negative 3 which is going to be 135 okay then plus i'm going to put some brackets around this we're going to multiply down again so 2 times negative 3 times negative 13 is going to be 78 okay then plus i'm going to multiply down again so negative 1 times negative 9 times 2 is going to give me positive 18. okay so then we're going to subtract away i'm just going to erase this normally i leave it but it's a little bit heavy so it might block us from seeing stuff i'm going to multiply up now so negative 13 times 3 times negative 1 which is going to be 39 okay and then plus we're going to multiply up so you've got 2 times negative 3 times negative 15. and so that's going to be 90 okay and then plus we have negative 3 times negative 9 times 2 so that's going to be 27 times 2 which is 54. so let's go ahead and crank this out so 135 plus 78 is 213 plus 18 is 231 okay then minus if we do 39 plus 90 we get 129 plus 54 is 183 okay so what we want is 231 minus 183 which is 48 okay so this is 48. so let's erase all this and just put that this is equal to 48. okay so let's go back up and let's say this is 48. so now let's find d sub y so again copy the first two columns so negative 3 negative 1 negative 1. you've got negative 15 negative 9 and negative 13. again i'm going to multiply down to start so i'm going to multiply this way so negative 3 times negative 9 times negative 3 is negative 81 okay then plus i'm going to multiply down here so you've got three negatives so you know it's going to be a negative and then 15 times 3 is going to be 45. and then plus if i go down this way negative 1 times negative 1 is positive 1 then times negative 13 is negative 13. okay so let's just go ahead and find this sum real quick so negative 81 plus negative 45 is negative 126 then if i add negative 13 i get negative 139 so let me just put that in there so negative 139 and then minus i'm going to multiply up now let me get rid of these kind of arrows so i'm going to start here and go up so you've got three negatives so you know it's negative and it's just going to be 9 there then for this guy you've got negative 13 times negative 3 times negative 3 so you know that's negative and 13 times 9 is basically what you have and that's 117. so this would be minus 117 or you put plus negative 117 if you want and then lastly you want to do this one so you've got negative 3 times negative 1 which is 3 times negative 15 which is minus 45 or plus negative 45. so negative 9 minus 117 is negative 126. if i subtract away another 45 i get negative 171. so this is negative 171. now you have minus a negative here which becomes plus a positive you get negative 139 plus 171 which is equal to 32 okay so this is 32. so let's erase all of this and say that this is equal to 32 okay and let's go back up and let's say this is 32. okay let's go back down okay so this is the last one to do let me erase that highlighting we don't need that and i'm just going to copy again the first two columns and so i would have negative 3 negative 1 negative 1 again 2 3 and then 2. so i'm going to multiply down so negative 3 times 3 times negative 13 is 117 okay then plus you've got 2 times negative 9 times negative 1 that's 18. then plus you've got negative 15 times negative 1 times 2 that's going to give me positive 3 okay so 117 plus 18 is 135 plus 30 is 165. so the first part is 165 then minus let me kind of erase these because they just get in our way so let me go up now so this way negative 1 times 3 is negative 3 times negative 15 is positive 45 then plus if i go up here 2 times negative 9 is negative 18 then times negative 3 is positive 54 okay and then plus let me go up here negative 13 times negative 1 is 13 then times 2 is 26 okay so 45 plus 54 is 99 plus 26 is 125 okay so this is minus 125 and this would be equal to what 165 minus 125 would be 40. okay so pretty simple so let's erase all of this and erase that and just say this is equal to 40. so let's go back up and say this is 40. it does take a while to kind of go through and get your determinants but once you have that set up you're basically at a point where you have your solutions so your x is equal to your d sub x over d your y is equal to your d sub y over d and your z is equal to your d sub z over d okay so in each case the denominator is the same it's going to be eight so let me just erase this or i can actually just slide this down so i'll just say this is equal to equal to equal to i'm going to put an eight in each denominator okay and the only thing that's going to change is the numerator so d sub x is 48 d sub z is 40 okay and d sub y is 32 okay so if you go through and do the calculations let me slide this down just a little bit more so we can fit everything 48 over 8 is obviously 6. 32 over 8 is 4 and 40 over 8 is 5. so x is 6 y is 4 and z is 5. okay so as an ordered triple this is 6 comma 4 comma 5. remember this is your x this is your y this is your z all right let's go ahead and take a look at another example again it's the same thing it's just a tedious process but we're going to get through it so in this example we already have all the equations written again in standard form that ax plus b y plus cz equals d so let's go right into it we want to start out with d again this is the determinant of the coefficient matrix so for the coefficients here i've got negative five i've got negative five again i've got a negative one i can go ahead and write that in so negative one okay and then for this one i've got a negative five a negative 4 and again a negative 1. and then for this one i've got a negative 1 i've got a negative 3 and then a 2. okay so let's recap i get negative 5 negative 5 and negative 1 negative 5 negative 5 negative 1. negative 5 negative 4 negative 1 negative 5 negative 4 and negative 1 and then negative 1 negative 3 and 2 negative 1 negative 3 and 2. so you always want to check stuff to make sure you got it right because the simplest error for getting a negative or something like that you're going to get the wrong answer okay so the next thing i want to do is find my d sub x so what's that equal to again i just take the column with the kind of coefficients for x in this case it's going to be this one and i replace it with the constants of the system so i'm going to take and put a negative 21 a negative 22 and a negative 11. and then the rest of this is the same so it's negative 5 negative 4 and negative 3 and then it's negative 1 negative 1 and positive 2. okay so let me cut this one away and i'll drag this over and now let me find my d sub y okay so what is d sub y again the same thing i'm going to kind of look at my column that is the coefficients for the y variable so that's this middle one here and i'm going to replace that with the constants so everything else is the same so you have your negative 5 your negative 5 and your negative 1. this is going to be my constants so negative 21 negative 22 and negative 11 and then for the last column again it stays the same so negative 1 negative 1 and positive 2. okay so let's cut this one away i'll just kind of slide this down and let's get the last one so we want d sub z as well okay so that's going to be equal to y so now i'm going to look at this kind of last column these are the coefficients for z i'm going to replace that with the constants so everything else is the same so you've got your negative 5 your negative 5 again you're negative 1 you've got your negative 5 your negative 4 and your negative 3. again the last column i'm just going to replace it so negative 21 negative 22 and negative 11. okay so let's go ahead and get rid of this okay so let's go back up and set everything up now so i'm going to grab this and move it to another sheet and i'm just going to erase all of my highlighting and let me go to that other sheet and paste this in for us and then we'll be ready to go okay so i can erase all this highlighting i don't need any of that and let me go back up here and we're just going to write some things down so we know that we want to find d we want to find d sub x we want to find d sub y and we want to find d sub z okay so we're going to have these values we're going to use them to calculate our x y and z so we already know that x is equal to d sub x over d okay then y is equal to what it's d sub y over d let me give myself some space i'll come over here and say z is equal to d sub z over d okay so we have all this set up and let's go down and let's start out with the value for d because this is going to be used in everything so let's just knock this one out first so again the quick way to do this is just to copy the first two columns so you have your negative 5 negative 5 and negative 1 negative 5 negative 4 and negative 3. again you're going to multiply down on that diagonal so i'm going to go down this way so negative 5 times negative 4 times 2 negative 5 times negative 4 is 20 20 times 2 is 40. then plus you're going to go down this diagonal so you have negative 5 times negative 1 times negative 1 which is negative 5. and then you're going to go down this diagonal you've got negative 1 times negative 5 which is 5 that's negative 3 which is negative 15. okay so negative 5 plus negative 15 is negative 20 plus 40 is 20. so this first part of this is 20 then minus let me kind of erase these and let me go up now so if we go up you have negative 1 times negative 4 times negative 1 which is going to be negative 4. so this is negative 4 then plus if i go up again negative 3 times negative 1 times negative 5 is going to be what was basically just negative 3 times 5 which is negative 15. okay so negative 15 again because you have three negatives there and then going up here 2 times negative 5 times negative 5 i know that's positive and basically 5 times 5 is 25 then times 2 is 50 okay so what is this going to give me negative 4 plus negative 15 is negative 19 plus 50 is 31 okay so basically what you have here is 20 minus 31 which is negative 11 okay so we can erase this stuff we have our first one we'll say this is negative 11. we'll go back up and just put that this is negative 11. so in each case i can go ahead and write negative 11 down here okay it's going to be in every formula and now we can go back and start getting some solutions okay so i find the determinant here i can go and plug it into my formula for x and then i'll find the determinant that's d sub y and then d sub z plug those in for their respective formulas and we'll be done so to calculate the determinant the fast way again i'm going to copy the first two columns so negative 21 negative 22 and negative 11 and then negative 5 negative 4 and negative 3. again i'm going to multiply going down first so i'm going to go this way negative 21 times negative 4 times 2 is going to be 168 then plus i'm going to do negative 5 times negative 1 times negative 11. so i know that's negative and 5 times 11 is 55 so this would be negative 55 here then plus if i go down this way you have negative times negative times negative which is negative okay and you basically have 22 times 3 which is 66. so what's this sum here 168 plus negative 55 is 113 plus negative 66 is 47 okay so the first part of this is 47 then minus let me erase those okay and then i want to go up now so negative 11 times negative 4 times negative 1 well we know that's negative so it's going to be negative 44 right 11 times 4 is 44. so negative 44 goes there and then going up here negative 3 times negative 1 is 3 and then times negative 21 is negative 63. so let's put plus negative 63 and then lastly we have 2 times negative 22 times negative 5. if you do 2 times negative 2 you get negative 44 and then times negative 5 that's going to be 220. so plus 220. so what is negative 44 plus negative 63 that's gonna be negative 107 okay so if i did negative 107 plus 220 i get 113 okay so i'm going to put minus 113 here so this is going to be negative and basically what is 113 minus 47 that's going to be 66 so this would be negative 66 okay so let's erase all of this and we'll say this has a value of negative 66 and of course we can just go back up and get our solution real quick so for this one it's negative 66 so we plug it into this formula negative 66 over negative 11 is positive 6 so we know that x is going to be 6 right that's my x value okay so let's go down and now we'll find y okay so let's go ahead and expand this by copying the first two columns so negative 5 negative 5 and negative 1 negative 21 negative 22 and negative 11 okay so let's go ahead and just multiply down negative 5 times negative 22 is 110 and if i multiply by 2 i get 220 okay so that's the first one and then plus if i multiply going down here so we would have negative 21 times negative 1 times negative 1 that's just negative 21 okay and then plus if i multiply down here i know that's negative i'll go ahead and put that in and 5 times 11 is 55. so what is the sum of 220 plus negative 21 well that's 199 and then plus negative 55 that's 144. so the first part of this is going to be 144 then minus remember we're going to go up now let me erase these when i go up i'm going to start here and go up so i know that's negative 22. if i go up here that's going to be negative 55 right because again it's 3 negatives 11 times 5 is 55 so it's negative 55 okay then this one 2 times negative 5 is negative 10 then times a negative okay so i know that's going to be positive 21 well 10 times 21 is 210 okay so what is negative 22 minus 55 that's negative 77 and if i add 210 i get 133 okay so what is 144 minus 133 that's going to give me positive 11. so let's erase this and let's put an 11 here and let's go back up and we'll say that this is 11. and if i put an 11 here 11 over negative 11 is negative one so my y value here is negative one okay so now i just need to find z and i just need to find the determinant of this guy first so i'm going to copy negative 5 negative 5 and negative 1 negative 5 negative 4 and negative 3. okay so we're going to multiply going down negative 5 times negative 4 times negative 11 is going to be negative 220 okay then plus negative 5 times negative 22 times negative 1 well we know that's negative what's 5 times 22 that's 110 so this would be negative 110 okay then plus you've got negative 21 times negative 5 times negative 3 which is negative 315 okay all right so now let's just go ahead and sum these so negative 220 plus negative 110 is negative 330 then if i add negative 315 i get negative 645. so the first part is negative 645. again then we subtract away this kind of second part where we're going to multiply up so if i go up here negative 1 times negative 4 is 4 then times negative 21 is negative 84. okay so that's negative 84 then plus negative 3 times negative 22 times negative 5. well negative 3 times negative 22 is 66 then times negative 5 would be negative 330 okay so then lastly i'm going to do this one so you have a negative there because it's 3 negative so let's just put that in first we know that 5 times 5 is 25 then times 11 would be 275 so this would be negative 275 here okay so what we want to do is sum these amounts of negative 84 plus negative 330 that's negative 414 plus negative 275 is negative 689. so this would be minus a negative 689 which is negative 645 plus a positive 689 which is equal to what it's just 689 minus 645 which is nothing more than 89 minus 45 which is 44 okay so this is going to be 44. so let's write that in we'll just erase this okay we'll say this value is 44. you can erase this we don't need it anymore and we can go back up now we have our solution so this is 44 so this is 44. so my solution here would be negative 4 right so 44 over negative 11 is negative 4. so i'll just write a negative 4 in here for z okay and so my order triple here is 6 comma negative 1 comma negative 4. again x is 6 y is negative 1 z is negative 4. 
hi there i'm david dye and welcome to the mathematics for machine learning specialization before we get stuck in let's set the scene machine learning is a set of powerful mathematical tools that enable us to represent interpret and control the complex world around us however even just the word mathematics makes some people feel uneasy and unwelcome to explore the topic the purpose of this specialization is to take you on a tour through the basic maths underlying these methods focusing in particular on building your intuition rather than worrying too much about the details thanks to the amazing machine learning community it's actually possible to apply many powerful machine learning methods without understanding very much about the underpinning mathematics by using open source libraries this is great but problems can arise and without some sense of the language and meaning of the relevant maths you can struggle to work out what's gone wrong or how to fix it the ideal outcome of this specialization is that it will give you the confidence and motivation to immediately dive into one of the hundreds of brilliant applied machine learning courses already available online and not be intimidated by the matrix notation or the calculus we want to open up machine learning to as many people as possible and not just leave all the fun to computer scientists this first course offers an introduction to linear algebra which is essentially a set of notational conventions and handy operations that allow you to manipulate large systems of equations conveniently over the next five modules we'll be focusing on building your intuition about vectors and transformations through the use of quizzes and interactive widgets as well as occasionally asking you to fill in the gaps in some python coding examples in the final module dr sam cooper will bring it all together by showing you how linear algebra is at the heart of google's famous page rank algorithm which is used for deciding the order of web pages in search results hopefully if you find this course useful you'll stick around for our follow-on course where sam and i will introduce you to multivariate calculus then in our other course dr mark dyson roth will introduce principal component analysis so welcome we really hope that the course will be productive and useful for you but also quite a lot of fun and i look forward to hearing from you in the forums in this video we're going to look a bit more at the types of problems we might want to solve and expose what linear algebra is and how it might help us to solve them the first problem i might think of is uh one of price discovery say i go shopping on two occasions and i buy apples and bananas and the first time i buy two apples and three bananas and they cost eight euros and the second time i buy say 10 apples and one banana and the cost is 13 euros and the a's and the b's here are the price of a single apple and a single banana and what i'm going to have to do is solve these what are called simultaneous equations in order to discover the price of individual apples and bananas now in the general case of lots of different types of items and lots of shopping trips then finding out the prices might be quite hard it might be quite difficult to solve all of these equations by hand so we might want a computer algorithm to do it for us in the general case now this is an example of a linear algebra problem i have some constant linear coefficients here these numbers 2 10 3 1 that relate the input variables a and b to the output 8 and 13. that is if i think about a vector a b that describes the prices of apples and bananas then this gets translated into a cost if i know how many of them i want to buy and the cost happens to be eight on the first trip and 13 euros on the second trip and i can write this down as a matrix problem where the 2 3 is my first trip and the 10 one is my second trip and then these are then matrices that's a matrix then and these are vectors and what we're going to do over the course of modules one to three is build up looking at these different types of mathematical objects and understanding what they are and how to work with them these vectors and these matrices um and then we'll come back and figure out how to solve this problem in the general case another type of problem we might be interested in is fitting an equation to some data in fact with neural networks and machine learning we want the computer in effect not only to fit the equation but to figure out what equation to use that's a highly inexact description really of what's going on but it gives the right sort of flavor but let's say we have uh some data like this histogram here this looks like a population with a an average and some variation here some width another type of problem we might want to solve as well as the apples and bananas problem is how to find the optimum value of the parameters in the equation describing this line the ones that fit the data in the histogram best that might be really handy then using that equation we'd have an easy portable description of the population we could carry around without needing all the original data which would freeze for example from privacy concerns now we could plot how good the fit was in terms of the parameters and that's what we'll look at in the next video in this video we've set up two problems in this first module on linear algebra first the problem of apples and bananas of solving simultaneous equations and secondly the optimization problem of fitting some data with an equation with some fitting parameters and these problems will go on to look at and motivate our work right through the course on linear algebra and its partner with multivariate calculus so the first thing we need to do in this course on linear algebra is to get a handle on vectors which will turn out to be really useful for us in solving those linear algebra problems that is the ones that are linear in their coefficients such as most fitting parameters we're going to first step back and look in some detail at the sort of things we're trying to do with data and why those vectors you first learned about in high school are even relevant and this will hopefully make all the work with vectors a lot more intuitive let's go back to that simpler problem from the last video the histogram distribution of heights of people in the population you know there aren't many people above about two meters say and there aren't really very many people below 1.5 meters saying we wanted to try fitting that uh distribution with an equation describing the variation of heights in the population and let's say that equation has just two parameters one describing the center of the distribution here and we'll call that mu and one describing how wide it is um which we'll call sigma um so we could fit it with some curve that had uh two parameters uh mu and sigma and i i would uh use an equation like this i i'd call it uh f of x uh some function of x where x is the is the height is equal to 1 over sigma root 2 pi times the exponential of minus x minus mu squared divided by two sigma squared so this equation only has two parameters sigma here a mu and it looks like this and it has an area here of one because there's only 100 of people in the population as a whole now don't worry too much about the form of the equation this one happens this is called the normal or gaussian distribution and it's got a center of mu and a width of sigma and it's normalized so that its area is one now how do we go about fitting this distribution that is finding mu and sigma or the best possible mu and sigma that fits the data as well as is possible imagine that we had guessed that the width was wider than it really is but keeping the area at one so if we guessed that it was a fatter and probably a bit shorter distribution something like that say so this one has uh something like that this one has a wider sigma but it's got the same mu it'd be too high at the edges here and too low in the middle so then we could add up the differences between all of our measurements and all of our estimates we've got all of these places where we underestimate here and all of these places where we overestimate here and we could add up those differences or in fact the squares of them to get a measure of the goodness or badness of the fit and we'll look in in detail at how we do that um once we've we've done all the vectors work and once we've done actually all the calculus work then we could plot how that goodness varied as we change the fitting parameters sigma and mu and we get a plot like this so if we had uh our correct value our best possible value for mu here and our best possible value for the width sigma here we get a we could then plot uh for a given value of mu and sigma what the difference was so if we were at the right value we'd get a value uh of goodness where the the sums of the squares the differences was naught and if uh mu was too far over if we had misestimated mu and we'd uh got the distribution shifted over either width was right but we had some wrong value of mu there that we get some value of all the sums of the squares of the differences of our goodness being some value here that was higher and it might be the same if we went over the other side and we had some value there and if we were too wide we'd get um something there or too thin it would we would get something that was too thin like that um something like that say so we'd get some other value of goodness and we could imagine plotting out uh all of the values of where we had the same value of goodness or badness for different values of mu and sigma and we could then do that for some other value of badness and we might get a contour that looked like this uh and another contour that looked like this and so on and so forth now say we don't want to compute the value of this goodness parameter for every possible mu and sigma we just want to do it a few times and then find our way to the best possible fit of all so say we started off here with some guess that was too big a mu and two smaller width we thought people were taller than they really are and that they were tighter packed in their heights than they really are uh well what we could do is we could say well if i do a little move in mu and sigma then does it get better or worse and if it gets better well we'll keep moving in that direction um so we could imagine making a vector of a change in mu and a change in sigma um and we could have our original mu and sigma there and we could have a new value u prime sigma prime and ask if that gives us a better answer if it's better there or if mu mu prime sigma prime took us over here if we were better or worse there something like that now actually if we could uh find what the steepest way down the hill was then we could go down this set of contours this sort of landscape here towards the minimum point towards the point where we get the best possible fit and what we're doing here these are vectors these are little moves around space they're not moves around a physical space that moves around a parameter space but it's the same thing so if we understand vectors and we understand how to get downhills that sort of curviness of of this value of goodness that's calculus then once we've got calculus and vectors we'll be able to solve this sort of problem so we can see that vectors don't have to be just geometric objects in the physical world of space they can describe directions along any sorts of axes so we can think of vectors as just being lists if we thought of the space of all possible cars for example so you know here's a car ah there's its back there's its window there's the front something like that um there's a car there's the window we could write down in a vector uh all the things about the car we could write down its cost in euros we could write down its emissions performance in grams of co2 per 100 kilometers we could write down its its knox performance um you know how much it polluted our city and kill people due to air pollution we could write down its euro end cap star rating how good it was in a crash we could write down its top speed and write those all down in a list that was a vector that'd be more of a computer science view of vectors whereas the spatial view is more familiar from physics in my field metallurgy i could think of any alloy as being described by a vector that describes all of the possible uh components all the compositions of that alloy einstein when he conceived relativity conceived of time as just being another dimension so space time is a four-dimensional space three dimensions of meters and one of time in seconds and he wrote those down as a vector of space time of uh x y z and time which he called space time when we put it like that it's not so crazy to think of the space of all the fitting parameters of a function and then of vectors as being things that take us around that space and what we're trying to do then is find the location in that space where the badness is minimized the goodness is maximized and the function fits the data best if the badness surface here was like a contour map of a landscape we're trying to find the bottom of the hill the lowest possible point in the landscape so to do this well we'll want to understand how to work with vectors and then how to do calculus on those vectors in order to find gradients in these contour maps and minima and all those sorts of things then we'll be able to go and do optimizations enabling us to go and work with data and do machine learning and data science so in this video we've revisited the problem of fitting a function to some data in this case the distribution of heights in the population what we've seen is that the function we fit whatever it is has some parameters and we can plot how the quality of the fit the goodness of the fit varies as we vary those parameters moves around these fitting parameter space are then just vectors in that space of fitting parameters and therefore we want to look at and revisit vector maths in order to be able to build on that and then do calculus and then do machine learning so we have these things called vectors like this guy here what we want to do first is get an idea of what makes a vector a vector what we'll do in this video is explore the operations we can do with vectors the sort of things we can do with them that define what they are and the sort of spaces they can apply to so a vector we can think of as an object that moves us about space like this guy here this could be a physical space or a space of data at school you probably thought of a vector as something that moved you around a physical space but in computer and data science we generalized that idea to think of a vector as maybe just a list of attributes of an object so we might think of a house say so here's a house um and we this might have a number of attributes we could say it was 120 square meters in in floor area it might have two bedrooms say it might have one bathroom that would be sort of sensible uh and it might be worth uh 150 000 euros say and i could write that down as the vector 120 square meters two bedrooms one bathroom and 150 000 euros um and this that is uh uh while in physics we think of this as being a thing that moves us about space in data science we think of this vector as being a thing that describes the object of a house so we've generalized the idea of moving about space to be include the description of the attributes of an object now a vector is just something that obeys two rules firstly addition and secondly multiplication by a scalar number we'll do this first thinking of a vector as just a geometric object starting at the origin so something like this so we've got a vector r there vector addition is then when we just take another vector so let's take another vector like this guy here let's call him s and where we put s on the end of r so then that's s and therefore if we put s on the end of r we get a sum that's r then going along s we'll call that guy r plus s now we could do this the other way around we could do s and then r and that would be s plus r to go along s and along that way and that would be s plus r there s plus r and we see that they actually give us the same thing the same answer so r plus s is equal to s plus r so it doesn't matter which way round we do the addition so the other thing we want to be able to do is scalar multiplication that is to scale vectors by a number so a number a say making it twice as long or half as long something like that so we'd say that uh say three r was doing r three times that would be three r there my a was three or we could do a half r which would be something like that the only tricky bit is uh is what we mean by a minus number and by minus r we'd mean going back the other way by a whole r so we take our we go back the other way the same distance there that would be minus r so in this framework minus means go back the other way at this point it's convenient to define a coordinate system so let's define space by uh two vectors let's call the first one that takes us left right and is of unit length one length one let's call that a vector i will have another vector here that goes up down a vector j it's also a unit length of length one and then we'd say just use our vector addition rules if we wanted a vector r here something like this that was uh three twos by which we mean we go uh three eyes one i two i three i's and then two j's so we go three i's plus two js and that gives us a vector r here just from our vector sum and what we mean in the three two is do three i's added together or a scalar multiple of three i's and then do a scalar multiple of three js as a vector sum and that's what we mean by a coordinate system of defining r as being three two so then if i have another vector s let's say s is equal to uh minus 1 i's and 2 js that is it takes us back 1 i and up 2 j's so that's s then r plus s would be that we just put s on the end of r and then r plus s is going to be therefore that total vector that's going to be r plus s and we can just add up the components right so uh r is three i's and s takes us back one so it's three plus minus one it gives us two eyes and in the js r takes us up two and s takes us up another two so that's a total of four j's so we can just add up the components when we're doing vector addition so we can see that because we're doing this component by component then vector addition must be what's called associative formally what this means is that if we have three vectors r s and another one t it doesn't matter whether we add r plus s and then add t or whether we add r to s plus t it doesn't matter where we put the bracket we can do this addition and then that one or we can do this addition and then that one so a consequence of it not mattering what order we had so x plus r is equal to r plus s we can also see that therefore it doesn't matter what order we do the additions in if we've got three and that's called uh associativity that's associative that's formally that definition and vector addition we can see when we're adding it up like this will be associative so i've just got rid of the uh the s's and so on so we can talk about another issue which is uh in a coordinate system what do we mean by multiplication by a scalar so if we want to take a multiplication by a scale let's say 2 then we define this to mean that 2r would be equal to 2 times the components of r so 2 times 3 for i's and 2 times 2 for the j's because we've got 2 there multiplied by 2 and that will give us 6 4. so 2r would be doing r and then doing another r so that would be 2r which would be the vector six four go along three eyes four five six eyes and up four j's now i need to think about another question which is minus r so r is this minus r is then that which will be uh minus 3 minus 2 that'll be minus r so then we see sort of obviously kind of that r plus minus r is equal to 3 uh plus minus 3 on the eyes and 2 plus minus 2 on the j's which is equal to 0 0. so if we do r and then add minus r we end up back at the origin and therefore we've defined what we mean by uh vector subtraction here vector subtraction is just uh addition of minus one times whatever i'm doing pulling after the minus sign so if we think of another vector s we had s was -1 2 before right minus 1 i plus 2 j's so then r minus s would be this so that's minus s there is equal to go along one on the eyes and -2 on the j's so r minus s add up the components r minus s let's switch to an addition so r minus s is this vector here that's r minus s if we add up the components of that it's three i's plus one three plus one on the eyes and two plus minus two on the j's so that gives us the vector four zero so if we do r is go along three a minus s is go along one we've got a total of four and if r is go up two and minus s's go down two we've ended up going uh up down zero in total so then we've not only done addition by components we've done now what we mean by vector subtraction as well as being addition of a negative one multiple of the thing that we're doing the minus by uh and that's vector subtraction edition by components so uh let's come back to the house example for a moment so we said we had a house that's my house that was 120 square meters two bedrooms one bathroom and 150 000 euros so if i put the units in that's square meters that's its number of beds that's its number of baths and that's it's thousands of euros but it's worth so two houses now is equal to the vector addition of those things is equal to uh two and the way we're defining vector addition times 120 to 150 which would be equal to 240 4 to 300 so we'd say that in this scheme the way we're defining it then two houses would be 240 square meters that makes sense uh four bedrooms two bathrooms and worth 300 000 euros if i bought two houses identically next to each other um and that would be a scalar multiple or an addition of one house to another yeah one house plus one house we could keep on doing that with three houses or differently shaped houses or whatever it was or negative houses um the way we've defined vectors that will still apply to these objects of houses so that's vectors we've uh defined two fundamental operations that vectors satisfy that is addition so like r plus s here uh a multiplication by a scalar so like 2r here and minus s here and we've explored the properties that those imply like associativity of addition and subtraction what subtraction really means of vectors r plus minus s being r minus s and we've noticed that it can be useful uh to define a coordinate system in which to do our addition and scalings like r32 here uh using these uh fundamental basis vectors uh these things that define the space i and j which we call the basis vectors or the things that define the coordinate system we've also seen that although perhaps it's easiest to think of vector operations geometrically we don't have to do it in a real space we can do it with vectors that are data science lists of different types of things like the attributes of a house so that's vectors that's all the fundamental operations so in this first module of our course on linear algebra we first looked at the problem of data that our world has so much of it and then if we could figure out how to analyze and use it we could really solve problems in the world and we've looked at where these courses fit in terms of helping us access the world of machine learning and data science then we've moved on to look at some example problems the problem of solving some simultaneous equations for example to discover the price of things in the apples and bananas problem or the problem of fitting a model equation with some fitting parameters we want uh to optimize against some data we've then said that both of these problems are going to involve vectors and possibly some calculus so we've started off our journey with vectors and with defining vector addition and scalar multiplication in the next module we'll go further to look at some more operations with vectors and define what we mean by a vector space at the coordinate system of a vector space or its basis in this module what we're looking at is the sort of things we can do with vectors first we can look at their modulus or magnitude we can look at a way to combine vectors together to get a number called the dot product which will take us on to find the scalar and vector projections that will then take us on to looking at the vectors that we use to define the space the basis vectors and at linear independence and linear combinations and that will wrap up module two this is hopefully going to be a good work we'll get all set with some nice problems and exercises to try this stuff out this is the main module on vectors after this in the next modules we'll move on to matrices so we've got stuff to do let's get started with the next video on the modulus and the dot product so we've looked at the two main vector operations of addition and scaling by a number and those are all the things we really need to be able to do to define what we mean by a vector the mathematical properties that a vector has now we can move on to define two things the length of a vector also called its size and the dot product of a vector also called its inner scalar or projection product the dot product is this huge and amazing concept in linear algebra with huge numbers of implications and we'll only be able to touch on a few parts here but enjoy it's one of the most beautiful parts of linear algebra so when we defined a vector initially say this guy r here we did it without reference to any coordinate system in fact the geometric object this thing r just has two properties its length and its direction that is pointing that way so irrespective of the coordinate system we decided to use we want to know how to calculate these two properties of length and direction if the coordinate system was constructed out of two unit vectors that were orthogonal to each other like i here and j here in 2d then we can say that r is equal to a times i plus b times j and when i say unit about i and j i mean they're of length one which people will often denote by putting a little hat over them like this then from pythagoras we can say that the length of r is given by the hypotenuse so what i mean by that is uh if we draw a little triangle here then we've got this length here is a eyes and so if we write the length being well with these two little vertical lines it's just of length a because eyes of length one right and this side here is bjs and that's of length b so this side here is from pythagoras it's just a squared plus b squared all square rooted and that's the size of r so we can write down r quite often people will do this write r down like this just ignoring the i and j and writing it as a column vector so r is equal to a b and the size of r we write it down as being the square root of a squared plus b squared now we've done this for two spatial directions defined by unit vectors i and j that are at right angles to each other but this definition of the size of a vector is more general than that it doesn't matter if the different components of the vector are dimensions in space like here or even things of different physical units like length and time and price we still define the size of a vector through the sums of the squares of its components the next thing we're going to do is to define the dot product one way one way among several of multiplying if you like two vectors together if we have two vectors r and s here r here has components r i r j so r in the i direction are in the j direction and s has components s i and sj then we define r dotted uh with s to be given by multiplying the i components together so that's ri times s i and adding uh the j components together so that's r j times s j that is uh the dot product is just a number a scalar number like three uh given by multiplying the components of the vector together in turn and adding those up so in this case uh that would be uh three and two for the uh for the rirj and minus one uh and two for s so if we do that then we get a sum the r dot s is equal to minus 3 plus 4 which gives us 1. so r dot s in this case is just 1. now we need to prove some properties of the dot product first it's commutative so commutative and what commutative means is that r dot s is equal to s dot r it doesn't matter which way round we do it and it doesn't matter because uh when we put these numbers in here if we interchange those the rs and s's we get the same thing when we multiply minus one by three it's the same as three times minus one so it doesn't matter which way round we do the dot product s dot r is equal to r dot s which means it's commutative the second property we want to prove is that the dot product is distributive over addition uh by which i mean that if i've got a third vector here now t that r dotted with s plus t is equal to r dotted with s plus r dotted with t i can multiply it out in that kind of way this probably feels kind of mundane or obvious but let's prove it in the general case so let's say i've got some n-dimensional vector r components r1 r2 all the way up to rn uh an s is the same as components s1 s2 all the way up to sn and t has components t1 t2 all the way up to tn then let's multiply it out so if we take the left hand side r dotted with s plus t that's going to be equal to r one r times s one plus t1 if we take the components and then r2 component r2 times components s2 plus t2 and then all the dimensions in between and then finally rn times sn plus tn and then what we can do is we can then separate that out so we've got multiply that out so we've got a r1 s1 plus r1 t1 plus r2 s2 plus r2 t2 plus all the ones in between rn sn plus rn tn and then we can collect them together cause we've got the r1 s1 terms r2 s2 r all the way to rnsn and that's of course just equal to r dotted with s and if we collect the rt terms together we've got r one t with two one r two t two all the ones in between and r n t n and that's just r dotted with t so we've demonstrated that this is in fact true that you can uh pull out plus signs and dots in this this way which is called being distributive over addition the third uh thing we want to look at is what's called associativity so uh that is if we take uh a vector a dot product and we've got r dotted with uh some multiple of s uh where a is just a number it's just a scalar number so we're multiplying s by uh a scalar and what we're going to say is that that is equal to a times r dotted with s and that means that it's associative over scalar multiplication and we can prove that quite easily just in the 2d case um so if we say we've got r1 times a s1 r plus r2 times a s2 that's the left-hand side just for a two-dimensional vector um then we can pull the a out right so we can take the a out of both of these and so then we've got r1 s1 plus r2 s2 and that's just r dot s a times r dot s so this is in fact true and so we've got our three properties that the dot product is commutative we can interchange it it's distributive over addition which is this expression and it's associative over scalar multiplication we can just pull out scalar numbers out as an aside sometimes you'll see people in physics and engineering write vectors in bold and numbers or scalars in normal font or they'll underline their vectors to easily distinguish them from things that are just scalars whereas in maths and computer science people don't tend to do that it's just a notation difference between different communities and it's not anything fundamental to worry about the last thing we need to do before we can move on is draw out the link between the dot product and the length or modulus of a vector if i take a vector and dot it with itself so r dotted with r what i get is just the sums of the squares of its components so i get you know r1 times r1 plus r2 times r2 and all the others if there are all the others so i get r1 squared plus r2 squared now that's quite interesting because that means if i take the dot product to a vector with itself i get the square of its size of its modulus so that is uh that equals uh r1 plus r2 uh squared square rooted all squared so that's mod r squared so if we want to get the size of a vector we can do that just by dotting the vector with itself and taking the square root and that's really kind of neat and really hopefully quite satisfying let's take the cosine rule from algebra which you'll remember probably vaguely from school and that said if we had a triangle with sides a b and c then what the cosine rule said was that c squared was equal to a squared plus b squared minus 2 a b times the cos of the angle between a and b because that angle theta there um now we can translate that into our vector notation we call this vector r here and we called this vector s here then this vector would be minus s plus r so that vector would be r minus s minus s plus r so we could say that c squared was the modulus of r minus s squared and that would be equal to the modulus the size of r squared plus the size of s squared minus 2 mod r mod s cos theta now here's the cool bit we can multiply this out using our dot product because we know that the size of r minus s squared is equal to r minus s dotted with itself now that's that's just that and we can multiply that out and then we'll compare it to this right hand side here so r minus s dotted with r minus s well that's going to be if we we need to figure out how to multiply that out that's going to be equal to r dotted with r uh and then take the next one minus s dotted with r minus s dotted with r again if you take that minus s and that r minus s dotted with r again and then minus s uh dotted with minus s so that is we've got the modulus of r squared here i mean alt dot r with itself minus twice s dotted with r uh and then minus s dotted with minus s well that's going to be the size of minus s squared which is just the size of s squared and then we can compare that to the right hand side and when we do that comparison compare that to the right hand side the minus r squareds are going to cancel uh the r squared even the s squares are going to cancel and so we get a result which is that minus twice s dotted with r is equal to minus twice modulus of r modulus of s cos theta that is and then we could lose the minus sign right minus signs will cancel out just multiply through by minus one so and then the twos we can cancel out again so we can say that the dot product r dot s just to put it in a more familiar form is equal to mod r mod s cos theta so what we found here is that the dot product really does something quite profound it takes the size of the two vectors if we if these were both unit length vectors those would be one and multiplies by cos of the angle between them it tells us something about the extent to which the two vectors go in the same direction because if cos theta uh if theta was zero then cos theta would be one and r dot s would just be the size of the two vectors multiplied together if the two vectors on the other hand were 90 degrees to each other if they were r was like this an s was like this and the angle between them theta was equal to 90 degrees cos theta cos 90 is zero and then r dot s is going to be we can immediately see r dot s is going to be some size of r some size of s times zero so if the two vectors are pointing uh at 90 degrees to each other if they're what's called orthogonal to each other then the dot product is going to give me zero if they're both pointed in the same direction say s was like that and the angle between them is naught cause uh of naught is equal to one and then r dot s is equal to the mod r times mod s just the multiplication of the two uh sizes together fun one last fun one here is that r and s were in opposite directions so let's say s was now going this way and the angle between them was 180 degrees cos of 180 180 degrees is equal to -1 so then r dot s would be equal to minus the size of r times the size of s and so what the dot product here really does with this cause it tells us when we get the minus sign out that they're going in opposite directions so there's some property here than the dot product we've derived by looking at the cosine rule that we've derived here when the dot product's zero they're 90 degrees to each other they're orthogonal when they go in the same way we get a positive answer when they're going more or less in opposite directions we get a negative answer for the dot product now there's one last thing to talk about in this segment which is called projection projection uh and for that we'll need to draw a triangle so if i've got a vector r and a another vector s now if i take a little right-handed triangle drop a little right-handed triangle down here where this angle is 90 degrees then uh i can do the following uh if i can say that if this angle here is theta that cos theta is equal to from socatoa it's equal to the adjacent length here over the uh hypotenuse there adjacent over the hypotenuse that is uh and this hypotenuse is the size of s so that's the adjacent over the size of s there now if i compare that to the definition of the dot product i can say that r dotted with we'll have fun with colors dotted with s is equal to uh mod r size of r times the size of s times cos theta cos theta uh but the size of s times cos theta if i pull s up here i just need to put my theta in there cos s cos theta is just that adjacent side so that's just that adjacent side here in the triangle so uh the adjacent side here is just kind of the shadow if i if i had light coming down from here it's the shadow of s on r that length there it's kind of a shadow cast if i had a light at 90 degrees to r shining down on s and that's called the projection so what the dot product gives us is it gives us the projection here of s on to r times the size of r um and uh one thing to notice here is that if s was perpendicular to r if s was pointing this way it would have no shadow that is if cos theta was 90 degrees that shadow would be naught the cos theta would be naught here and i get no projection um so uh the other thing the dot product gives us is it gives us uh the size of r times some idea about uh the projection of s onto r the shadow of s onto r so if i divide the dot product r dot s by the length of r just bring the r down here i get mod s cos theta uh i get that adjacent side i get a number which is called because r dot s is a number and the size of r is a number and that's called the scalar projection um and that's why the dot product is also called the projection product because it takes the projection of one vector onto another we just have to divide by the length of r and if r happened to be a unit vector or one of the vectors we used to define the space of length one then that would be of length one and r dot s would just be the scalar projection of s onto that r that that vector defining the axes or whatever it was now if i want to remember to encode something about r which way r was going uh into the dot product or into the project product it's i could define something called the vector projection and that's defined to be r dot s over mod r dotted with itself so r dot r mod r squared uh so that's r dot s over r dot r if you like because mod r squared is equal to r dot r and that and we multiply that by the vector r itself so that is that's dot product's just a number these sizes are just a number ah and r itself is a vector so what we've done here is we've taken the scalar projection r dot s over r this guy that's how much s goes along r and we've multiplied it by r divided by its length we've multiplied it by a vector going the direction of r but that's been normalized to have a length one so that vector projection is a number times a unit vector that goes in the direction of r so if r say was some number of lengths the vector that would be r divided by its size say if that was a unit length vector i've just drawn there and the vector projection would be that number s dot r that adjacent side times a vector going in the unit length of r so that's uh if you like the scalar projection also encoded with something about the direction of r just a unit vector going in the direction of r so we've defined a scalar projection here and we've defined a vector projection there so good job this was really the core video for this week we've done some real work here we've found the size of a vector and we've defined the dot or projection product we've then found out some mathematical operations we can do with the dot product um but it's distributed over vector addition and associative with scalar multiplication now that it's commutative we've then found that it finds the angle between two vectors the extent to which they go in the same direction and then it finds the projection of one vector onto another it's kind of how one vector will collapse onto another which is what we'll explore in the next two videos so good work now's a good time to pause and try some examples but put all this together and give it all a workout and a bit of a try before we move on now so far we haven't really talked about the coordinate system of our vector space the coordinates in which all of our vectors exist but it turns out that in doing this thing of projecting of taking the dot product we're projecting our vector onto one which we might use as part of a new definition of the coordinate system so in this video we'll look at what we mean by a coordinate system and then we'll do a few cases of changing from one coordinate system to another so remember that a vector like this guy r here is just an object that takes us from the origin to some point in space which could be some physical space or could be some space of data like bedrooms and thousands of euros for the house or something like that what we haven't talked about so far really is the coordinate system that we use to describe space so uh we could use a coordinate system defined by these two vectors here i'm going to give them names we call them i and j before i'm going to give them names e1 and e2 i'm going to find to be of unit length so i'm going to give them a little hat meaning they're of unit length and i'm going to define them to be the vectors 1 zero and zero one and if i had more dimensions in my space i could have e3 hat e4 high e five hat e million hat whatever here the instruction then is that r is going to be uh equal to doing a vector sum of two e ones or three e ones and then some number of e two so we'll call it doing three e ones hats plus four e e2 hats and so we'll write it down as a little list 3 4. so r is the 3 4 here is the instruction to do 3 e1 hats plus 4 e2 hats but if you think about it my choice of e1 hat and e2 hat here is kind of arbitrary it depends entirely on the way i set up the coordinate system there's no reason i couldn't have set up some uh coordinates but some angle to that um or even use vectors to find the axes that weren't even at 90 degrees to each other and were of different lengths i could still have described r as being some sum of some vectors i used to define uh the space so i could uh have say another set of vectors b uh i'll call b1 here in the vector 2 1 and i could have another vector here b 2 as the vector -2 4 and i've defined it in terms of the coordinates e and i could then describe r in terms of you you're using those vectors b1 and b2 it's just the numbers in r would be different so we call the vectors we use to define the space these guys e or these guys b we call them basis vectors so the numbers i've used to define are only have any meaning when i know what about the basis vectors so i'll refer to these basis vectors e is 3 4 but r referred to the basis vectors b also exists we just don't know what the numbers are yet so this should be kind of amazing r the vector r has some existence in a deep sort of mathematical sense completely independently of the coordinate system we use to describe the numbers in the list describing r ah the vector that takes us from there from the origin to there still exists independently of the numbers used in her which is kind of neat right sort of fundamentally sort of idea now if the new basis vectors these guys b are at 90 degrees to each other then it turns out the projection product has a nice application we can use the projection or dot product to find out the numbers for r in the new basis b so long as we know what the b's are in terms of e so here i've described b1 as being 2 1 as being e1 plus e2 e twice e1 plus 1 e2 and i've described b2 as being -2 e1s plus 4 e2s and if i know b in terms of e i'm going to be able to do use the projection product to find r described in terms of the b's but this is a big if the b 1 and b 2 have to be at 90 degrees to each other if they're not we end up being in big trouble and need matrices to do what's called a transformation of axes from the e to the b set of basis vectors we'll look at matrices later but this will help us out a lot for now using dot products in this special case where the new basis factors are orthogonal to each other is computationally a lot faster and easier it's just less generic but if you can arrange the new axis to be orthogonal you should because it makes the computations much faster and easier to do so you can see that if i project r down onto b1 so i look down from here and project down at 90 degrees i get a a length here for the scalar product and that scalar projection is the shadow of r on to b1 and the a number of the scalar projection describes how much of this vector i need and the vector projection is going to actually give me a vector in the direction of b1 of length equal to that projection now if i take the vector projection of r onto b2 going this way i'm going to get a vector in the direction of b2 of length equal to that projection and if i do a vector sum of that vector projection plus this guy's vector projection i'll just get r so if i can do those two vector projections and add up their vector sum i'll then have rb being the numbers in those two vector projections and so i found how to get from r in the e set of basis vectors to the b set of basis vectors now how do i check that these two new basis factors are 90 degrees to each other i just take the dot product so we said before the dot product cos theta was equal to the dot of two vectors together so b1 and b2 divided by their lengths so if b1 dot b2 is zero then cos theta is zero and cos theta's zero if they're at 90 degrees to each other if they're orthogonal so i don't even need to calculate thanks i'll just calculate the dot product so b1 dot b2 here i take 2 times -2 and i add it to 1 times four which is minus four plus four which is zero so these two vectors are at ninety degrees to each other so it's going to be safe to do the projection so having talked through it let's now do it numerically so if i want to know what r uh described in the basis e an r's pink right if i take r in the basis e and i'm gonna dot him with b1 and the vector projection divides by the length of b1 squared so r in e dotted with b1 is going to be 3 times 2 plus 4 times 1 4 times 1 divided by the length of b1 squared so that's the sum of the squares of the components of b so that's 2 squared plus 1 squared so that gives me 6 plus 4 is 10 divided by 5 which is 2. so this projection here is of length 2 times b1 so that projection there that vector is going to be 2 times b1 so that is in terms of the original set of vectors b original vectors e are r e dot b1 over b1 squared times b1 is the vector 2 is 2 times the vector 2 1 is the vector 4 2. and i can do then now this projection onto e2 so i can do our e dotted with b2 and divide by b the length of b2 squared and re dot b2 is 3 times -2 plus 4 times 4 divided by the length of b2 squared which is minus two squared plus four squared so that's three times minus two is six minus six plus four times four is sixteen so that's a minus six plus sixteen is ten divided by this length here is uh 4 squared is 16 2 squared is 4 so 20 so that's equal to a half so this vector projection here is that guy times b 2 so that's r e dot b 2 over the modulus of b 2 squared that's my half times the vector b 2 so that's a half times the vector b 2 which is minus 2 4. now if i add those two together 4 2 this bit that vector projection plus this vector projection so this guy is going to be a half b2 plus uh my half minus 2 4 is minus 1 2. if i add those together i've got 3 4 which is just my original vector r 3 4 in the basis e so in the basis of b 1 and b 2 our b is going to be 2 a half very nice to a half so actually in the basis b is going to be two a half there so our b is two times b one plus a half times b two very nice so i've converted from the e set of basis vectors to the b set of basis vectors which is very neat just using a couple of dot products so this is really handy this is really cool we've seen that our vector describing our data isn't tied to the axes that we originally used to describe it at all we can redescribe it using some other axes some other basis vectors so the basis factors we use to describe the space of data and choosing them carefully to help us solve our problem will be a very important thing in linear algebra and in general and what we've seen is we can move the the numbers in the vector we used to describe a data item from one basis to another we can do that change just by taking the dot or projection product in the case where the new basis vectors are orthogonal to each other so we've seen how we don't just have to have basis vectors that are normal one o and o one the so-called natural basis we can have different basis vectors that redefine how we move about space in this video we're going to define what we mean by a basis by a vector space by the term linear independence which is going to let us understand how many dimensions our vector space possesses so first let's define what we mean by a basis a basis is a set of n vectors that are not linear combinations of each other which means they're linearly independent of each other and that span the space they describe the space is then n-dimensional this first criteria i can write down by saying that they're linearly independent if i can't write any of them down by taking some combination of the others so for example let's say i've got a vector b1 here by taking multiples of b1 i can get anywhere along the 1d space of a line if i take a second vector b2 that isn't just a multiple of b1 then i can get anywhere in the plane of this board by taking combinations of b1 and b2 some number of b1s plus some number of b2s and this is a 2d space now let's take a third vector b3 now for b3 to be a valid third basis vector it has to be impossible to be for me to find some numbers a1 and a2 uh such that i can satisfy this sub so it has to be impossible for me to find b3 as some combination of b1s and b2s where a1 and a2 are just numbers that has to be impossible and if it is impossible b3 is a third basis vector and b3 is linearly independent if it is possible for me to find an a1 and a2 that satisfies that sub b3 uh is then linearly dependent on b1 and b2 and it lies in the same plane as b1 and b2 and if it's uh possible for me to you know impossible to find an a1 and a2 b3 must have some component out of the board so i can then use b3 to give me a three-dimensional space so that lets us define what we mean by the number of independent linearly independent basis vectors in our space if i had a fourth basis vector b4 that wasn't a linear combination of b1 b2 and b3 i'd have a four dimensional space and so on up to as many dimensions as i like now notice what my base suspect vectors b don't have to be they don't have to be unit vectors by which i mean vectors of length one and they don't have to be orthogonal that is at 90 degrees to each other but everything is going to be much easier if they are so if at all possible you want to use all for normal basis vector sets 90 degrees of unit length now let's think about what happens when we map from one basis to another the number line of the axis of the original grid then projects down onto the new grid and potentially has different values on that grid but the projection keeps the grid being evenly spaced therefore any mapping we do from one set of basis vectors from one coordinate system to another keeps the vector space being a regularly spaced grid where our original vector rules of vector addition and multiplication by a scalar still work it doesn't warp or fold space which is what the linear bit in linear algebra means things might be stretched or rotated or inverted but everything remains evenly spaced and linear combinations still work now where the new basis vectors aren't orthogonal then to do the change from one basis to another we won't just be able to use the dot product we'll need to use matrices instead which will meter the next module so that's the formal definition of what we mean by a basis and by linear independence for example say i have a bunch of of 2d data points like this and so they all more or less lie on a straight line right we can we can kind of see that these guys all more or less learn a line that's going to be something like that i can imagine uh redescribing that data by mapping them onto that line and then saying how far they're along that line so i can map this guy down onto the line and i can say the origin maps down there and i can then say this data point is that far along the line and he's also so he's that far along the line there and he's that this far away from the line so i've got two dimensions here how far i am along the line and how far i am from the line and these guys they're all slightly different distances from the line there's a little bit of an argument in stats as to whether we do the distance that way vertically or that way um as a projection for the distance from the line but it's sort of a theoretical argument but notice that this distance from the line is effectively a measure of how noisy this data cloud is if they are all tight on the line they'd all be very small distances away and if they were all quite spread they'd be quite big distances away so this distance from the lineness is in fact the noise um and that's information that isn't very useful to us so we might want to collapse it except that that noise dimension tells me how good this line fit is if the best fit line was was all skewed was all wrong i get a much bigger numbers for the noisiness and if the best fit line was as good as possible i get the minimum possible uh number for the noisiness so that noise dimension contains information that's going to tell me how good my fit is so what i'm doing data science it tells me how good my fit of my to my data is and the way i've defined these uh two directions along the line and away from the line they're orthogonal to each other so i can use the dot product to do the projection to map the data from the x y space onto the space of the line along the line and away from the line um which is what we did and do in the last little segment now if we're thinking about a neural network in machine learning that recognizes faces say maybe i'd want to make some transformation of all the fix pixels in a face into a new basis that describes the the nose shape the skin hue the distance between the eyes those sorts of things and discard the actual pixel data so the goal of the learning process of the neural network is going to be to somehow derive a set of basis vectors that extract the most information rich features of the faces so in this video we've talked about the dimensionality of a vector space in terms of the number of independent basis vectors that it has and we found a test for independence that the set of vectors are independent if uh one of them were a linear combination of the others um we've talked more importantly about what that means in terms of mapping from one space to another and how that's going to be useful in data science and machine learning let's just take a moment to think about what we've done in this module because you've worked quite hard and if all this is completely new to you you've had to think about a lot of new ideas we've looked at vectors as being objects that describe where we are in space which could be a physical space a space of data or a parameter space of the parameters of a function it doesn't really matter it's just some space then we've defined vector addition and scaling a vector by a number making it bigger or reversing its direction then we've gone on to find the magnitude or modulus of a vector and the dot scalar and vector projection product we've defined the basis of a vector space its dimension and the ideas of linear independence and linear combinations we've used projections to look at one case of changes from one basis to another for the case where the new basis is orthogonal so we've done a lot of stuff with vectors and along the way we've done a bit of thinking about how this will apply to working with data so hopefully that's been really useful and you've had enjoyed giving it all a workout in the exercises and activities and i'll see you in the next modules where we'll move on to think about the related idea of matrices so back at the start of the course we encountered the apples and bananas problems how to find the price of things when we only have the total bill and we've looked at vectors so far and now we're going to look at matrices and these are objects that rotate and stretch vectors but they're also objects that let us solve these sorts of problems so let's go back to that apples and bananas problem so i walk into a shop and i buy two apples and three bananas and that that costs eight euros so we're saying two apples and three bananas cost eight euros now so i go to the shop on another day and i buy ten apples and one banana and that that costs me well the shopkeeper charges me 13 euros and i want to discover what the price for one apple and one banana is so i can decide which offers better value or even just predict my bill now you might say this is silly uh what shop doesn't have sticker prices after all but actually in business uh with complicated products and service agreements and higher purchase this sort of thing price discovery happens all the time you know think about what happens when you buy a car for instance now these are just simultaneous equations but i can write them down in another way so the way i would write this down with matrices would be as follows so i'd write it down it's this matrix what i what i'm now calling a matrix an object with numbers in 2 3 10 1 a b equals 8 13. and uh these things are call matrices this is a two by two matrix this is a a two row by one column matrix and this is another two row by one column matrix and the instruction here is to uh multiply this out in the following way so i would multiply the elements in the rows by the elements in the column so i'd multiply 2 by a plus 3 times b that's that row times that column and i'd say that equaled the top row on the right hand side and i'd do the same for the next row that row times that column is 10a plus 1b is equal to the row on the bottom on the right hand side and that looks like my two simultaneous equations but this is really handy because these things notice look like vectors so this matrix operates on this vector to give this other vector and my question is what vector transforms to give me this guy on the right now let's look at what happens if i multiply this matrix here by uh the unit basis vector the x-axis vector well when i do that when i do that multiplication i'm going to get 2 times 1 plus 3 times naught and i'm going to get 10 times 1 plus 1 times naught so i get the vector 210 so what this does is it takes the little unit vector which we called e1 hat and it transforms it to another place which is 210 which is going to be up here somewhere so that's e1 hat changed and that's equal to 2 10. now if i do that with the other basis vector if i do 2 3 10 1 multiplied by 0 1 then i'm going to get 2 times 0 plus 3 times 1 10 times 0 plus 1 times 1 i'm going to get 3 1. so the other basis vector e 2 hat gets transformed over to 3 1 which is going to be over here somewhere so that's e2 changed i'm using the prime here to indicate changed 3 1. so what this matrix does is it moves the basis vectors in some way it transforms them it changes the space so what this matrix 2 3 10 1 does is it's a function that operates on input vectors and gives us other output vectors and the set of simultaneous equations here is asking in effect what vector i need in order to get a transform product at the position 813 in order to get an output of 813 now we can see what we mean now by the term linear algebra linear algebra is linear because it just takes input values are a and b and multiplies them by constants so everything is linear and it's algebra that is it's a notation describing mathematical objects in a system of manipulating those notations so linear algebra is a mathematical system for manipulating vectors and the space is described by vectors so this is interesting there seems to be some kind of deep connection between simultaneous equations these things called matrices and the vectors we were talking about last week and it turns out the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices which is the heart of linear algebra previously we introduced the idea of a matrix and related it to the problem of solving simultaneous equations and we show that the columns of a matrix just read us what it does to the unit vector along each axis now we'll look at different types of matrix and what they do to space and what happens if we apply one matrix transformation and then another which is term composition now because we can make any vector out of a vector sum of the scaled versions of e1 hat and e2 hat then what that means is the result of the transformation is just going to be some sum of the transform vectors which i'm calling e1 hat and e2 hat this is a bit hard to see but what it means is that the grid lines of our space stay parallel and evenly spaced they might be stretched or sheared but the origin stays where it is and there isn't any curviness to the space it doesn't get warped and that's a consequence of our scalar addition and multiplication rules for vectors so that is if i write down the matrix as capital a and the vector it's transforming as r you know whatever it was a b in our apples and bananas problem and that gives me some uh altered version we said it was 813 before but i'm going to call it r transformed or r prime then we can look at what happens if i do algebra with it so if i multiply uh r by some number just a number let's call it n and if i apply a to n r what i'm saying is that i will get n r prime and hopefully you can see if i put an n in there when i multiply it all out i'm going to get an n in there similarly if i multiply a by the vector r plus s then i will get a r plus a s so if i get that multiplication get that do the whole thing again with another vector and get another vector s and add those two that will be true so what i'm saying here is that if i can then think of these as being the original basis vectors e 1 hat and e 2 hat i'll then get some addition of e 2 and e 1 primed so if i say that's n e 1 hat plus m e 2 hat i'll get a uh n times e 1 hat plus m times a e 2 hat which is n e one prime plus m e two primed which is just uh the uh vector sum of some multiple of those so this space gets transformed e1 e and e2 get moved and then i can just add up vectors with them so that's very nice that means that all of our vector sum rules work now maybe that's a bit confusing so let's try it with an example so i've got my matrix a here for my apples and bananas problems of 2 3 10 1 or if we like the vector 210 and the vector 3 1 and let's try uh an example like a vector 3 2. now if i multiply that out just straightforwardly as we probably did at school i've got 2 times 3 plus 3 times 2 so that's 6 plus 6 that's 12. and i've got 10 times 3 which is 30 plus 1 times 2 to 2 so that's 32 but i could think of that as being 2 3 10 1 times 3 times 1 0 plus 2 times 0 1 that is 3 of e 1 hat and 2 of e 2 hat in a vector sum now i can take the 3 outs that's 3 times 2 3 10 1 times 1 0 plus 2 times 2 2 3 10 1 times 0 1 and this we know that this is what happens from e1 hat to get to e1 prime here so that's 3 times 210 and that's 2 times what happens to e2 hat and that goes to 3 1. so that gives us 6 plus 6 is 12 and 30 plus 2 is 32 so it really is true these rules really do work and we can think of a matrix multiplication as just being the multiplication of the vector sum of the transformed basis vectors so pause them for a moment and try that maybe with an example of your own and verify that that really does work because that's really quite deep this matrix just tells us where the basis vectors go that's the transformation it does it's not a complicated multiplying out thing we don't need to worry about the mechanics of doing the sum we can just think of it in terms of what it does to vectors in the space let's look at some special types of matrices that do simple things uh and then we'll think about how to combine them to do complicated things first let's think about a matrix that doesn't change anything uh a matrix that's just composed of the basis vectors of the space so 1 0 and 0 1. so that if i multiply it by some vector x y that's not going to change x y if i multiply it out i'm going to get 1 times x plus 0 times y 0 times x plus 1 times y x y so it doesn't change the vector x y the it's just composed of the basis vectors here and it doesn't change them and that's called therefore the identity matrix it's the matrix that does nothing and leaves everything preserved and it's called i now what if i have different numbers along the leading diagonal something like three zero zero two for instance well that's going to scale the x axis here by a factor of three it's going to go to three naught when i multiply it out and the y axis is going to scale by a multiple of 2. it's going to go from 0 1 to 0 2. so i've scaled space this way by a factor of 3 and that way by a factor of 2 so my little unit square has gone to being a rectangle from being a square originally it scaled up this way three times and this way two times and of course if the scale factor here was a fraction if it was a third or something then i'd have squished space that way instead i'd have gone that way and made it thinner and taller something like that so a fraction then squishes space the next thing to think about is what happens if i've got a matrix where i scale by say -1 here on one of the axes well what that's going to do so the original axes is it's going to flip them around so if i've got one zero here and zero one being the other axis of course then that's going to scale the first one over here to being minus one zero out of the the zero two is going to scale the other axis up to zero two here so my original little cube goes from here up to here so it's changed in area by a factor of two one times two but it's also flipped over um the x-axis has gone over there now what does that mean well if i had previously an axis system where i went using my right hand for my right hand there that's my first one that's my second axis around and i went counterclockwise now i'm going the other way now i've got a left-handed coordinate system and i go clockwise so i need to get my left hand out to describe them now so i've changed the sense of the coordinate system in flipping it over now the other matrix we need to think about get another pen is minus one zero zero minus one and that inverts everything it inverts both axes it takes one zero to minus one zero and it takes 0 1 down here to 0 minus 1. so it flips everything in both coordinates and that's called an inversion another matrix i can think of is 0 1 1 0. that's kind of fun what that does is it takes uh i hat 0 1 0 here it takes it to 0 1 so it takes it to there 01 that guy goes there and it takes the other axis which was there which was also 0 1 and it takes it to 1 0. so what it does is it flips them around it's like i put a mirror in at 45 degrees i could have another mirror would be 0 minus 1 minus 1 0 and that would take 1 zero here and it would make it zero minus one take it down there zero minus one and it would take this axis zero one and it would make it to minus one zero so it would make it over there so that's like having a mirror plane in there and just for completeness i can think of another two mirrors and they would be minus one zero 0 1 and that flips the x axis that's like a vertical mirror that's that guy and i can have another one which would be 1 0 0 -1 and that flips the horizontal axis that flips this guy down but leaves this guy unchanged so those are all my mirrors another thing i might want to think about are shears so i wanted to keep e1 hat where it was one zero but move e2 hat over so e2 hat is zero one so i wanted to move e2 hat over to here say so i wanted to get me two primed to be equal to one one so now in which case i only just write down my matrix right i can say that uh e1 becomes itself one zero and e2 becomes one one that would be the transformation matrix for that shear it would be shearing the unit square over from being a little square to being a little parallelogram here something like that of course i could shear the x-axis as well i could do some combination of shears but that's basically how a shear would look now the last sort of shape change i can do after stretches inversion mirrors and shears is a rotation if i take e1 hat again an e2 hat if i rotate them round well e1 hat's going to go round here so e1 primed is going to be equal to 01. and e2 hat is going to go round to here an e2 primed is going to become -1 0. so that 90 degree rotation there is going to have the transformation matrix 0 1 minus 1 0. and in general i can write down a rotation by an angle here uh let's say an angle here of theta i can write that down as being cos theta uh sine theta sine theta cos theta and i need to put a minus sign in here where positive thetas are actually that way um so we did a rotation by -90 so sine of minus 90 is -1 and that's a general expression for a rotation in 2d if i wanted to do it in 3d i need to think about the axis i was doing it along or around so if i was rotating about z i would preserve all of the z's if for a 3d rotation something like that but this isn't really a course about uh matrix rotations and matrix geometry and and so on that would be something like crystallography this of course about data science so we don't need to think too much about rotations but it is interesting if we need to do things like transform faces if we wanted to do facial recognition we'll want to do these sorts of stretches and mirrors and shears and rotations to faces to get them all facing like that rather facing like this or at some funny angle that we had from our camera that was looking at somebody so we do need to do this in data science on occasion and that's rotations so what we've described in this video is we've described all the possible sort of changes we can do with a matrix so now we think need to think about how we do a rotation and then a stretch and that's the next little part now what's the point of telling about all these different geometric transformations well if you want to do any kind of shape alteration say of all the pixels in an image of a face or something like that then you can always make that shape change out of some combination of rotations shears stretches and inverses that is if i first apply one transformation a1 to a vector r then that makes some first change then if i apply another transformation a2 to that to the result of that then i can perform a composition of the two transformations what i've done is i've performed first a1 and then a2 2r now maybe this isn't so obvious so let's slow down and do a concrete example let's uh start out with our basis vectors so we've got uh our e1 as being 1o uh and our e2 as being o1 now let's take our first transformation a1 as being a 90 degree anti-clockwise rotation so we're going to work out what happens if we rotate this by 90 degrees anti-clockwise so e1 comes down to some transformed e1 let's call it e1 prime of uh 0-1 so we can put zero minus one in there and our e2 rotates down here to be some transformed version of e2 which we'll call which will then be e2 primed which will be at one zero so then we've got an overall a1 which describes a 90 degree rotation now then let's take another one another matrix which also transforms our original basis vectors our original e1 and e2 and what we'll say is that that is a let's say a mirror say so that moves a vertical mirror moves e1 to e1 prime is going to be -1 0 and it's going to leave e2 where it was if i just reflect vertically so my transformation a2 now is going to be minus 1 0 and it leaves the other one the same 0 1. now let's ask ourselves what happens if i do a2 to a1 so now i'm going to reflect over the result of doing a1 so e1 prime is actually when i reflect vertically going to stay in the same place and that's going to give me e1 let's say double prime for doing it twice and e2 prime is going to reflect over here and it's going to become e2 double prime is going to be one zero so i'm going to have an overall result of doing a2 2 a1 which is going to be uh what i get e 1 prime i first write down 0 minus 1 and e 2 double prime i'm going to write down minus 1 0. now i can work out what that is actually in a matrix way without having to draw it all out by saying uh a2 to a1 is doing a2 which is -1 0 0 1. to a ones basis vectors and a uh a one sorry eight one there a one was zero minus one one zero which is just the two transform basis vectors the single primes and when i do that i just have to do this matrix to that basis vector and then this matrix to that basis vector and what that looks like if i do that is i uh do this matrix to that basis vector so i'll do that row times that column and that gives me minus one times zero plus zero times minus one that gives me zero and that one to that one so the second row uh first column 0 times 0 plus 1 times -1 gives me -1 and then i do that a-2 transformation to the second column now to the second basis vector of a1 so minus 1 times 1 uh plus zero times zero a minus one times one uh second row zero times one plus one times zero gives me zero so i do the row times the column for all the possible row and column combinations so that is in fact the same thing so we can have discovered really how to do matrix composition or matrix multiplication doing uh one matrix to another transformation matrix notice that geometrically we can show that a2 then a1 or isn't the same as doing the transformations in the other order first a1 then a2 so just watch that for a minute so this one we've done a2 and then what if we then do a1 well a1 is a 90 degree rotation so if we rotate uh e1 prime then we find that e1 double prime would be equal to our original o2 at e2 that is o1 and our e2 primed which was just staying where it was when we rotate that down then that'll come down to here that'll come down to 1o so our e2 double prime will be 1o let's look how that works out matrix wise so if we do a1 2a2 a1 was 0 1 minus 1 0 and if we do that 2 a 2 which was -101 then we've got to do that matrix multiplication and what that's going to give us is uh 0 times -1 plus 1 times 0 0 then this one times this one minus 1 times -1 plus what 0 times zero is one this row second column zero zero one one and second row second column minus one naught naught one gives me naught so that is the first column is zero one and the second column is 1 0. so that isn't the same as doing the operations in the other sequence you see these minus signs are flipped in fact what happens what's happened is this composition rotating and then flipping over is the equivalent of reflecting the whole lot the original basis vectors in this mirror and flipping and then rotating around the effect is i've just flipped my original axes e1 e2 which is putting a mirror in here so doing the two operations in opposites in different sequences doesn't give you the same operations what we've shown is that matrix multiplication isn't commutative a2 a1 isn't the same as a1 a2 so we have to be very careful with matrix multiplication we can do them in any order meaning they're associative that is uh we could if we do a3 to a2 to a1 we can do a2 a1 and then do a3 or we can do uh a3 a2 a1 so we could do that and then that those are the same they're associative but we can't interchange the order we can't swap them around it's not commutative so this is interesting there seems to be some deep connection between simultaneous equations these things called matrices and the vectors we were talking about in the last module and it turns out the key to solving simultaneous equation problems is appreciating how vectors are transformed by matrices which is the heart of linear algebra so now we're finally going to find a way to solve the apples and bananas problem and along the way we're gonna find out about a thing called the inverse of a matrix and a method for finding it so let's go back to that apples and bananas problem we said that i walked into a shop and i bought two apples and three bananas and that that cost eight dollars or euros or whatever my currency is then i went to the shop on another day and i buy 10 apples and one banana and i got charged uh 13 euros for that so i wrote that down as a matrix times a vector and i could call that matrix a and i could call this vector r and i could call that output vector s so a operates on r and gives me s and what i want to do is i want to find out what r is what this vector a b is in order to give me the right numbers for the output vector now let's just think about this matrix a for a moment let's see if i can think of another matrix i'm going to call this matrix a to the minus 1 that when i multiply it by a gives me the identity matrix and i call this matrix here the inverse of a because what it does to a is it exactly reverses whatever a does and gives me just the identity so if i take this equation uh a r equals s if i multiply it on the left by a to the minus 1 on both sides well what i find is is that this is just going to be the identity which is the matrix that does nothing so when i multiply the identity by r i just get r so then i've got r is equal to a to the minus 1 times s the inverse matrix of a times s so if i could find the inverse of a the matrix when multiplied by a that gives me the inverse if i can find the inverse of a i can solve my problem and i can find what my a and b are and i've solved my apples and bananas problem now hold that thought about inverses for just a moment so we said the inverse of a times a was the identity matrix and we'll just part that up there for a moment now i actually don't have to go all the way to the inverse to solve the apples of my nars problem i can do it just by substitutions so let's uh look at a slightly more complicated problem and see what i mean so i'm going to take a matrix that looks like uh like this one one three one two four one one two and i'm gonna say that i have apples and bananas and carrots say okay they're not a fruit but they're another sort of fruit-like object that we eat and so i'm saying that i bought one apple one banana and three carrots and that cost me let's say 15 euros another day i went and i bought one two and four and that cost me 21 euros and another day i bought one one and two and that cost me 13 euros now the thing to notice about a system of equations like this is that i can if i take this row off of another row i haven't changed anything right so i know that an apple a banana and three carrots cost 15. so if i take an apple a banana and three carrots and 15 off of the next row i haven't changed anything really about that row the same for the third row so uh i can make this problem simpler by going and doing that that's sort of a process of elimination so if i take uh if i call this row one row two and row three if i take row one off of row two then i would have take row one off of row two i'll have zero one one if i take row 1 off of row 3 i'll have 0 0 and minus 1. i'd still have a b and c and i've taken on the right hand side i've taken row 1 i've taken row 1 off of row 2 so i've taken 15 off of 21 so i'll have 6 left and i've taken row 1 off of row 3 so i'll have 15 minus 13 uh is 13 minus 15 is minus 2. so ah interesting so now i can see that actually minus c is equal to minus 2. so i can multiply that row through by -1 here i'll just change pens and i've got a solution now for c but c is equal to 2. so that's interesting now i've got what it's called a triangular matrix that is everything below the body diagonal is zero and that's a very interesting sort of matrix i've reduced it to what's called echelon form all the numbers below the leading diagonal is zero now i can do what's called back substitution i can take my answer for c here and i can put my answer for c back into the first two rows so if i take 1c equals 2 i know that 1 times c here is 2 here so i can take that off there and i can do the same i can take 3 times that off the first row so if i go take three times it off the first try i'm gonna get one one zero abc and i've taken three times the first row off of here so i've taken three twos off of here so i'm going to get nine and if i take that third row off of the second row i'm going to get zero one zero and i've got two offer six is four and i can complete i haven't changed the last row at all so now i know that c here is equal to two i know that if i multiply that row by that column i've got just b is equal to four and i've got a plus b is equal to nine but i can take my answer for b back off this first row so i've got then 1 0 0 0 1 0 0 0 1 a b c and i've taken uh one b i've taken four off of the first row so that's five four two so my solution here for my apples bananas and carrots problem is that apples cost five euros bananas cost four euros and carrots cost two euros um and that's solved the problem for me very nice um so i didn't really have to compute the inverse at all but i've only found out the answer for this specific set of uh specific output vector so we said that this was a times r is equal to s i've only found it for this specific s if i did the inverse in a general case i could do it for any s i could find r for any s so what i've done here is i've done uh what's first elimination i've taken multiples of rows and i've taken them off of each other to get to having a triangular form where the bottom corner here is zeros and i've just got ones on the leading diagonal and then i've done back substitution back substitution well i've gone back up of putting the numbers for c back into the first two rows the number b back into the first row and so on to get my solution to my problem and that's very nice this is actually one of the most computationally efficient ways to solve this problem and it's going to work every time and it's very simple to understand and we can do it in relatively few operations so it's really really nice and one thing to notice here is that in doing so what i've done is i've transformed a into the identity matrix here this is just the identity matrix this one's on the leading diagonals and zeros everywhere else so this is going to be a key to a way to find the inverse which is what we'll look at next now let's think about how i can apply this idea of elimination to find the inverse matrix which solves the more general problem no matter what vector i write down on the right hand side so have a three by three matrix a and it's inverse b which are multiplied together to give the identity matrix i so before we had the matrix for a was 1 1 3 1 2 4 1 1 2. and b here is is i'm going to actually introduce some notation i'm going to call b uh uh composed of elements b11 uh b12 b13 where the first digit represents the row so then i'll have b 2 1 the second digit represents the column so this would then be b 2 2 and this one would be b row 2 column 3. i'll have b 3 1 b 3 2 b 3 3. so that is that's the row and that's the column r equals i what we're saying here is that b is actually the inverse of a so that is if i multiply a by its inverse i'll get the identity and actually the uh the inverse is special because i can do it either way around i can apply it to a on the right or on the left and it'll still work because i times the inverse of a is just the inverse of a so it doesn't really matter which way round i i do it um so but i'm just doing it here so that i've got some b's to play with and i don't get confused through my a's and my beat right but for this example i'm saying b is the inverse matrix of a now notice that this first column of b here is just a vector it's a vector that describes what the b matrix the inverse of a does to space actually it's the transformation that that vector does to the uh first uh the x-axis if you like so uh and the identity matrix is just one zero zero zero one zero zero zero one right so i could write this down i could write down this a that big square guy times b one one b two one b three one and i would get just the first column of the identity matrix 1 0 0. now i could solve that by my elimination method and back substitution just in the way i did before except i'd be juggling different numbers here then i could do it again for the second column of b the second vector of b for the second vector of the identity matrix and i could do it again for the third one so i'd be doing that in some series if you like but here's the fun bit i could do it all at once so i'm going to do this process of elimination and back substitution all at once um i'm going to do it for all the columns on the right hand side simultaneously so if i take the first row here and i take it off the second row as before i'm going to get rid of this one and if i take the first row off of the third row i'm going to get rid of both this one and this one so uh if i've got put the unaltered ones down here 1 1 3. if i take that off of the second row i'm going to get 0 1 1 and if i take that off the third row i'm going to get 0 0 minus 1. so on the right hand side i'm going to have uh the unaltered one is 100 and i've taken that off of the second row so i've got taken one off that that's -1 1 0 and i've taken that off of the third row as well so i've got minus 1 0 1. and so now i can multiply that third row through by minus one and that's in the form i then want well i've got one's on the leading diagonal and zero is below it so when i multiply that three by minus one i get a plus there and a minus there now i can substitute that third row back into the second and first rows so i can take one of it off of the second row and i can take three of it off the first row so now the unaltered one is the bottom one one o minus one if i take one of those off of the second row then i've got zero one zero there i'm going to take that off of the second row over here so if i take one off of minus one i get minus two take zero off one i've got one take minus one off of zero i've got i'm effectively adding one then i want to do that again to the first row i wanna take three of them off to make this guy here zero so i've got then one one zero there and i want to take three of these off the first row so i take three off of one gives me minus two take 0 off there and i've got to take 3 of minus 1 off of 0 so that gives me plus 3. so we're nearly there i've just got this irritating one here i've got to take this row off of this row and i'll be home so if i take my third rows unaltered let's put some brackets in my third row is unaltered my second row is going to be unaltered 1 0 minus 1 minus 2 1 1. i take that row off of that row then that altered one gives me one zero zero take the one off there take this row off of this row minus two off of minus two um i've got to take minus 2 off of minus 2 so that gives me 0. got to take 1 off of 0 that gives me minus 1 and i've got to take 1 off of 3 which gives me 2. so that's my answer so now i've got the identity matrix here um for a in effect i've transformed a till it's the identity matrix i've got my b matrix which i haven't really changed and my identity matrix over here i've changed so now i've got an identity times a b matrix is equal to this guy so the identity times something is just itself so this is in fact my answer for the inverse of a or b so i've found a way here to find the inverse of a matrix just by doing uh my row elimination and then my back substitution which is really cool so what we've done is we've found an inverse matrix a to the minus 1 here and if we multiply a times a to the minus 1 we'll get the identity and prove to yourself if you like just pause for a moment and have a go at doing that for yourself and verify that that times that does in fact give you the identity matrix and in school you probably did this a different way but computationally this way is much easier particularly when it comes to higher dimensions you know 100 rows and columns or something like that there are computationally faster method of doing what's called a decomposition process and in practice what you do in any program that you write is you simply call the solver for your problem or the function uh something like inv a or whatever it is and uh it will pick the best method by inspecting the matrix you give it and return the answer but the point here is to show how these problems are actually solved in a computer and also we'll observe some of the properties of these methods in different circumstances that will affect the sorts of things we want to do when solving these sorts of problems so what we've done here is figured out how to solve both sets of linear equations in the general case by a procedure we can implement a computer really easily and we've made that general by finding a general method to find the inverse of a matrix in the general case for whatever is on the right hand side of our system of equations and hopefully that's really satisfying and really nice in the final video in this module we're going to look at a property of a matrix called the determinant and what happens when the matrix doesn't have linearly independent basis vectors picking up on the basis vectors we looked at in the last module let's go back and look at a simple matrix uh like uh a nor nor d what this matrix does is it scales space so if we have our original uh basis vectors e hat and e2 hat it scales them out um by a factor of a in this direction to a naught and by a factor of d in this direction to naught d and we call those uh e1 prime and e2 prime um now see what i've done to the space it was originally uh this size one by one and what i've done is i've scaled the space every vector in the space by a factor of a this way and by a factor of d this way and therefore i've scaled the space by a factor by of a d all areas in the space everything's got bigger by a factor of a d and i call that the determinant of the transformation matrix now if i instead have a matrix which is a naught b d instead the same but with a b what that does is e one hat stays as it was it's scaled by a factor of a but e two hat goes somewhere different so e1 hat goes to a naught but e2 goes to bd somewhere like this bd now see what's happened to the space i've gone from being something like this to being something like this i've made a parallelogram but the area of the panagram is still a d it's its base times its perpendicular height so the determinant is still a d of this transformation matrix now if i have a general matrix a c b d then it turns the original unit square into a parallelogram like this it moves uh this vector to here this vector to here then to find the area of this parallelogram i'm going to have to do a bit of maths so i'm going to do that and then we'll have a look at it in a moment so i've done the maths and it's here and you can do the geometry and puzzle it out for yourself and pause it if you'd like to verify that i'm correct but the area of the parallelogram here is actually a d minus bc and i'm going to denote the operation of finding that area uh with vertical lines like this and call that finding the determinant of a which you probably saw in school now in school when you looked at matrices you probably saw that you could find uh the inverse of a matrix like abcd by flipping the terms on the leading diagonal and taking the minus of the off diagonal terms for a 2 by 2. so let's do that and see what we get so when we multiply that out we get a d minus b c interesting then on uh this this element here is a uh minus b minus uh plus b a and so we've got a minus b plus b a so that's zero when we do this term we get uh c d minus c d and when we do this term here we get c b times minus b so that's minus b c plus a d so that's a d minus b c interesting so that's the determinant so if i multiply by one over the determinant a d minus b c then these terms will become one and i'll get the identity matrix so if this was the matrix a and i think of this and this together as being a to the minus 1 then i've got uh this is in fact when i pre-multiplied by 1 over the determinant is in fact the inverse of a so we've proved here that that determinant that inversely learned in school is in fact correct but the interesting thing is that this determinant there this amount that it scales space if we then take this matrix when we do the flipping around we haven't changed its scaling of space we need to undo that scaling and bring it back down to a scale of one so the determinant here is what we need to divide the inverse matrix by in order for the it to properly be an inverse now we could spend another video looking at the extension of the idea of row echelon form to find out how to find determinants in the general case computationally but this is both tricky to show and it's kind of pointless knowing how to do the operations isn't a useful skill anymore because we just type uh debt a into our computer and python or matlab will do it for us from a learning perspective it doesn't add much uh to row echelon well echelon does actually which is why we went through it i'm not going to so i'm not going to teach you how to find determinants in the general case if you want to know lookup qr decomposition uh and then follow that through and that's how computationally you go and find it out or in a linear algebra textbook is the other place to look that's how you do it in the general case now let's think about this matrix a here it transforms e1 and e2 hat to two points on the same line it transforms e1 hat to one one and it transforms e2 hat from there over to 2 2 and they're both points on the same line they're just a multiple of each other they're not linearly independent um what this matrix in fact does is it transforms every point in space on a line and notice that the determinant of that matrix is going to be zero if i take a d minus b c determinant of a is not because the area of any area has gone onto a line and therefore that area is naught so however you compute it either geometrically or computationally you get a determinant of naught so if i had a three by three matrix with a similar situation describing a 3d space and if i had the same position where one of the new bases vectors was just a linear multiple of the other two it wasn't linearly independent then that would mean the new space was either a plane or if there was only one independent basis vector a line like we have here in either case the volume closed would be zero so the determinant would be zero now let's turn back to our row echelon form and apply that idea let's take this set of simultaneous equations say you can see that the third row is just the sum of the first two so row 3 is equal to row 1 plus row 2. and you can see that column 3 is just given by 2 of column 1 plus column 2. if you want to pause for a moment to verify that but it really is true so this transformation matrix doesn't describe three independent basis vectors one of them is learning independent on the other two so this doesn't describe a new 3d space it collapses into a 2d space so let's see what happens when i try to reduce this to row echelon form if i uh take off the first row from the second okay so far so good i've got that and my abc stays the same and i take the first one off the second one um i've got 12 take 12 off 17 i've got five if i then take the first and second ones off the third one i've now got zeros everywhere here and if i do that on here i take 12 and 17 of 29 i get 0 here so now i've got c equals zero which is sort of true but not useful there are an infinite number of solutions for c in effect any value of c would work so now i can't solve my system of equations anymore i don't have enough information so if we think about this from a sort of um solving simultaneous equations point of view my mistake was when i went into the shop to buy apples and bananas and carrots the third time i went in i just ordered a copy of my first two orders so i didn't get any new information and i don't have enough data therefore to find out the solution for how much individual apples and bananas and carrots cost so what we've shown is that where the basis vectors describing the matrix aren't linearly independent then the determinant is zero and that means i can't solve the system of simultaneous equations anymore which means i can't invert the matrix because i can't take one over the determinant either and that means i'm stuck this matrix has no inverse so there are situations where i might want to do a transformation that collapses the number of dimensions in a space but that will come at a cost another way of looking at this is that the inverse matrix lets me undo my transformation it lets me get from the new vectors to the original vectors and if i've dumped a dimension by turning a 2d space into a line i can't undo that anymore i don't have enough information because i've lost some of it during the transformation i've lost that extra dimension so in general it's worth checking before you propose a new basis vector set and then use a matrix to transform your data vectors that this is a transformation you can undo by checking that the new basis vectors are linearly independent so what we've done in this last video in this module is look at the determinant how much we grow space we've also looked at the special case by the determinant is zero which means that the basis vectors aren't linearly independent which means the inverse doesn't exist in this first module on matrices what we've done so far is define what a matrix is as something that transforms space we've looked at different archetypes of matrices like rotations and inverses and stretches and shears and how to combine them by doing successive transformations then we've looked at how to solve systems of linear equations by elimination and how to find inverses and then finally we've looked at determinants and linear independence so what we've done in this last video in this module is look at the determinant how much we grow space the area change we've also looked at the special case where the determinant is zero and found that that means that the basis vectors aren't linearly independent and that that means that the inverse doesn't exist so in this first module on matrices what we've done is define what a matrix is it's something that transforms space we've looked at different archetypes of matrices like rotations and inverses and stretches and shears and how to combine them by doing successive transformations matrix multiplication or composition then we've looked at how to solve systems of linear equations by elimination and how to find inverses and then finally we've looked at determinants and linear independence and next week we'll carry on and look at matrices in some more detail now there's an important other way to write matrix transformations down which is called the einstein summation convention and that writes down what the actual operations are on the elements of a matrix which is useful when you're coding or programming it also lets us see something neat about the dot product that i want to show you and it lets us deal with non-square matrices when we started we said that multiplying a matrix by a vector or with another matrix is a process of taking every element in each row in turn multiplying with corresponding element in each column in the other matrix and adding them all up and putting them in place so let's write that down just to make that concrete so i'm going to write down a matrix a here and i'm going to give it elements and a is an n by n matrix and i'm going to give it elements a11 a21 all the way down to a n1 and then a12 all the way across to a1n and then i'll have a22 here all the way across all the way down until i fill it all in i've got a n n down here so the first suffix on this matrix first suffix on all of these elements in the matrix is the row number and the second one is the column number now if i want to multiply a by another matrix b and that's also going to be an n by n matrix and that will have elements uh b 1 1 b 1 2 across to b 1 n and down to b n 1 and across to b n n um dot dot dot dot dot if i multiply those together i'm going to get another matrix which i'll call a b um and then what i'm going to do is i'm going to take a row of a multiplied by the elements of a column of b and put those in the corresponding place so let's do an example so if i want element let's say a b element 2 3 i'm going to get that by taking row 2 of a multiplied by column 3 of b so i'm going to take row 2 of a that's going to be a 2 1 a 2 2 and all the others up to a 2 n and i'm going to multiply it by column 3 of b so that's b 1 3 b 2 3 all the way to b n 3. and i'm going to add all those up and i'll have a dot dot in between so that's going to be this element row 2 column 3 of a b now in einstein's convention what you do is you say well okay this is the sum over some elements j of a i j b j k where so if i add these up over all the possible js i'm going to get um a11b11 plus a 1 2 b 2 1 and so on and so on and that's for i and k is 1. i'm going to then go around all the possible i's in k's so what einstein then says is he then says well okay if i've got a repeated index i just won't bother with the sum and i'll just write that down as being a i j b j k and that's equal to this uh the product uh a b r i k um so a b i k is equal to a i 1 b 1 k plus a i 2 b 2 k plus a 1 uh up sorry i 3 b 3 k and so on and so on until you've done all the possible js and then you do that for all the possible i's and k's and that will give you your whole matrix for a b for the product now this is quite nice if you were coding you just run three loops over i j and k and then use an accumulator on the js here to find the elements of the product matrix a b so the summation convention gives you a quick way of coding up these sorts of operations now we haven't talked about this so far but now we can see it there's no reason so long as the matrices have the same number of entries in j that we can't multiply them together even if they're not the same shape so we could multiply a two by three matrix something with two rows and three columns so one two three one two three by a three by four matrix three there and four there so it's got one two three four times and when i multiply those together i'm going to go that row times that column i've got the same number of js in each case so and then i'm going to be able to do that for all of the possible columns so i'm going to get something with four columns and when i i'm going to be able to do that for the two rows here i'm going to do that row times that one so i'm going to get a 2 by 4 matrix out so it's going to have 1 2 3 4 1 2 3 4. so i can multiply together these non-square matrices if i want to and i'll get in the general case some other non-square matrix are going to have the number of rows of the one on the left and the number of columns of the one on the right now uh all sorts of matrix properties that you might want inverses and so on determinants all start to get messy and mucky and you sometimes can't even compute them when you're doing this sort of thing but there are times when you want to do it and the einstein summation convention makes it very easy to see how you do it and how it's going to work as long as you've got the same number of j's you're good you can multiply them together now let's revisit the dot product uh in light of the summation convention so if we've got two vectors let's call them u and v and we'll say uh use a column vector having elements u i and v is another column vector having elements v i and when we dot them together what we're doing is we're multiplying u1 by v1 adding u2 v2 all the way up so in the summation convention that's just u i v i where we repeat over all the i's and add but this is just like writing u as a row matrix pushing you over from being a vector to being a matrix with elements u1 u2 all the way up to un and multiplying it by another matrix v1 v2 all the way up to v n that's the same that matrix multiplication is the same thing as the dot product i just push the u vector over and then my dot product is just like doing a matrix multiplication which is sort of neat so there's some uh equivalence between a matrix transformation a matrix multiplication and a dot product so let's look at that now if i take a a unit vector here let's call him u hat with components u1 and u2 and let's imagine uh what happens if i dot him with the axis vectors so if i've got an axis here e1 hat which would be 1 0 and i've got another axis here e 2 hat which will be 0 1. now let's think about what happens when i dot u hat with e 1 when i do the projection of u hat onto e1 so when i drop u-hat down onto the axis here when i do the projection of u-hat onto e1 i'm going to get a length here just of u1 just of the x-axis element of u-hat now what happens if i drop uh project e1 onto u hat well i'm then going to get this projection and i'm going to get a length here this projected length here now the fun thing is we can actually draw a line of symmetry here through these two projections where they cross and this little triangle and this little triangle are actually the same you can go and do a bit of geometry and prove to yourself that that's true so this length here this projection there that projection is the same length as that projection which is implied by the dot product if i dot e1 with u hat um when i do this multiplication here it's symmetric i can flip them around and i get the same answer so this shows geometrically why that's true and if we repeat this with the other axes with e2 here and any other axes there are then we'll also get the same results so this is why the projection is symmetric and the dot product is symmetric and why projection is the dot product so there's this connection between this numerical thing matrix multiplication and this geometric thing projection which is quite quite beautiful and mind-blowing really and that's why we talk about uh a matrix uh multiplication with a vector as being the projection of that vector onto the vectors composing the matrix the columns of the matrix so what we've done in this video is look at the summation convention which is a compact and computationally useful but not very visual way to write down matrix operations and that's opened up looking at funny shape matrices and that's opened up re-examining the dot product here so that's really nice we've said before that the columns of a transformation matrix are the axes of the new basis vectors of the mapping in my coordinate system we're now going to spend a little while looking at how to transform a vector from one set of basis vectors to another let's say i have two new basis vectors that describe the world of panda bear here and panda's world is orange so panda's got a first a basis vector there and then another basis vector there say and let's say in my world panda bears basis vectors are at 3 1 and a 1 1 and my uh my basis vectors here are e 1 hat equals naught 1 for sorry for e 2 hat and e 1 hat is equal to 1 naught so those are my basis vectors and the orange ones are panda's basis vectors now uh and so panda's basis factors if the first one for panda is his one zero and the first one the second one is zero one in panda's world so bears basis vectors are 3 1 and 1 1 in my blue frame that is i can write down bear's transformation matrix as 3 1 1 1. now let's take some vector i want to transform let's say that vector is in bears world is the vector uh a half of three one in bear's world so it's three over two one over two so the instruction there is do uh three over two of three one and then do one over two a half of one one in my frame if you like so in my world that's going to give me the answer of three times three over two plus one times one over two is 9 10 halves which is 5 and 1 times 3 over 2 plus 1 times a half so that's a total of 2. so that's the vector 5 2 in my world 5 2. those two are the same things so this is bears vector and this is my vector so this transformation matrix here are bears basis in my coordinates in my coordinate system so that transforms bears vectors into my world um which is a bit of a problem you know usually i'd want to translate my world into bears world so we need to figure out how to go the other way so my next question is how do i perform that reverse process well it's going to involve the inverse so if i call bears transformation matrix b i'm going to want b inverse b to the minus 1. and the inverse of this matrix well it's actually pretty easy we can write down the inverse of that matrix pretty easily it's going to be a half of 1 3 flip the elements on the leading diagonal and put a minus on the off diagonal terms and we can see the determinant of that's three minus one is two so we divide by the terminals by half so that's going to be b to the minus one and that's the uh my basis vectors in bears coordinates so that's my basis in bear in bear's world so my 1 0 is going to be a half of 1 minus 1 in bear system and my 0 1 is going to be a half of minus 1 3 in their system and we can verify that this is true if we take this guy a half one minus one and compose it with bears vectors we've got uh one plus minus one of those is going to give me three plus one is uh three minus one is 2 1 minus 1 is 0 so that's 2 0 halve it gives you 1 0. so that really does work if i take a half 1 minus 1 of bears vectors i get my unit vector okay so that really does do the reverse thing so then if i take my vector which was 5 2 and then i do that sum i should get the world in bears basis so i've got five times uh a half minus a half using that guy plus two times r minus one three and that will give me a half of 3 2 when i multiply it all out and if i do the same thing here i've got 5 times 1 minus 1 times 2 gives me 3 over 2 gives me 3 over 2. it all works out if you do it that way or if you do it that way you still get that answer so that's bear's vector again which is the vector we started out with so that's how you do the reverse process you need to if you want to take bears vector into my world you need bears basis in my coordinate frame and if you want to do the reverse process you want my basis in bears coordinate frame that's probably quite counter-intuitive so let's try another example where this time bears world is going to be an orthonormal basis vector set so here's our basis vectors at one zero and zero one in my world in blue and bears world is in orange bears world and bears world has uh one one times and i've made it unit length so it's one over root two and minus one one again the unit lengths are one over root two so they're those two and those you can do a dot product to verify that those two are 90 degrees to each other and there bears vectors 1 0 and 0 1. so that's then i can write down bear's uh transformation matrix that transforms a vector of bears now if i've got the vector in bears world that's 2 1 then i can write that down and i will therefore get the vector in my world so when i multiply that out what i get is i'll get 1 over root 2 times 2 minus 1 which is 1 and then 1 times 2 plus 1 times 1 gives me 3. so in my world the vector is as i've written down 1 over root 2 times 1 3. so if i want to do the reverse transformation i need b to the minus 1 and b to the minus 1 is actually quite easy because this is an orthonormal basis the determinant of this matrix is one so it all becomes quite easy so i just get one over root two keep the leading terms the same flip the sign of the off diagonal terms because it's a two by two matrix so it's really simple and if you go and put that in if you say uh if i take one over root two times one minus one so i take one of those plus one of those uh divide by uh multiplied by root two i do in fact get one zero and the same for zero one it all works so then if i take uh the vector in my world this vector in my world which is one three and multiply it out then what i get is the vector in bears world so that's 1 plus 3 which is 4 1 so minus 1 plus 3 is 2 and i've got 1 over root 2 times 1 over root 2 so that's a half so in bears world this vector is 2 1 which is what we originally said so it really works now this was all prep really for the fun part which is we said before in the vectors module that we could do this just by using projections if the new basis vectors were orthogonal which these are so let's see how this works with projections so let's try it with projections what we said before was that if i take my version of the vector and dot it with a bear's axis so the first of bear's axes is that in my world then i will get the answer of the vector in bears world so that gives me uh 1 over root 2 times one over root two which is a half of one plus three which is four so that gives me two and that's going to be the first component of bears vector because it's the first of bears axes and i can do it again with the other of bear's axes so that's 1 over root 2 1 3 that's the vector in my world with the other of bear's axes which is 1 over root 2 times minus 1 1. and when i do that dot product what i'll get 1 1 over root 2's will multiply to give me a half again and i've got 1 times minus 1 plus 3 times 1 is a total of 2 which is 1 and that's bear's vector notice 2 1. so i've used projections here to translate my vector to bears vector just using the dot product now remember with the vector product um what i'd have to do is i'd have to remember to normalize when i did the multiplication by theirs vectors i'd have to normalize by their lengths but in this case their lengths are one so it's actually really easy so we don't have to do the complicated matrix maths we can just use the dot product if bears vectors are orthogonal now there is one last thing if you try this with the example we did before with bears vectors of 3 1 and 1 1 so before we had those being bears vectors if you try the dot product with those because they're not orthogonal to each other it won't work and give it a go for yourself and verify that that it really won't work that they need to be orthogonal for this to work if you have them not being orthogonal you can still do it with the matrix transformation you just can't do it with the dot product now let's look at how we do the transformation of a vector that's already in a change basis last time we looked at bears basis bears bases had a first axis of 3 1 and a second axis of 1 1 and let's say i have a vector x y defined in bears basis and say i want to transform it by doing something like a rotation of 45 degrees but the problem is i don't know how to write a 45 degree rotation in bear's funny coordinate system i only know how to write down a 45 degree rotation in my normal 1001 system so in my system which is 1 0 01 a 45 degree rotation rotates one o up like that so it becomes if it's still a unit vector one over root two one over root two that is a normalized one one and it takes one round to minus 1 over root 2 1 over root 2. that is that 45 degrees there that 45 degrees there so i can write down the rotation in my notation let's call it r being a 45 degree rotation as being 1 over root 2 times 1 1 minus 1 1. that's what a 45 degree rotation is in my world so what i need to do is first transform the vector x y into my basis and i do that by multiplying it by b right then i can apply my nice sensible rotation r to that vector that's now in my basis so what i get here when i do rb i've got the vector in my coordinate frame now the problem is bear doesn't care about my world he wants to get the rotation in his basis so then i have to transform the resulting vector back into bears basis and i do that by applying b to the minus one and b to the minus one i get by flipping the terms on the leading diagonal taking minus the off diagonal terms and dividing by the determinant here which is two so i multiply by a half so that then gives me the vector back in the vector back in bears in bears frame so overall what i've done is i've done b to the minus 1 times r times b and what that's giving me is it's giving me the rotation in bear's coordinate system which is really neat so now all we have to do is do the sums and when we do that our b gives us this uh and b to the minus one rb gives us this i've written them down there so pause and ponder if you uh want to verify those on your own so this is what a 45 degree rotation looks like in bear's coordinate system notice that it's completely different to the one in my standard basis it isn't very easy necessarily or obvious to enter it just out of your head you have to do the calculation so if you want to do some kind of transformation but in some funny basis this equation b to the minus 1 rb is going to be very useful to step back here the point is that if we want to transform to non-or normal coordinate systems then the transformation matrices also change and we have to be mindful of that and this is the sort of algebra you see all the time we've got the transformation matrix r wrapped around by b b to the -1 that does the translation from my world to the world of the new basis system so thanks a lot bear you really helped us out to understand all this stuff so what we've done in these two videos is we've looked at how the numbers in a vector change when we change the basis and we thought about the coordinate systems and how to do transformations in non-orthogonal coordinate systems it's been quite hard work but this really sets us up for example in principle component analysis to operate in different basis systems now it's going to be really useful if we can make a transformation matrix whose column vectors make up a new basis all of whom's vectors are perpendicular or what's called orthogonal to each other in this video we're going to look at this and why this is but first i want to define a new operation on a matrix called the transpose this is where we interchange all the elements of the rows and columns of the matrix so i denote the transpose as a t and i say that the ijth element of the transpose of a is equal to uh the elements on the opposite diagonal aji so if i had a matrix like one two three four and i wanted to know it's transpose then i'd interchange the elements uh that are off the diagonal so the one and the four stay where they are because if it was if i and j were the same one one one one they would stay the same the same for two two but the elements element one two interchanges with element two one so they go like that so one two three four becomes when i transpose it one three two four now let's imagine that i have a square matrix of dimension n by n um that defines a transformation with a new basis uh and the columns of that transformation matrix a are some vectors a1 a2 uh that are the basis vectors in the new space right as we've said before and i'm going to make this a special matrix where i impose the condition that uh these vectors are orthogonal to each other and they have unit length that is a i dotted with aj is equal to zero if i isn't equal to j that is their orthogonal and it's equal to one if i is equal to j that is their of unit length now let's think about what happens if i multiply this matrix on the left by a transpose so a transpose is going to be given by just uh flipping the rows and columns of all of the elements across the leading diagonal of a so that is i'm going to have a 1 is going to become a row a 2 is going to become the next row all the way down to a n because i flip all of the elements across the leading diagonal so when i multiply a t by a let's see what happens it's going to be quite magical so if i get a t times a then i'm going to get well the first element here is going to be the first row times the first column and that's a1 times dotted with a1 and that's one the second element here the first row the second column is a1 dotted with a2 and that's zero and a1 dot a3 all the way up to a n is going to give me a whole series of zeros where this guy remembers a n and then when i do the second row and the first column i'm going to get another zero i want to do the second row and the second column i'm going to get a one second row third column zero all the way across and when i do the third one i'm going to get the same again i'm going to get zero zero like it's all i mean zero except a3 times a3 and that's going to be that element all the way across and what i'm gradually building up here is i'm building up the identity matrix so what i found is that a t times a gives me the identity matrix and what that means is is that a t is a valid inverse of a which is really kind of neat a set of unit length basis vectors that are all perpendicular to each other are called an orthonormal basis set orthonormal and the matrix composed of them is called an orthogonal matrix one thing also to know about an orthogonal matrix is that because all the basis vectors in it are of unit length it must scale space by a factor of one so the determinant of an orthogonal matrix must be either plus or minus one the minus what arises in the new basis if the new basis vector set flips space around as they sort of invert it they make it left-handed notice that if a t is the inverse then i should be able to post multiply a by a t and get the identity um as i can i can do this either way around which also means by the same logic that the rows of the orthogonal matrix are for orthonormal as well as the columns which is kind of neat and we saw in the last video that actually the inverse is the matrix that does the reverse transformation so the transpose matrix of an orthonormal basis vector set is itself another orthogonal basis vector set which is it's really neat now remember that in the last module on vectors we said that transforming a vector to a new coordinate system was just taking the projection or dot product of that vector so say that's the vector with each of the basis vectors so basis vector this one basis vector that one and so on um so long as those basis vectors were orthogonal to each other if you want to pause and think about that for a moment in the light of all we've learned about matrices just think look and think about this for a moment now in data science what we're really saying here is that wherever possible we want to use an orthonormal basis vector set when we transform our data that is we want our transformation matrix to be an orthogonal matrix that means the inverse is easy to compute it means the transformation is reversible because it doesn't collapse space it means that the projection is just the dot product lots of things are nice and pleasant and easy if i arrange the basis vectors in the right order then the determinant is one and that's an easy way to check and if they aren't just exchange a pair of them and actually then they will be a determinant one rather than minus one so what we've done in this video is look at the transpose and that's led us to find out about the most convenient basis vector set of all which is the all for normal basis vector set which together make the orthogonal matrix whose inverse is its transpose so that's really cool we've said several times now that life is much easier if we can construct an orthonormal basis vector set but we haven't talked about how to do it so in this video we'll do that starting from the assumption that we already have some linearly independent vectors that span the space we're interested in so let's say i have some vectors v and i've got a whole group of them v1 v2 all the way up to vn and there's enough of them that they span the space so let's sort of sketch them out so i've got say a v1 here a v2 over here another v3 down there somewhere and uh they're linearly independent let's assume that if you want to check linear independence you can write down their columns in a matrix and check the determinant isn't zero if they are linearly dependent uh one of that would give you a zero determinant um but they aren't orthogonal to each other or of unit length my life would probably be easier if i could construct some orthonormal basis somehow and there's a process for doing that which is called the gram schmidt process the gram schmidt process which is what we're going to look at now let's take arbitrarily the first vector in my set call him v1 so we take v1 in this first step uh she let's call him she actually she gets to survive unscathed so we're just going to normalize her and we're going to say that my eventual first basis vector e is going to be equal to v 1 just normalized to b of unit length just divided by that its length so e is just going to be some normalized version of v1 and i can now think of v2 as being composed of two things one is a component let's do this in orange a component that's in the direction of e1 like that plus a component that's perpendicular to e1 and uh but the component that's in the direction of e1 i can find by taking the vector projection of v2 onto e1 so i can say v2 is equal to the vector projection of v2 onto e1 um dotted together and if i want to get that actually as a vector i'll have to take e1 um which is of unit length i'd have to divide by the length of e1 but the length of e1 is one so forget it um and if i take that off of v2 then i'll have this guy and let's call him u2 so i can then say that u2 so plus u2 so i can then rearrange this and say that u2 is equal to v2 minus this projection v2 dot e1 times e1 and if i normalize u2 if i take u2 divided by its length then i'll have a unit vector which is going to be normal 2v1 so if i take a normalized version of that let's say it's that will be e2 and that will be at 90 degrees to e1 so it'll actually be there e2 once i've moved it over and that will be another unit length vector normal to e1 so that's the first part of taking an orthonormal basis now my third vector v3 isn't a linear combination of v1 and v2 so v3 isn't in the plane defined by v1 and v2 so it's not in the plane of e1 and e2 either so i can project v3 down let's say something like that onto the plane of e2 and e1 and that projection will be some vector in the plane composed of e2s and e1s so i can then write down that v3 minus v3 dotted with e1 e1s that's going to be the component of v3 that's made up of e1s minus v3 dotted with e2 e2s that's the component of v of v3 that's made up of e2s and then all that's going to be left is going to be this perpendicular guy there so that's going to be a perpendicular vector which we'll call u3 which is perpendicular to the plane now some funny 3d space so diagram gets quite messy and then if i normalize u3 divide by the length of u3 then i'll have a unit vector which is normal to the plane normal to the other two so now i've got an orthonormal basis for e1 e2 e3 and i can keep on going through all the vn's until i've got enough orthonormal basis vectors to complete the set and span the space that i originally had but i've gone from a bunch of awkward non-orthogonal non-unit vectors to a bunch of nice orthogonal unit vectors an orthonormal basis set so that's how i construct an orthonormal basis set and make my life easy so that my transformation vectors are nice my transformation matrices are nice sorry and so that i can do the transposes the inverse and all those lovely things so i can use dot product projections for the transformations all those nice things that are going to make my life very very much nicer whenever i'm doing any transformations or rotations or whatever it is i want to do with my vectors so that's going to be really nice this is a really nice process and what we'll do next is we'll apply this we'll try this for an example and see how it rolls and then apply that to doing a transformation okay so let's put all this together let's use our transformations knowledge and our bases knowledge in order to do something quite tricky and see if we can't actually make our life quite simple what i want to do here is know what a vector looks like when i reflect it in some funny plane for example the way this board works when i write on the light board here if you're looking at it it will all the writing would appear mirrored but what we do to make that work is we reflect everything in post-production left right and then everything comes out okay the example we're gonna do here ask what the reflection of bear say something in a mirror would look like to me uh if he the mirror was off at some funny angle now my first challenge is going to be that i don't know the plane of the mirror very well but i do know two vectors in the mirror one one one and two zero one and i've got a third vector which is out of the plane of the mirror which is at 3 1 -1 that's my third vector so i've got vectors v 1 v 2 and v 3 and these two guys are in the plane of the mirror we could draw it something like v1 and v2 and they're in some playing like this and v3 is out of the plane so i've got v3 there v1 and v2 so first let's do the gram-schmidt process and find some awful normal vectors describing this plane and its normal v3 so my first vector e1 is going to be just the normalized version of v1 v1 here is of length uh root 3 1 squared plus 1 squared plus 1 squared all square rooted so it's going to be 1 over root 3 times 1 1 1. that's a normalized version of v1 so then we can carry on and i can find u2 is equal to v2 minus some number of e1 so that's going to be then the sum number is going to be the projection of v2 onto e1 times e1 so that's going to be 201 minus 201 dotted with e1 which is 1 over root 3 1 1 1 times 1 over root 3 1 1 1 because that's e 1. so that's 201 minus the root 3s are going to come outside so i can just have them being a third 201 dotted with one more one is two plus zero plus one is three so that actually goes and has a party and becomes one and yeah okay i confess i fixed the example so it's 201 minus 1 1 1 which is going to give me 1 minus 1 1 minus 1 is 0. so 1 minus 1 0 that's u2 now if i want to normalize u2 i can say e2 is equal to the normalized version of u2 which is just 1 over root 2 times 1 minus 1 o so then i just need to find u3 and i can do the same again with u3 i can say that that's equal to v3 minus the projection of v3 onto e1 minus the projection of v3 onto e2 so that's going to be 3 1 minus 1 minus 3 1 minus 1 dotted with 1 over root 3 1 1 1 and that's a number and it's going in the direction of the unit vector of e2 of e1 minus v3 dotted with e2 so that's 3 1 minus 1 dotted with 1 over root 2 1 minus 1 0. times e 2 which is 1 over root 2 1 minus 1 0. so it's quite a complicated sum um so i've got but i've got an answer here which i can do 3 1 minus 1 minus the 1 over root 3's come out again 3 plus 1 minus 1 is 3 so that goes and i've got one one one there so that becomes one minus uh the halves are going to come out the one over root twos i've got three minus one minus zero so that's two so they cancel and become one again as i said i fixed the example to make my life easy um so then i've got three one minus one minus one one one minus 1 minus 1 0. so therefore i get an answer for u 2 being 3 minus 1 minus 1 so that's 1. 1 minus 1 is 0 minus -1 is plus 1 minus 1 minus 1 is -2 and so i can then normalize that and get e3 so e3 is just the normalized version of that which is going to be one over root six of one one minus two now let's just check so one one minus two is normal to one mono it is normal to have one one one those two are normal to each other so our normal base is set uh just need to make sure that's one over root six they are all of unit length so i can write down my new uh transformation matrix which i'm going to call e it's the transformation matrix described by the basis vectors e1 e2 e3 so i've got e1 e2 e3 all written down as column vectors and that's going to be my transformation matrix that first contains the plane notice and then contains the normal to the plane so i've redrawn everything just to get it all more compact so we can carry on we've got our original two vectors v1 and v2 and we've defined e1 to be the normalized version of v1 and we've defined e2 to be the perpendicular part of v2 to e1 normalized to be of unit length so these are all in a plane and then e3 is normal to that plane it's the bit of v3 that we can't make by projection onto v1 and v2 then of unit length now say i've got a vector r over here r now what i want to do is reflect r down through this plane so i'm going to drop r down through this plane there is when he intersects the plane and then out the other side to get a vector r prime and let's say that r has some number like i know two three five two three five now this is going to be really awkward this planes off at some funny angle composed of these vectors and even these basis vectors it's some funny angle and then how do i drop it down and do the perfect there's gonna be a lot of trigonometry but the neat thing here is that i can think of r as being composed of a vector that's in the plane so some vector that's composed of uh e1s and e2s um sorry so here's e1 here and some vector that's normal so some vector that's some number of e3s and when i reflect it through the bit that's in the plane is going to be the same but this bit that's some number of e3s this bit here i'm just going to make into minus this bit here so if i wrote that down as a transformation matrix the transformation matrix in my basis e is going to be to keep the uh e1 bit the same keep the e2 bit the same so that's the e1 bit that's the e2 bit and then reflect the e3 bit from being up to being down 0 0 -1 so that's a reflection matrix in e3 so that's a reflection in the plane so just by thinking about it quite carefully i can think about what the reflection is and that te is in the basis of the plane not in my basis but in the basis of the plane so that is easy to define and therefore if i can get the vector r defined in the plane's basis vector set in the e basis i can then do the reflection and then i can put it back into my basis vector set um and then i've have the complete transformation so a way of thinking about that is that if i've got my vector r and i'm trying to get it through some transformation matrix to r prime but that's going to be hard that's going to be tricky but i can transform it into the basis of the plane so i can make an r in the basis of the plane and i'm going to do that using e to the minus one e to the minus one is the thing that got me into i've got a a vector of mine into bears basis remember then i can do my transformation i can do my reflection uh in the plot basis of the plane i can do my reflection transformation and then i will get r uh that's been transformed primed in the basis of the plane then i can read that back into my basis by doing e because e is the thing that takes bear's vector and puts it back into my basis so if i go the i can avoid the hard thing by going around doing these three operations so r e to the minus 1 e inverse t in the e basis e if i do those three things i've done the complete transformation and i get r prime so this problem reduces to doing that matrix multiplication um so we've got that we've got that so we can just do the math now and then we'll be done so i've just put the logic up there so that i can uh have the space down here for later and i've put the transformation we're going to do there a couple other things to note one is because e we've carefully constructed by our gram schmidt process to be orthonormal we know that e transpose is the inverse so calculating the inverse here isn't going to be a pain in the neck the other thing is compared to the situation with bear where we're changing bases here we're changing from our vector r to bears or actually the plane's coordinate system then we're doing the transformation of the reflection in the plane and then we're coming back to our basis so the e and e to the minus one are flipped compared to the last video because we're doing the logic the other way about it's quite neat right the actual multiplication of doing this isn't awfully edifying it doesn't build you up very much it's just doing some arithmetic so i'm going to write it down here and if you want to verify you can pause the video and then we'll come back and we'll comment on it so this is uh t e times the transpose of e then i take that and multiply it by e itself and i get e t e e transpose which is this guy simplifies to this guy so that's t all comes out quite nicely so that's very very nice so then we can apply that to r so we can say that t times r is equal to t times our vector 2 3 5 and that's going to give us r prime and that gives us an answer of one third of eleven fourteen five so r prime here is equal to one third of eleven fourteen and five so that's a process that would have been very very difficult to do with trigonometry but actually uh with transformations once we get into the plane of the mirror and the normal to the mirror then it all becomes very easy to do that reflection operation and it's quite magical really it's really amazing so that's really nice right it's really cool so what we've done here is we've done an example where we've put everything we've learned about matrices and vectors together to describe how to do something fun like reflect a point in space in a mirror this might be useful for instance if you want to transform images of faces for the purposes of doing facial recognition you know transform my face from being like that to being like that and then we could use our neural networks on machine learning to do that facial recognition part in summary this week we've gone out into the world with matrices and learned about constructing orthogonal bases changing bases and we've related that back to vectors and projections so it's been a lot of fun and it sets us up for the next topic which sam is going to lead on eigenvalues and eigenvectors welcome to the final module i'm dr sam cooper from the dyson school of design engineering at imperial college london and i'm an energy researcher who uses machine learning as a tool to help me understand lithium-ion batteries i'll be taking over from dave for the last topic of the linear algebra course but if you continue with the specialization i'll also be leading the next course which is on multivariate calculus in this final module we're going to be focusing on eigen problems which will require us to draw on nearly all of the ideas that you've learned in the previous modules then to finish the course you'll be applying this tool in a coding exercise to recreate google's famous pagerank algorithm which takes your internet search results and displays them in some kind of convenient order i hope you enjoy this module and make use of the forums to let us know how you're getting on the word eigen is perhaps most usefully translated from the german as meaning characteristic so when we talk about an eigen problem we're talking about finding the characteristic properties of something but characteristic of what this module like the previous weeks will try and explain this concept of eigenness primarily through a geometric interpretation which allows us to discuss images rather than immediately getting tangled up in the maths when i first learned this topic not only did we start by grinding through formulae we also spent very little time discussing what all the maths was doing this topic is often considered by students to be quite tricky but it's my belief that once you know how to sketch these problems the rest is just algebra so as you've seen from previous weeks it's possible to express the concept of linear transformations using matrices these operations can include scalings rotations and shears often when applying these transformations we are thinking about what they might do to a specific vector however it can also be useful to think about what it might look like when they're applied to every vector in the space this can be most easily visualized by drawing a square centered at the origin and then seeing how your shape is distorted when you apply the transformation so if we apply a scaling of two in the vertical direction the square would now become a rectangle whereas if we applied a horizontal shear to this space it might look something like this now here's the key concept we are using our little square to help us visualize what is happening to many vectors but notice that some vectors end up lying along the same line that they started on whereas others do not to highlight this i'm going to draw three specific vectors onto our initial square now let's consider our vertical scaling again and think about what will happen to these three vectors as you can see the horizontal green vector is unchanged pointing in the same direction and having the same length the vertical pink vector is also still pointing in the same direction as before but its length is doubled lastly the diagonal orange vector used to be exactly 45 degrees to the axis its angle has now increased as has its length i hope you can see that actually besides the horizontal and vertical vectors any other vector's direction would have been changed by this vertical scaling so in some sense the horizontal and vertical vectors are special they are characteristic of this particular transform which is why we refer to them as eigenvectors furthermore because the horizontal vector's length was unchanged we say that it has a corresponding eigenvalue of 1 whereas the vertical eigenvector doubled in length so we say it has an eigenvalue of two so from a conceptual perspective that's about it for 2d eigenproblems we simply take a transform and we look for the vectors who are still laying along the same span as before and then we measure how much their length has changed this is basically what eigenvectors and their corresponding eigenvalues are let's look at two more classic examples to make sure that we can generalize what we've learned here's our marked up square again and now let's look at pure shear where pure means that we aren't performing any scaling or rotation in addition so the area is unchanged as i hope you spotted it's only the green horizontal line that is still laying along its original span and all the other vectors will be shifted finally let's look at rotation clearly this thing has got no eigenvectors at all as all of the vectors have been rotated off their original span in this lecture we've already covered almost all of what you need to know about eigenvectors and eigenvalues although we've only been working in two dimensions so far the concept is exactly the same in three or more dimensions in the rest of the module we will be having a look at some special cases as well as discussing how to describe what we've observed in more mathematical terms as we saw previously eigenvectors are those which lie along the same span both before and after applying a linear transform to a space and then eigenvalues are simply amount that each of those vectors has been stretched in the process in this video we're going to look at three special cases to make sure the intuition we've built so far is robust and then we're going to try and extend this concept into three dimensions the first example we're going to consider is that of a uniform scaling which is where we scale by the same amount in each direction as you will hopefully have spotted not only are all three of the vectors that i've highlighted eigenvectors but in fact for a uniform scaling any vector would be an eigenvector in this second example we're going to look at rotation in the previous video we applied a small rotation and we found that it had no eigenvectors however there is one case of non-zero pure rotation which does have at least some eigenvectors and that is 180 degrees as you can see the three eigenvectors are still laying on the same spans as before but just pointing in the opposite direction this means that once again all vectors for this transform are eigenvectors and they all have eigenvalues of -1 which means that although the eigenvectors haven't changed length they are all now pointing in the opposite direction this third case we're going to look at a combination of a horizontal shear and a vertical scaling and it's slightly less obvious than some of the previous examples just like the pure shear case we saw previously the green horizontal vector is an eigenvector and its eigenvalue is still one however despite the fact that neither of the two vectors shown arrigon this transformation does have two eigenvectors here i've now added the second eigenvector onto the image and it shows us that although the concept is fairly straightforward eigenvectors aren't always easy to spot let's now apply the inverse transform and watch our parallelogram go back to its original square but this time with our eigenvector visible hopefully you're at least convinced that it is indeed an eigenvector and it's as it stays on its own span this problem is even tougher in three or more dimensions and many of the uses of eigen theory in machine learning frame the system as being composed of hundreds of dimensions or more so clearly we're going to need a more robust mathematical description of this concept to allow us to proceed before we do let's just take a look at one quick example in 3d clearly scaling and shear are all going to operate in much the same in 3d as they do in 2d however rotation does take on a neat new meaning as you can see from the image although both the pink and green vectors have changed direction the orange vector has not moved this means that the orange vector is an eigenvector but it also tells us as a physical interpretation that if we find the eigenvector of a 3d rotation it means we've also found the axis of rotation in this video we've covered a range of special cases which i hope have prompted the questions in your mind about how we're going to go about writing a formal definition of the an eigen problem and this is exactly what we're going to be discussing next time see you then hopefully you all now have a reasonable feeling for an eigenproblem looks like geometrically so in this video we're going to formalize this concept into an algebraic expression which will allow us to calculate eigenvalues and eigenvectors whenever they exist once you've understood this method we'll be in a good position to see why you should be glad that computers can do this for you if we consider a transformation a what we have seen is that if it has eigenvectors at all then these are simply the vectors which stay on the same span following a transformation they can change length and even point in an opposite direction entirely but if they remain in the same span they are eigenvectors if we call our eigenvector x then we can say the following expression a x equals lambda x where on the left hand side we're applying the transformation matrix a to a vector x and on the right hand side we are simply stretching the vector x by some scalar factor lambda so lambda is just some number we're trying to find values of x that make the two sides equal another way of saying this is that for our eigenvectors having a applied to them just scales their length or does nothing at all which is the same as scaling the length by a factor of 1. so in this equation a is an n-dimensional transform meaning it must be an n-by-n square matrix the eigenvector x must therefore be an n-dimensional vector to help us find the solutions to this expression we can rewrite it by putting all the terms on one side and then factorizing so a minus lambda i times x equals zero if you're wondering about where the i term came from it's just an n by n identity matrix which means it's a matrix the same size as a but with ones along the leading diagonal and zeros everywhere else we didn't need this in the first expression we wrote as multiplying vectors by scalars is defined however subtracting scalars from matrices is not defined so that i just tidies up the maths without changing the meaning now that we have this expression we can see that for the left-hand side to equal zero either the contents of the brackets must be zero or the vector x is zero so we're actually not interested in the case where the vector x is zero that's when it has no length or direction and is what we call a trivial solution instead we must find when the term in the brackets is zero referring back to the material in the previous parts of the course we can test if a matrix operation will result in a zero output by calculating its determinant so debt of a minus lambda i equals zero calculating the determinants manually is a lot of work for high dimensional matrices so let's just try applying this to an arbitrary 2 by 2 transformation let's say a equals a b c d substituting this into our eigen finding expression gives the following debt of a b c d minus lambda 0 0 lambda evaluating this determinant we get what is referred to as the characteristic polynomial which looks like this so lambda squared minus a plus d lambda plus a d minus b c equals zero our eigenvalues are simply the solutions of this equation and we can then plug these eigenvalues back into the original expression to calculate our eigenvectors rather than continuing with our generalized form this is a good moment to apply this to a simple transformation for which we already know the eigen solution let's take the case of a vertical scaling by a factor of two which is represented by the transformation matrix a equals one zero zero two we can then apply the method that we just described and take the determinant of a minus lambda i and then set it to 0 and solve so the determinant of uh one minus lambda zero zero two minus lambda equals one minus lambda two minus lambda which is of course equal to zero this means that our equation must have solutions at lambda equals one and lambda equals two thinking back to our original eigenfinding formula a minus lambda i x equals zero we can now sub these two solutions back in so thinking about the case where lambda equals one we can say one minus one zero zero two minus one times this x vector x y x one and x two must equal to 0 0 0 1 times x1 x2 therefore we've got 0 and x2 must equal zero now thinking about the case where lambda equals two at lambda equals two you get one minus two and two minus two and then you get of course minus one zero zero zero which equals 2 minus x1 0 which equals 0. so what do these two expressions tell us well in the case where our eigenvalue lambda equals 1 we've got an eigenvector where the x2 term must be 0 but we don't really know anything about the x1 term well this is because of course any vector that points along the horizontal axis could be an eigenvector of this system so we write that by saying at lambda equals 1 x our eigenvector can equal anything along the horizontal axis as long as it's zero in the vertical direction so we put in an arbitrary parameter t similarly for the lambda equals two case we can say that our eigenvector must equal zero t because as long as it doesn't move at all in the horizontal direction any vector that's purely vertical would therefore also be an eigenvector of this system as they all would lie along the same span so now we have two eigenvalues and their two corresponding eigenvectors let's now try the case of a rotation by 90 degrees anti-clockwise to ensure that we get the result that we expect which if you remember is no eigenvectors at all the transformation matrix corresponding to a 90 degree rotation is as follows a equals 0 minus 1 1 0. so applying the formula once again we get the determinant of 0 minus lambda minus 1 1 0 minus lambda which if we calculate this through comes out to lambda squared plus one equals zero which doesn't have any real numbered solutions at all hence no real eigenvectors we could still calculate some complex eigenvectors using imaginary numbers but this is beyond what we need for this particular course despite all the fun that we've just been having the truth is that you will almost certainly never have to perform this calculation by hand furthermore we saw that our approach required finding the roots of a polynomial of order n i.e the dimension of your matrix which means that the problem will very quickly stop being possible by analytical methods alone so when a computer finds the eigen solutions of a hundred dimensional problem it's forced to employ iterative numerical methods however i can assure you that developing a strong conceptual understanding of eiger problems will be much more useful than being really good at calculating them by hand in this video we translated our geometrical understanding of eigenvectors into a robust mathematical expression and validated it on a few test cases but i hope that i've also convinced you that working through lots of eigen problems is as is often done in engineering undergraduate degrees is not a good investment of your time if you already understand the underlying concepts this is what computers are for next video we'll be referring back to the concept of basis change to see what magic happens when you use eigenvectors as your basis see you then so now that we know what eigenvectors are and how to calculate them we can combine this idea with a concept of changing basis which was covered earlier in the course what emerges from this synthesis is a particularly powerful tool for performing efficient matrix operations called diagonalization sometimes we need to apply the same matrix multiplication many times for example imagine that a transformation matrix t represents the change in location of a particle after a single time step so we can write that our initial position described by vector v0 multiplied by the transformation t gives us our new location v1 to work out where our particle will be after two time steps we can find v2 by simply multiplying v1 by t which is of course the same thing as multiplying v0 by t two times so v2 equals t squared times v0 now imagine that we expect this same linear opera transformation to occur every time step for n time steps where we can write v n is t to the power of n times v zero you've already seen how much work it takes to apply a single 3d matrix multiplication so if we were to imagine that t tells us what happens in one second but we'd like to know where our particle is in two weeks from now then n is going to be around 1.2 million i.e we'd need to multiply t by itself more than a million times which may take quite a while if all the terms in the matrix are zero except for those along the leading diagonal we refer to it as a diagonal matrix and when raising matrices to powers diagonal matrices make things a lot easier in fact have a go just now to see what i mean all you need to do is put each of the terms on the diagonal to the power of n and you've got the answer so in this case t to the n is a to the n b to the n and c to the n it's simple enough but what if t is not a diagonal matrix well as you may have guessed the answer comes from eigen analysis essentially what we're going to do is simply change to a basis where our transformation t becomes diagonal which is what we call an eigen basis we can then easily apply our power of n to the diagonalized form and finally transform the resulting matrix back again giving us t to the power of n but avoiding much of the work as we saw in the section on changing basis each column of our transform matrix simply represents the new location of the transformed unit vectors so to build our eigenbasis conversion matrix we just plug in each of our eigenvectors as columns c equals eigenvector 1 2 and eigenvector 3 in this case as we're using a three-dimensional example however don't forget that some of these may be complex so not easy to spot using the purely geometrical approach but they appear in the maths just like the others applying this transform we find ourselves in a world where multiplying by t is effectively just a pure scaling which is another way of saying that it can now be represented by a diagonal matrix crucially this diagonal matrix d contains the corresponding eigenvalues of the matrix t so d equals lambda 1 lambda 2 and lambda 3 with zeros elsewhere we're so close now to unleashing the power of eigen the final link that we need to see is the following bringing together everything we've just said it should now be clear that applying the transformation t is just the same as converting to our eigen basis applying the diagonalized matrix and then converting back again so t equals c d c inverse which suggests that t squared can be written as c d c inverse multiplied again by c d c inverse so hopefully you've spotted that in the middle of our expression on the right hand side you've got c multiplied by c inverse but multiplying a matrix and then by its inverse is just the same as doing nothing at all so we can simply remove this operation equals c d d c inverse and then we can finish this expression by saying well this must be c d squared c inverse we can of course then generalize this to any power of t we'd like so finally we can say that t to the power of n is going to equal c d to the power of n multiplied by c inverse we now have a method which lets us apply a transformation matrix as many times as we'd like without paying a large computational cost this result brings together many of the ideas that we've encountered so far in this course and in the next video we'll work through a short example just to ensure that this approach lines up with our expectations when applied to a simple case see you then now that we've walked through the theory of eigenbases and diagonalization let's have a go at a simple 2d example where we can see the answer graphically so we'll know whether or not our method has worked as expected consider the transformation matrix t equals 1 1 0 2. hopefully you'd feel fairly comfortable at this point in drawing the transformed square and vectors that were used in the previous examples as the first column is just 1 0 this means that our i hat vector will be unchanged however the second column tells us that j hat the second vector will be moving to the point 1 2. let's also consider the orange diagonal vector to point one one multiplying through gives us one one zero two multiplied by one one so thinking about rows times coals we're going to get one plus 1 and 0 plus 2 which gives us 2 2. it's interesting to consider that this particular transform could be decomposed into a vertical scaling by a factor of two and then a horizontal shear by a half step because we've chosen such a simple transformation hopefully you've already spotted the eigenvectors and can state their eigenvalues these are at lambda equals 1 our eigenvector is 1 0 and at lambda equals 2 our eigenvector equals 1 1. now let's consider what happens to the vector minus 1 1 when we apply t so 1 1 0 2 applied to minus 1 1 this time is going to equal rose times coles minus one plus one and zero plus two which equals zero two and if we apply t again we're gonna get the following 1 1 0 2 applied to 0 2 which is going to equal rows times cos again 0 plus 2 and 0 plus 4. so this thing finally is going to equal 2 4. now instead if we were to start by finding t squared so t squared is going to equal this vector this matrix multiplied by itself so applying rows times coles we're going to get 1 times 1 times 1 times 0 so that's 1 rows times cos here we're going to get a 3 rows times cos here we're going to get a 0 rows times cos here we're going to get a 4. now we can apply this to our vector and see if we get the same result so 1 3 0 4 multiplied by -1 1 is going to equal rows times coals so we're going to get minus 1 plus 3 and 0 plus 4 which of course equals 2 4. we can now try this whole process again but using our eigenbasis approach we've already built our conversion matrix c from our eigenvectors so c is going to equal one zero one one but we are now going to have to find its inverse however because we've picked such a simple 2x2 example it's possible just to write this inverse down directly by considering that c would just be a horizontal shear one step to the right so c inverse must just be the same shift back to the left again so c inverse is going to equal 1 minus 1 0 1. it's worth noting that despite how easy this was i would still always feed this to the computer instead of risking making silly mistakes we can now construct our problem so t squared is going to equal c d squared c inverse which of course in our case is going to equal 1 1 0 1 multiplied by our diagonal matrix which is going to be 1 and 2. and that's all squared multiplied by c inverse which is 1 minus 1 0 1. working this through we can see that let's keep this first matrix 1 1 0 1 and work out this bit so we'll say okay this is going to be 1 and 4 on the diagonal 1 minus 1 0 1 and let's work out these two matrices here so we've got one one zero one multiplied by so we're doing rows times coles in each case so for example one zero times one zero we get a one here we're gonna do the second row in the first column we get a zero there first row and second column we're going to get a minus one there and the second row and the second column we're going to get four here okay and then working it through one more step we're gonna see that we get more more grinding we get first row first column one second row first column zero first row second column is three and second row second column is four and applying this to the vector minus one one we're going to get something like this so one three zero four applied to minus one one is going to be rows times coals so minus one plus three and zero plus four which equals 2 4 which pleasingly enough is the same result as we found before now there is a sense in which for much of mathematics once you're sure that you've really understood a concept then because of computers you may never have to do this again by hand however it is still good to work through a couple of examples on your own just to be absolutely sure that you get it there are of course many aspects of eigen theory that we haven't covered in this short video series including undiagonizable matrices and complex eigenvectors however if you are comfortable with the core topics that we've discussed then you're already in great shape in the next video we're going to be looking at a real world application of eigen theory to finish off this linear algebra course this is a particularly famous application which requires abstraction away from the sort of geometric interpretations that we've been using so far which means that you'll be taking the plunge and just trusting the maths see you then the final topic of this module on eigenproblems as well as the final topic of this course as a whole will focus on an algorithm called pagerank this algorithm was famously published by and named after google founder larry page and colleagues in 1998 and was used by google to help them decide which order to display their websites when they returned from search the central assumption underpinning page rank is that the importance of a website is related to its links to and from other websites and somehow eigen theory comes up this bubble diagram represents a model mini internet where each bubble is a web page and each arrow from a b c and d represents a link on that web page which takes you to one of the others we're trying to build an expression that tells us based on this network structure which of these web pages is most relevant to the person who made the search as such we're going to use the concept of procrastinating pat who is an imaginary person who goes on the internet and just randomly clicks links to avoid doing their work by mapping all the possible links we can build a model to estimate the amount of time we would expect pat to spend on each web page we can describe the links present on page a as a vector where each row is either a 1 or a 0 based on whether there is a link to the corresponding page and then normalize the vector by the total number of links such that they can be used to describe a probability for that page for example the vector of links from page a would be 0 1 1 1 because vector a has links to sites b to c and to d but it doesn't have a link to itself also because there are three links on this page in total we would normalize by a factor of a third so the total click probability sums to one so we can write l a equals zero a third a third a third following the same logic the link vectors of the next two sites are shown here and finally for page d we can write l d is going to equal so d is connected to b and c but not a two sites in total zero a half a half zero we can now build our link matrix l by using each of our link vectors as a column which you can see will form a square matrix what we're trying to represent here with our matrix l is the probability of ending up on each of the pages for example the only way to get to a is by being at b so you then need to know the probability of being at b which you could have got to from either a or d as you can see this problem is self-referential as the ranks of all the pages depend on all the others although we built our matrix from columns of outward links we can see the rows describe inward links normalized with respect to their page of origin we can now write an expression which summarizes the approach we're going to use the vector r to store the rank of all web pages to calculate the rank of page a you need to know three things about all other pages on the internet what's your rank do you link to page a and how many outgoing links do you have in total the following expression combines these three pieces of information for web page a only so r a is going to equal the sum from j equals 1 to n where n's all the web pages of the link matrix relevant to web page a and location j multiplied by the rank at location j so this is going to scroll through each of our web pages which means that the rank of a is the sum of the ranks of all the pages which link to it weighted by their specific link probability taken from matrix l now we want to be able to write this expression for all pages and solve them simultaneously thinking back to our linear algebra we can rewrite the above expression applied to all web pages as a simple matrix multiplication so r equals l r clearly we start off not knowing r so we simply assume that all the ranks are equally and normalize them by the total number of web pages in our analysis which in this case is four so r equals a quarter a quarter a quarter a quarter then each time we multiply r by our matrix l this gives us an updated value for r so we can say that r i plus 1 is going to be l times ri applying this expression repeatedly means that we are solving this problem iteratively each time we do this we update the values in r until eventually r stops changing so now r really does equal lr thinking back to the previous videos in this module this implies that r is now an eigenvector of matrix l with an eigenvalue of 1. at this point you might well be thinking if we want to multiply r by l many times perhaps this would be a best tackled by applying the diagonalization method that we saw in the last video but don't forget this would require us to already know all the eigenvectors which is what we're trying to find in the first place so now that we have an equation and hopefully some idea of where it came from we can ask our computer to iteratively apply it until it converges to find our rank vector you can see that although it takes about 10 iterations for the numbers to settle down the order is already established after the first iteration however this is just an artifact of our system being so tiny so now we have our result which says that as procrastinating pat randomly clicks around our network we'd expect them to spend about 40 percent of their time on page d but only about 12 percent of their time on page a with 24 on each of pages b and c we now have our ranking with d at the top an a at the bottom and b and c equal in the middle as it turns out although there are many approaches for efficiently calculating eigenvectors that have been developed over the years repeatedly multiplying a randomly selected initial guess vector by your matrix which is called the power method is still very effective for the pagerank problem for two reasons firstly although the power method will clearly only give you one eigenvector when we know that there will be n for an n webpage system it turns out that because of the way we've structured our link matrix the vector it gives you will always be the one that you're looking for with an eigenvalue of one secondly although this is not true for the four web page mini internet when looking at the real internet you can imagine that almost every entry in the link matrix will be zero i.e most pages don't connect to most other pages this is referred to as a sparse matrix and algorithms exist such that multiplication can be performed very efficiently one key aspect of the page rank algorithm that we haven't discussed so far is called the damping factor d this adds an additional term to our iterative formula so r i plus 1 is now going to equal d times l r i plus 1 minus d over n d is something between zero and one and you can think of it as one minus the probability with which procrastinating pat suddenly randomly types in a web address rather than clicking on a link on his current page the effect that this has on the actual calculation is about finding a compromise between speed and stability of the iterative convergence process there are over 1 billion websites on the internet today compared with just a few million when the pagerank algorithm was first published in 1998 and so the methods for search and ranking have had to evolve to maximize efficiency although the core concept has remained unchanged for many years this brings us to the end of our introduction to the page rank algorithm there are of course many details which we didn't cover in this video but i hope this has allowed you to come away with some insight and understanding into how the pagerank works and hopefully the confidence to apply this to some larger networks yourself this brings us to the end of the fifth module and also to the end of this course on linear algebra for machine learning we've covered a lot of ground in the past five modules but i hope that we've managed to balance the speed with the level of detail to ensure that you've stayed with us throughout there is a tension at the heart of mathematics teaching in the computer age classical teaching approaches focus around working through lots of examples by hand without much emphasis on building intuition however computers now do nearly all of the calculation work for us and it's not typical for the methods appropriate to hand calculation to be the same as those employed by a computer this can mean that despite doing lots of work students can come away from a classical education missing both the detailed view of the computational methods but also the high-level view of what each method is really doing the concepts that you've been exposed to of the last five modules cover the core of linear algebra that you will need as you progress your study of machine learning and we hope that at the very least when you get stuck in the future you'll know the appropriate language so that you can quickly look up some help when you need it which after all is the most important skill of a professional coder that's it for this course on linear algebra in our specialization on mathematics for machine learning our goal was to give you some of the underpinnings of linear algebra in order to enable you to access neural networks machine learning and data science courses more generally this is intended as a course for a social scientist engineer or physicist to develop the insight required to access other courses on machine learning tools and in order to do useful work in to solve problems with machine learning in the real world it's not intended as a foundation for a graduate linear algebra course and nor have we have the time to discuss topics like the cross product that don't have applications in machine learning we started out in the course by thinking of some data problems fitting functions to data or the apples and bananas price discovery problem which we used to take us on a journey into linear algebra first vectors and their matrices through it all you did exercises to try out what you'd learned and then took those into a couple of programming exercises in python at the end this will have given you the confidence to say that you're really getting towards having an intuitive understanding of linear algebra and how to apply it in code if you've enjoyed this course we continue the journey in machine learning and data science in the next course in this specialization multivariate calculus there we look at calculus and how to find the gradients of functions of more than one variable this will let us examine minimization and how to fit models to data once we have minimization and linear algebra and probability you'll have all the underpinning mathematical tools and concepts you need to then look at principal component analysis machine learning and neural networks sam and i and all the team here at imperial really hope that you found this course valuable and useful and fun and of course we wish you the very best for the future and we'll look forward to seeing you again in the next course welcome to introduction to multivariate calculus course two of the mathematics for machine learning specialization we'll be embarking on a whistle stop tour of calculus with a special focus on interactive animations and practical coding examples i'm dr sam cooper from the dyson school of design engineering at imperial college london and i use machine learning as a tool to help me characterize and design energy materials i'm also an online learning addict to understand multivariate calculus you need to be able to apply the linear algebra methods covered in the previous course so if you've not completed it already you may wish to go back many important machine learning approaches such as neural networks have calculus at their very core so the aim of this course is for you to see the connection between the maths and the meaning the ideal outcome will be for you to finish this specialization and then feel confident enough to immediately dive into one of the many wonderful applied machine learning courses already available online we're going to start the course by laying down the foundations of calculus through the use of intuition building animations then we're going to generalize these concepts to multi-dimensional systems where you will discover how to navigate mountain ranges in the dark next you'll be learning exactly how calculus is used to train neural networks as well as working through how we would actually write some code to do this using the python programming language finally you will put all of your multivariate calculus skills into action with my colleague professor david dye who will explain the fundamentals of regression before asking you to write some code to fit complicated functions to real data i really hope you enjoy the course and i look forward to hearing from you in the forums welcome to module one of six on this course introducing you to various concepts in calculus that we believe you'll find useful when studying machine learning we start right from the basics but build you up fairly quickly to some interesting applications in modules five and six this week we'll be focusing on the fundamental theory of calculus as well as some handy rules to speed things up where possible i'm going to try and represent the concepts graphically so that you can see the derivations rather than just reading them this sometimes means that we'll be skimming over a few details but i'll also provide you with links to the more rigorous descriptions so that you can check them out if you're interested speaking for myself i find it generally true that if i can't draw something then i don't really get it even if i could write down all the relevant equations so for the tests i'm going to be asking you to identify solutions on graphs rather than grinding through the algebra for the most part i hope you enjoy this first module and i look forward to meeting many of you in the discussion forums before diving into calculus we should first talk briefly about what functions are and where we use them essentially a function is a relationship between some inputs and an output so for example if i had a function for modeling the distribution of temperature in this room i might input the x y and z coordinates of a specific location i'm interested in as well as the time t and then the function would return me the temperature at that specific point in space at that moment in time like so many areas of maths even if the idea is quite straightforward often the notation can make things unnecessarily confusing we'll be confronted by this again later in the course and although it's sometimes fairly arbitrary historical reasons that decide this like different people inventing different parts of maths at different times it can also be because a particular notation style is more convenient for the specific application that it was developed for however and here is where a lot of the problems seem to arise in order to use and play with the interesting applications of maths it requires you to have done quite a large amount of often quite boring groundwork mathematical language is like learning any other language in that respect you can't enjoy french poetry until you've learned a lot of french vocabulary and grammar including all its quirks and irregularities quite understandably some people find this off-putting my french is still terrible for example this is made worse by the fact that most people have not even realized that there is maths poetry waiting for them at the end of this algebra tunnel machine learning is a whole genre of this poetry so stick with us for the rest of the specialization and you'll be ready to appreciate it and even write some of your own back to functions we often see expressions such as f of x equals x squared plus three perhaps the only thing worth mentioning about this notation style is yes i admit it it is absurd that you should somehow just know that f brackets x means f is a function of x and not f multiplied by x sometimes this gets genuinely unclear when other bracket terms appear in your expression for example f of x equals this thing over here you can assume that g h and a are all not variables otherwise i would have to write f of x g h and a but you could only know for sure what was going on here if it was explained to you with more context for example is g a function being applied to x what about h and a over here maybe they're both just constants although you do face the same problem of missing context in any language like the famous panda who goes to a restaurant and eats shoots and leaves and they were both delicious i can only apologize on behalf of maths and hopefully console you with two facts firstly maths is at the core of my day job and i often still get confused myself and secondly i still love it and it get better at second guessing any missing context all the time just like any other language in the following exercise i'm either going to show you some data or describe a concept to you and you're going to have to select a function which you think might be used to represent it this is not intended to be a difficult exercise but what i would like you to understand is that selecting a function is the creative essence of science this process of selecting a candidate function or hypothesis to model a world is what the great geniuses of science are remembered for there then follows a potentially long and difficult process of testing this hypothesis but there will be nothing to test without that first creative step calculus is simply the study of how these functions change with respect to their input variables and it allows you to investigate and manipulate them but ultimately it's just a set of tools and by the end of this course you'll be using them yourself to model some real world data in the last video i told you that calculus is just a set of tools for describing the relationship between a function and the change in its variables and so in this video we're going to explore what this means and how it might be useful let's start by having a look at a classic example a speed versus time graph for a car the most obvious thing this graph tells us is that the car's speed is not constant because a constant speed would just be a flat horizontal line furthermore starting from zero speed at zero time this car's speed is initially increasing with time which is another way of saying that it is accelerating towards the end of the time period the car's speed is shown to be rapidly decreasing meaning that it is decelerating there's a lot of information in this graph and calculus will allow us to extract much more than just the speed as we've already said a horizontal line implies a constant speed and then the more the line is sloping up the greater the acceleration in fact acceleration can be defined as the local gradient of a speed time graph and clearly acceleration itself is also a function of time in our example we refer to the gradient at a single point as the local gradient and we can illustrate this concept by drawing a tangent line which is a straight line that touches the curve at a point and is also the same gradient of the curve at that point after the initial acceleration the car's speed reaches a peak and then begins to decelerate again deceleration has a negative slope by recording the slope of these tangent lines at every point we could plot an entirely new graph which would show us acceleration against time rather than speed against time before we plot this for the complex case let's think about what this acceleration time graph would look like for a car travelling at constant speed constant speed we've got a flat horizontal line on our speed time graph then we can say that its gradient is of course zero so the acceleration time graph would also just be a horizontal line but in this case an acceleration equals zero going back to our more complex place let's just talk through what we should expect before we have a go at plotting it so initially the gradient is positive and fairly constant before it drops to zero at the peak it then becomes negative for a period before returning to zero let's now take a look at the graph for acceleration versus time overlaid onto the speed time graph don't forget the vertical axis for the blue line is speed and will have units of distance per time whereas the vertical axis for the orange line is acceleration and will have units of distance per time squared because they've got different units we could scale either of these two lines vertically in this plot and the meaning would still be identical however these have been scaled just to make the most use of this plot area are available you can see that the points at which the acceleration function is 0 ie where it crosses the horizontal axis coincide with where the speed time graph is flat and has zero gradient as we would expect although we will be discussing the formal definition of a derivative in a later video what we've just done by i is the essence of calculus where we took a continuous function and described its slope at every point by constructing a new function which is its derivative we can in principle plot the derivative of the acceleration function following the same procedure where we simply take the slope of the acceleration function at every point this is the rate of change of acceleration which we can also think of as being the second derivative of the speed and it's actually referred to as the jerk of the car think of the jerky motion of a car as it stops and starts now you may have never heard of this concept before but hopefully just by telling you that it's the derivative of the acceleration curve this should be all you need to know to approximately sketch the jerk also very interesting is the idea of taking our baseline speed function and trying to imagine what function this would have been the gradient of as in applying the inverse procedure to the one that we've just discussed we can refer to this as the antiderivative which for those who've done calculus before you may remember that it's closely related to something called the integral for the example we're discussing here it would represent the distance of the car from its starting position this should make more sense when you consider that the change in the distance with respect to time i.e the slope of the distance time graph i.e how much distance you are covering per unit time is just the speed this analysis of slopes is all we're going to discuss in the video and even though you haven't laid out yet the formal definition of calculus you should already be able to answer lots of gradient type questions in the following exercise which will put you in a very strong position to start thinking about differentiation more formally now that we've explored the relationship between functions and their gradient we should be ready to lay out the more formal definition of a derivative all we're going to do is translate the understanding about gradients that we saw in the previous video into some mathematical notation that we can write down unfortunately this is the bit that a lot of people seem to find uncomfortable so i will do my best to make it as painless as possible we talked in the last video about horizontal lines having a gradient of zero whilst upwards and downward sloping lines having positive or negative gradients respectively we can write down a definition of this concept by taking the example of a linear function which has the same gradient everywhere so if we start by picking any two points let's say here and here we can then say that the gradient of this line is equal to the amount of that function that increases in this interval divided by the length of the interval that we're considering this description is often just to condense to the expression rise over run where rise is the increase in the vertical direction and run is the distance along the horizontal axis so we've got the rise here and the run down here if our function was sloping down and we pick points at the same horizontal locations then our run would be the same but our rise would now be negative so our gradient equals rise divided by run fairly straightforward so far but how does it relate to the more complicated functions we saw previously where the gradient is different at every point now the rise over run type gradient varies depending on where we choose our points let's pick a single point where we'd like to know the gradient which we'll say is at point x on the horizontal axis the value of our function at this point is therefore clearly just f of x we're going to use the same logic as before so we now need to pick a second point to draw our rise over run triangle we can call the horizontal distance between our two points delta x where as usual a delta is being used to express a small change in something which means our second one must be at position x plus delta x we can also write down the vertical position of our second point as a function f evaluated at our new location x plus delta x ie f of x plus delta x so we can now build an expression for the approximate gradient at our point x based on the rise of a run gradient between point x and any second point remembering that the run will be our distance delta x and our rise is just the difference in heights of the two points the last step in this process is simply to notice that for nice smooth continuous functions like the one we're showing here as delta x gets smaller the line connecting two points becomes a better and better approximation of the actual gradient at our point x we can express this concept formally by using the limit notation scheme which says that as delta x goes to zero our expression will give us a function for our gradient at any point we choose which we write as f dash of x or d f by dx depending on which notation scheme you prefer this is a slightly strange concept as we're not talking about delta x equals zero as dividing by zero is not defined but instead just when x is extremely close to zero this process of extreme rise over run is what differentiation is so when you're asked to differentiate a function this literally just means substitute the function into this expression we could fill up several videos with more robust interpretations of this infinitely small but non-zero delta x but for now just don't worry about it too much we know more than enough to continue on our journey let's now put our new derivative expression into practice and see if it works first we should try this out on a linear function once again as we know that the answer is just going to be a constant so what will the gradient of f of x equals three x plus two b so we can immediately sub this straight in and say okay f dash of x is going to equal the limit as delta x goes to 0 of so we're taking our function and subbing it in 3 x plus delta x plus 2 minus 3 x plus 2 all divided by delta x okay and now we just work through the algebra so we can say the limit of delta x goes to zero of and so we can expand these brackets out so we're going to get three x plus three delta x plus two minus three x minus 2 all divided by delta x okay and now we can look at this and say some of the terms are going to cancel so this 3x here goes with this 3x here and this plus 2 here goes to this minus 2 here so we can keep going with this line and say well the limit and it's going to be 3 delta x all over delta x and then working down here we can say okay well the delta x's themselves are just going to cancel so you end up with the limit of delta as delta x goes to 0 of just 3. now this thing doesn't have a delta x in it so actually the answer for this one is just the number 3. because all the delta x terms have disappeared in this case then the limit expression has no effect so we can just ignore it the gradient of our linear function is just a constant as we expected in fact i'm sure many of you will have known already that the gradient of an equation of the form f equals ax plus b is just a but it's reassuring to see this result emerging from our expression something else to take away from this simple example is that we actually differentiated two things at once a three x bit and a plus two bit we could have differentiated them separately and then added them together and still got the same result this interchangeability of the approach is called the sum rule and it's pretty handy let's now try a slightly more complicated example so if we take the function f of x equals 5 x squared all we're going to do is take this thing and put it into our differentiation expression at the top so f dash of x equals the limit as delta x goes to zero of well we've got five x plus delta x all squared minus 5 x squared all divided by delta x and all we're going to do now is work through the algebra and see what comes out the other side so equals the limit once again as delta x goes to 0 of and let's expand this thing out so we're going to think x plus delta x squared we're going to get x squared plus 2 x delta x plus delta x squared so which means we're going to get 5 x squared plus 10 x delta x plus 5 delta x squared once again minus our 5x squared here and all of that is divided by delta x and now what do we got on this top row here so we've got a pair of 5x squares so we can get rid of those because it's 5x squared and minus 5x squared over here okay and we can also see that we've got a delta x in both the top terms and also in this bottom term here so we can get rid of that with this and this over here okay so let's write that line again so we can now say it's the limit as delta x goes to zero of we've just got 10 x plus uh 5 delta x okay and we're looking at this thing so it's saying the limit as delta x goes to 0 of this expression here now only the second term has got a delta x in it so what's going to happen as our delta x goes to 0 we're going to just forget about this term it's going to become extremely small so we can write that this is equal to just 10 x so the derivative of the expression 5x squared is just 10x we can generalize the lesson from this example to a rule for handling functions with powers of x for example if we take the function f of x equals ax to the power of b and substitute it into our differentiation expression we will find that the derivative is always f dash of x equals a b x to the power of b minus one so the original power gets multiplied to the front and then the new power is just one less than it was before this is known as the power rule and you can put this into your calculus toolbox along with the sum rule that we saw earlier so you've now seen two examples in which we've applied the limit of rise over run method to differentiate two simple functions but as you can probably imagine if we wanted to differentiate a long complicated expression this process is going to become quite tedious what we're going to see in later videos are more rules like the sum rule and the power rule which will help us speed up the process however before we do that we're just going to look at some fairly magical special case functions which differentiate in an interesting way in this video we're going to run through three special case functions which give us interesting results when differentiated the first example we're going to work through is the function f of x equals 1 over x which you can see plotted in the corner now take a minute to notice that the gradient of this function is negative everywhere except at the point x equals zero where we can't see what it is and actually something quite interesting must be happening at this point as on the negative side the function drops down presumably towards negative infinity but then it somehow re-emerges from above on the positive side this sudden break in our otherwise smooth function is what we refer to as a discontinuity we've mentioned already that the operation divide by zero is undefined which means that this function simply doesn't have a value at the point x equals zero but what about the gradient well let's sub our function into the differentiation expression to investigate so if we take it f of x equals one over x we can say that f dash of x must equal the limit as delta x goes to zero of 1 over x plus delta x minus 1 over x all divided by delta x and we look at this thing and we say we're going to have to make this top row the numerator have a single fraction so we're gonna have to combine these two fractions here which means that we're gonna have to make the denominators the same so we'll multiply top and bottom by x here and top and bottom by x plus delta x here so we can say that the limit of x over x brackets x plus delta x minus x minus x plus delta x over x brackets x plus delta x all divided by once again delta x now looking at these two we can see that we've got an x and a minus x both with the same denominator so we can subtract these now so this cancels with this so we can say okay this thing is going to be the limit of minus delta x divided by x brackets x plus delta x all divided by delta x now at this point you can see that you've got a delta x at the very top and a delta x at the bottom so once again these two terms cancel each other out and what you're left with is the limit as delta x goes to zero of minus 1 divided by x squared we're going to open up this bracket plus x delta x and this once again is where the magic of limits comes into play so we look at this function and we say okay this term here has got a delta x in it this means that as delta x becomes very very small this term itself is going to become very small and therefore eventually become irrelevant so what we can say is as we apply our limit we can actually ignore this term here entirely and we get minus 1 divided by x squared which looks like this so as we realize just by looking this derivative function is negative everywhere and like our base function the derivative is also undefined at x equals zero the second function we're going to look at i'll start simply by explaining what it does it's a function that has the special property that the value of the function f of x is always equal to the value of its own gradient f dash of x now there is a boring function which has this property which is the function f of x equals zero because as it's a horizontal line clearly both the function and the gradient are zero everywhere but there's also a much more interesting case we're not going to work through the rigorous derivation but we can skip through some of the more interesting bits so let's start by noticing that our mystery function must always either be positive or always be negative as if it ever tried to cross the horizontal axis then both the function and the gradient would be zero and so it gets stuck so we'd just be at our boring function zero again the next thing to realize is that by virtue of always increasing or always decreasing it can never return to the same value again plenty of functions could fit these criteria and focusing on the positive case they all look something like this however besides the zero function there is only one function that will satisfy all our demands this is the exponential function e to the x where e is euler's number named after the 18th century mathematician the number e which is approximately 2.718 is very important for the study of calculus but more than that e like pi turns up all over mathematics and seems to be written all over the fabric of the universe as differentiating e to the x gives us e to the x clearly we can just keep differentiating this thing as many times as we'd like and nothing's going to change this self-similarity is going to come in very handy the last special case function that we're going to talk about in this video are the trigonometric functions sine and cosine you may recall that for a right angle triangle sine of angle x multiplied by the hypotenuse r gives you the length of the opposite side to the angle and the graph of sine x looks like this let's take a look at this function and see if we can work out what shape its derivative would be by i so sine x starts with a positive gradient which gently decreases until it's zero at the top of the bump and then it starts being negative again until it gets the bottom of the next bump and it turns out that the derivative of sine x is actually just cosine x now what happens when we differentiate cosine x well actually we get minus sine x differentiating a third time gives us minus cosine x and then amazingly differentiating a fourth time brings us all the way back to our original function sine x and then the pattern of course repeats this self-similarity may remind you somewhat of the exponential function we discussed above and that is because these trigonometric functions are actually just exponentials in disguise albeit quite a convincing disguise which we won't be discussing here many of the details of the functions that we've talked about in this video were skimmed over rather quickly but for the benefit of this particular course all i need you to understand is that differentiation is fundamentally quite a simple concept even when you might not be able to battle through all the algebra you're ultimately still just looking for the rise of a run gradient at each point this pragmatic approach to calculus is going to come up again when we start talking about calculating gradients with computers sometimes we really can just find an equation for the gradient as we've been doing for all of our examples so far however if instead of a nice smooth function we just have discrete data points then it can seem as if there's nothing for us to differentiate but as we shall see rise over run comes back once again to save the day now that we've defined the differentiation operation in terms of an exact mathematical formula it's become clear that even for relatively simple functions calculating derivatives can be quite tedious however mathematicians have found a variety of convenient rules that allow us to avoid working through the limit of rise over run operation whenever possible so far we've met the sum rule and the power rule in this video we will cover a convenient shortcut for differentiating the product of two functions which sensibly enough is called the product rule it is of course possible to derive the product rule purely algebraically but it doesn't really help you to develop much insight into what's going on for all the topics covered in this course i'd always prefer you came away with some degree of intuitive understanding rather than just watching me grind through examples so let's see if we can make any progress by drawing things imagine a rectangle where the length of one side is the function f of x and the other side is the function g of x this means that the product of these two functions must give us the rectangle's area which we can call a of x now consider that if we differentiate f of x times g of x what we're really looking for is the change in area of our rectangle as we vary x so let's see what happens to the area when we increase x by some small amount delta x a quick footnote here for the case we've shown we've picked a particularly friendly pair of functions where they both happen to increase with x however this won't necessarily always be the case but it does just make drawing things a lot easier and the conclusions would ultimately be the same we can now divide up our rectangle into four regions one of which was our original area a of x as the total edge length along the top is now f of x plus delta x this means that the width of the new region must be the difference between the original width and the new width and of course the same logic applies to the height we can now write an expression for just the extra area which we will call delta a this is the sum of the area of the three new rectangles i want to avoid a drawn out conversation about limits but fundamentally i hope you can see that as delta x goes to zero although all of the new rectangles will shrink it's the smallest rectangle that is going to shrink the fastest this is the intuition that justifies how we can ultimately ignore the small rectangle and leave its contribution to the area out of our differential expression altogether now that we've got our expression for approximating delta x we return to our original question what is the derivative of a with respect to x so we want the limit of delta a divided by x ie rise over run which means we also need to divide the right hand side by delta x we are so close at this point all we need to do is slightly rearrange this equation so firstly by splitting it into two fractions and then secondly by moving f of x and g of x out of the numerators what i hope you can see now is that the first part contains the definition of the derivative of g of x and the second part contains the derivative of f of x which means that we are now ready to write down our final expression for the derivative of a with respect to x which has now just been reduced to this the derivative of a of x is just f of x times the derivative of g of x plus g of x times the derivative of f of x so we can now add the product rule to the list of tools in our calculus toolbox if we want to differentiate the product of two functions f of x and g of x we simply find the sum of f of x g dash of x and g of x f dash of x this is going to come in really handy in later videos as we start to deal with some more complicated functions see you then so far we have learned about the sum rule the power rule and the product rule in this video we will be discussing our fourth and final tool for this module which is called the chain rule following this our toolbox will then be sufficiently well stocked that we'll be ready to start tackling some heftier more interesting problems sometimes we use functions as the inputs of other functions as you can probably imagine describing this can get a little bit confusing so what we're going to do is give each of our functions meanings so that we can hopefully keep track of what's going on consider the nested function h of p of m we have the function h which is describing how happy i am as a function of p how many pizzas i've eaten that day then the pizzas i get to eat per day is itself a function of m which is how much money i make so we're still ultimately relating money to happiness but via the concept of pizza this nested function scenario comes up a lot in science and engineering as you relate chains of concepts together and i suppose this particular example i've chosen here also gives you some insight into my priorities in life so first i'm going to give you the function relating happiness and pizza which has the following polynomial form h of p equals minus a third p squared plus p plus one over five which is easily understandable from a plot what we can see is that although without any pizza it's still possible to be happy in principle my peak happiness is with about one and a half pizzas any more pizza than this and i become less happy and then beyond about three pizzas my happiness becomes rapidly negative next is our function relating pizza and money p of m equals e to the power of m minus one which is also fairly straightforward to understand by looking at a plot if you've got no money you can't buy any pizza but the more money you have your pizza purchasing power increases exponentially as at first you can take advantage of bulk buy discounts but as you start getting really rich you can buy your own pizza oven and eventually even build your own pizza factory what we'd like to know is by considering how much money i have now how much effort should i put into making more if my aim is to be happy to work this out we're going to need to know what the rate of change of happiness is with respect to money which is of course just d dh by dm for now this relatively simple example we could just directly substitute our pizza money function into our happiness pizza function which would give us this and then differentiate this thing directly however the chain rule provides us with a more elegant approach which importantly will still work even for complicated functions where direct substitution like this may not be an option consider the derivative of h with respect to p and of p with respect to m you'll notice that in this particular notation convention where the derivatives are represented by quotients the product of these two quantities looks like it would give you the desired function dh by dm and in actual fact this is a perfectly sensible way to think about what's going on this approach is called the chain rule because in a sense we are making a chain of derivative relationships now this is certainly not what you might describe as a formal derivation but it is already good enough to enable you to make use of the chain rule effectively so let's now apply this rule to our function firstly let's differentiate our two functions which give us dh by dp equals 1 minus 2 over 3p dp by dm equals e to the power of m and then multiplying these together is simple enough however we just need to remember that if we don't want pizzas p to appear in our final expression then we just need to sub in our expression for p in terms of m and then finally by simply rearranging the terms we recover the expression that we saw at the start of the video so dh by dm equals 1 3 e to the m times 5 minus 2 e to the power of m and there we have it now don't forget that although for this simple example the chain rule didn't save us a huge amount of time compared to substituting in before the differentiation don't let that fool you what's magic about the chain rule is that it for some real world applications we may not have a nice analytical expression for our function but we may still have the derivatives so being able to simply combine them with the chain rule becomes very powerful indeed before we finish let's have a quick look at our money happiness function and its derivative on a graph as we can see if you're broke and it's really worthwhile making some money but the benefit of getting more especially once you have enough pizza decreases dramatically and quickly becomes negative we now have a fourth and final tool for the module and in the next video we'll be putting them all to use in an example in this video we're going to work through a nasty looking function that will require us to use all four of the time saving rules that we've learned so far to make matters worse this function isn't going to describe anything familiar so we'll just be flying blind and have to trust the maths this will hopefully give you the confidence to dive into questions in the following exercise consider the rather nasty function f of x equals sine of two x to the power of five plus three x all over e to the seven x the essence of the sum product and chain rules are all about breaking your function down into manageable pieces so the first thing to spot is that although it is currently expressed as a fraction we can rewrite f of x as a product by moving the denominator up and raising it to the power of minus one there is actually another rule specifically for dealing with fractions directly called the quotient rule but it requires memorizing an extra expression so we're not going to cover it in this course as you can always use the approach that we've used here and rewrite your fraction as a product next we'll split f x up into the two parts of the product and work out how to differentiate each part separately ready to apply the product rule later on let's start with the first part which we can call g of x we've got the trigonometric function sine applied to a polynomial 2x to the 5 plus 3x this is a classic target for the chain rule so all we need to do is take our function and split it up into the two parts in order to apply the chain rule so we can take g of u equals sine of u and u of x equals 2 x to the power of 5 plus 3 x now we've got these two separate functions and we're going to want to differentiate each of them in order to apply the chain rule so we say okay g dash of u is going to just equal sine differentiates to cos so cos u and u dash of x is going to equal 10 x to the power of 4 plus 3. now we've got these two expressions and i'm going to use a mixed notation for convenience and i'm going to say okay well d g by d u multiplied by d u by d x is going to give us so we've got cos of u multiplied by 10 x to the power of 4 plus 3. and now we're going to want to have a final expression that doesn't include a u so we can think about these two cancel each other out and we get d g by d x and this just equals k cos of u which is just 2 x to the power of 5 plus 3 x multiplied by 10 x to the power of 4 plus 3. and that's it we now have an expression for the derivative of g of x and already we've made use of the chain rule the sum rule and the power rule now for the second half of our original expression which we will call h of x once again we can simply apply the chain rule after splitting our function up so we can say that h of v equals e to the v and v of x equals minus 7 x now once again we've got our two functions we just want to find the derivatives so h prime of v is just going to equal well we've seen this one before e to the v again and v prime of x is going to just be minus seven so combining these two back together and using different notation we can say that d h by d v multiplied by d v by d x is just going to give us we've got minus 7 times e to the minus 7 x and actually already all of our v's have disappeared so this is already our final expression as we now have expressions for the derivatives of both parts of our product we can just apply the product rule to generate the final answer and that's it we could rearrange and factorize this in various fancy ways or even express it in terms of the original function however there is a saying amongst coders that i think can be applied quite generally which is that premature optimization is the root of all evil which in this case means don't spend time tidying things up and rearranging them until that you're sure that you've finished making a mess i hope that you've managed to follow along with this example and will now have the confidence to apply our four rules to other problems yourself in calculus it's often the case that some initially scary looking functions like the one we worked with just now turn out to be easy to tame if you have the right tools meanwhile other seemingly simple functions occasionally turn out to be beasts but this can also be fun so happy hunting that brings us to the end of module one i hope those of you familiar with calculus from high school still managed to find this a useful refresher and for those of you new to calculus well done for sticking out to the end of the first module in the rest of this course we're going to be extending our analysis of functions to multi-variable systems and also applying calculus to some interesting data analysis problems however the basic ideas that we learnt in this module like the limit of rise over run operation will be at the core of everything we'll be doing so if you're happy so far then you're in a great position to make it through to the end see you in module two welcome to module 2 of this course on calculus as i mentioned at the end of the first module if you're happy with the concept of differentiation then in many ways everything we add from here will just be extensions of this core idea as well as some interesting applications the title of this course is multivariate calculus so you won't be surprised to hear that we are now going to generalize the concept of gradients to multi-variable systems the words multivariable and multivariate are typically used interchangeably and i will undoubtedly be using both in this module there is however a subtle difference between these two terms which is related to whether there are multiple input variables or multiple output variables or both but this distinction is not critical in the study of calculus that we're doing here so we can leave it to the statisticians to sweat over with more than one variable to play with we will now be able to use calculus to help us navigate around high dimensional spaces and hopefully by the end of the module we'll find some treasure see you then in the first module we started by trying to develop a strong visual intuition relating derivatives to the gradient of a function at each point we then followed this up by describing four handy rules to help speed up the process of finding derivatives however all of the examples we looked at were for systems involving a single variable so now we're going to have to see what happens when we apply the same idea to systems with many variables known as multivariate systems but before we do that we need to talk about what a variable is in the first place previously we've shown examples of problems where one of the variables is a function of the other ie y equals f of x but where it wouldn't necessarily make sense to say that x equals g of y for example a vehicle speed is clearly a function of time as at each time the vehicle can only have one speed however we could not say that the time was a function of the vehicle's speed as there might be multiple times at which the vehicle was traveling at any given speed this is why we typically refer to the speed as a dependent variable as it depends on time and so time can be thought of as an independent variable in this particular context typically when you first learn calculus you take functions containing variables and constants and then differentiate the dependent variables such as speed with respect to independent variables like time however what gets labeled as a constant or a variable can be subtler than you might think and will require you to understand the context of the problem being described let's continue using the example of a car to see how this might come up consider the following highly simplified expression relating the force f generated by a car's engine to its mass m acceleration a aerodynamic drag d and velocity v if you're driving a car then you can change your speed and acceleration by pressing the accelerated pedal to adjust the force but the mass and the drag are fixed features of the car's design this means that we would call the force an independent variable as you control it directly but your speed and acceleration depend on the variables as they are consequences of your applied force your mass and drag coefficients are clearly constants in this context however if you are a car designer looking to design each engine size in a new fleet perhaps your market research has given you a specific acceleration and speed target so in this context now your force is still the independent variable but now speed and acceleration are constants whereas the mass and drag have become variables which you can adjust by redesigning your car we refer to these slightly confusing variable design constants as parameters we often think of varying them as exploring a family of similar functions rather than describing them as variables in their own right but these are largely engineering distinctions and not the kind of thing that would worry a mathematician as you will see later in this course when fitting functions to some data it's the parameters of our fitting function which we are varying to find the best fit which means that we'll have to differentiate with respect to these parameters there is still some logic at work here but my advice to you is just don't worry about it too much the key takeaway here is you can in principle differentiate any term with respect to any other so don't get caught off guard when things you thought were constants suddenly start to vary let's now look at a different example imagine that you wanted to manufacture a metal can so you need to understand the relationship between the various key design parameters we can start by writing a reasonable approximation for the cans empty mass m by breaking the area down into pieces so the circles on top and bottom are pi r squared each and when we unroll the body we get a rectangle where the width must be the same as the circumference of the circle ie 2 pi r and we'll just call the height h so taking these areas and multiplying them by a thickness t we get the total volume of metal in the can finally multiplying this by the metals density rho we get its mass so we can say that the mass m is going to equal the area of these two little circles pi r squared times two of course plus the area of the rectangle in the middle so that's the circumference two pi r multiplied by the height h and both of these are going to be multiplied by the thickness t and the density rho so t rho and t rho i've written it in this long form here to make life a bit easier later on at this point what should we label as a constant or a variable well with the exception of pi which is definitely a constant for this universe it's not entirely clear we could in principle change any of the radius the height the wall thickness even the material's density so let's find the derivative of the cans mass with respect to each of them to calculate these all we do is when differentiating with respect to a certain variable simply consider all of the others to behave as constants so starting with an easy one let's try h so dm by dh is going to equal this thing 2 pi rt rho as the first term did not contain the variable h and we're treating all the other terms as constants then just like last week a constant simply differentiates to zero so we ignored it whereas for the second term it does contain h and so it's just multiplied by some constants so differentiating this just leaves those constants we can see from this expression that the partial derivative with respect to h no longer even contains h which is what you would expect as the mass will vary linearly with the height when all else is kept constant notice that instead of using the normal d that we saw from doing derivatives in last module we must use the curly partial symbol which signifies that you've differentiated a function of more than one variable let's now find the partial derivative with respect to the other variables starting with r so partial m by partial r is going to equal so there's an r squared term here which differentiates the 2r so we're going to have 4 pi r t rho and this term just contains an r so the r disappears 2 pi h t rho next we're going to do the variable t and we can say dm by dt is going to equal where there's just a t in here so it's not very exciting 2 pi r squared rho plus 2 pi r h rho and finally all we've got left is rho so we can say dm by d rho is just going to equal 2 pi r squared t 2 pi r squared t and 2 pi r h t plus 2 pi r h t and really although this is quite a straightforward example that's basically it for partial differentiation it's no more complicated than univariate calculus we met last module the only difference being that you have to be careful to keep track of what you're considering to be held constant when you take each derivative i hope this short introduction to multivariate calculus has helped you see that this concept is nothing to be intimidated by partial differentiation is essentially just taking a multi-dimensional problem and pretending that it's just a standard 1d problem when we consider each variable separately i look forward to showing you the pretty amazing things that we can use this concept for later in the module see you there we've already seen how to think about partial differentiation as just a simple extension of the single variable method that we derived in the last module in this video we're going to explore some slightly more complicated partial differential examples and we're also going to build something called the total derivative of a function so let's dive straight into a slightly more tricky problem than we saw last time consider the function f x y and z equals sine of x e to the power of y z squared we're now just going to work through and find the derivatives with respect to each of these three variables so let's start with x as the exponential term does not refer to x we can treat it as a constant and sine as we saw last week differentiates the cosine so we can write df by dx is just going to equal cos x e to the y z squared next we'll differentiate with respect to y in this case the sine term does not refer to y so we treat this as a constant but for the exponential term we can either apply the chain rule to it or just remember that the result of this operation for an exponential will just be to multiply the derivative of the exponent to the front and the derivative of y z squared with respect to y is just z squared so we can write d f by d y is going to equal sine of x e to the y z squared multiplied by z squared lastly we'll differentiate with respect to z once again the only z is in the exponential term so similar to the previous example we simply multiply through by the derivative of the exponential with respect to z so we just get d f by d z is going to equal sine x once again e to the y z squared and this time the derivative of this thing is 2 y z so now that we have these three partial derivatives we're going to introduce a new idea called the total derivative imagine that the variables x y and z were actually all themselves a function of a single other parameter t where x equals t minus one y equals t squared and z equals one over t and what we're looking for is the derivative of x with respect to t in this simple case we could just substitute for all our three variables directly in terms of t simplify a little bit and then differentiate directly with respect to t which gives us sine of t minus 1 times e however in a more complicated scenario with many variables the expression we needed to differentiate might have become unmanageably complex and perhaps we won't have a nice analytical expression at all the alternative approach is to once again use the logic of chain rule to solve this problem where the derivative with respect to our new variable t is the sum of the chains of the other three variable as shown in this expression since we've already got our three partial derivatives of f with respect to x y and z now we just need to find the derivatives of the three variables with respect to t and we'll have all the things we need to evaluate our expression so dx by dt is clearly 1 dy by dt is 2t and dz by dt if you remember from our 1 over t example is minus t to the power of -2 nothing hugely complicated here however when we then sub it into our total derivative expression you can probably see why i've chosen not to write this all out by hand as at first the expression is a bit of a monster however after substituting for x y and z all in terms of t and then simplifying down again we can see that the second and third terms are the same just with opposite signs and so they will cancel each other out then kind of amazingly we arrive at the same result as we saw at the beginning of the lecture so hopefully now you'll be feeling reasonably comfortable with partial differentiation as we've already done quite a lot of it and maybe you can even see why the total derivative function might be quite handy you are now ready to have a go yourselves in the following exercises and then you'll have all the pieces that you'll need in place for us to build our partial derivatives into something really useful see you then previously we saw that we can differentiate functions of multiple variables and then it isn't much harder than the univariate calculus we met at the start of the course in this video we're going to introduce the jacobian which brings in some of the ideas from linear algebra to build these partial derivatives into something particularly useful the concept of the jacobian can be applied to a variety of different problems but in the context of getting started with optimization and machine learning there is a particular scenario that comes up a lot which is the jacobian of a single function of many variables in short if you have a function of many variables so f x1 x2 x3 etc then the jacobian is simply a vector where each entry is the partial derivative of f with respect to each one of those variables in turn by convention we write this as a row vector rather than a column vector for reasons that will become clear later in the course let's start by looking at a nice simple function to see just how straightforward building a jacobian can be consider the function f of x y and z equals x squared y plus 3 z to build the jacobian we just find each of the partial derivatives of the function one by one so we've got d f by d x is going to equal we're just differentiating with respect to x so we get 2 x y when we treat everything else as conference d f by d y is going to equal just x squared and d f by d z is just going to be so this thing's just a constant and the z disappears so we just get the number three now bringing all of those together we just end up with a jacobian j which is just going to be 2 x y x squared 3. so what does this tell us we now have an algebraic expression for a vector which when we give it a specific x y z coordinate we'll return a vector pointing in the direction of steepest slope of this function the vector for this particular function has a constant contribution in the z direction which does not depend on the location selected for example at the point 0 0 0 we can see that our jacobian is just going to be j of 0 0 0 is just going to be well 0 0 again but the number 3 over here so we can see that our jacobian is a vector of length 3 pointing directly in the z direction some of the numerical methods that we will discuss later in this course require us to calculate jacobians in hundreds of dimensions however even for the 3d example we just solved graphical representation is already quite difficult so we are now going to drop down to two dimensions so that we can actually see what's going on here but to keep things interesting we're going to look at a particularly complicated but rather attractive function so here is the equation that we're going to plot but i'm showing it to you briefly just so that you can see that even though it is a bit of a monster i hope you would agree that just with the tools we've seen so far we really could calculate the partial derivatives of this thing but you wouldn't really learn anything new from grinding through this instead i'm going to show you the results graphically so here is this function with the x-axis horizontal and the y-axis vertical and the colors are indicating the value of z with the bright region suggesting high values and the dark regions suggesting low values because of the nature of this particular function i know that nothing interesting happens outside of the regions shown here so we can forget about it for now although this plot is fairly clear our intuition about the gradient is a bit lacking in this format so let's briefly make things 3d where the values of z are now also represented by the height so as we said at the start of the video the jacobian is simply a vector that we can calculate for each location on this plot which points in the direction of the steepest uphill slope furthermore the steeper the slope the greater the magnitude of the jacobian at that point hold the image of this 3d space in your mind as we now go back to 2d and rather than showing all of the grid points i used to plot the graph let's instead convert to a contour plot where just like a map of actual mountains we will draw lines along the regions of the same height which for us means the same value of zed this removes a lot of the clutter from the plot which is useful for the final step that i'd like to show you which will be adding lots of jacobian vectors on top of our contour plot however before i do that let's take a look at these four points and see if your intuition is now in tune by guessing which one will have the jacobian with the largest magnitude overlaying the jacobian vector field we can see that they are clearly all pointing uphill away from the low dark regions and towards the high bright regions also we see that where the contour lines are tightly packed this is where we find our largest jacobian vectors such as at point a whereas at the peaks of the mountains and at the bottom of the valleys or even out on the wide flat plains our gradients and therefore our jacobians are small my hope is that this clear two-dimensional example will give you the confidence to trust the maths when we come up against much higher dimensional problems later in the course see you then in this video we're going to extend the concept of the jacobian from vectors up to matrices which will allow us to describe the rates of change of a vector-valued function however before we do this we are first going to recap by applying what we've learned so far about jacobians to another simple system if we consider the function f of x and y equals e to the power of minus x squared plus y squared then using our knowledge of partial differentiation it's fairly straightforward to find its jacobian vector so we're now going to do the reverse of our approach from the last time and start by looking at the vector field of the jacobians then see if we can understand how the function must look let's start by finding the jacobian at a few specific points firstly the point -1 1 which we'll highlight on our axis in pink substituting these coordinates into our jacobian expression and simplifying we can see a vector pointing directly towards the origin next if we move further out to the point 2 2 find the jacobian we now get a much smaller vector but pointing once again directly at the origin lastly before i reveal the whole vector field let's look at what's going on at the origin itself subbing the point 0 0 returns the zero vector suggesting that the function is flat at this point which must mean one of three things either this point is a maximum minimum or something called a saddle which we'll cover later in this module however if we now reveal the rest of the jacobian vector field it becomes clear that the origin must be the maximum of this system let's now move back to the color map representation of this function where the brighter regions represent high values of f so finally we can remove the vector field and observe the function in 3d which i hope matches up with your expectations next we're going to build a jacobian matrix which describes functions that take a vector as an input but unlike our previous examples also give a vector as the output if we consider the two functions u of x and y equals x plus 2y and v of x and y equals 3y minus 2x we can think of these as two vector spaces one containing vectors with coordinates in uv and the other with coordinates in x and y each point in x y has a corresponding location in uv so as we move around x y space we would expect our corresponding path in uv space to be quite different and so it is for those of you familiar with a linear algebra you may have guessed already where this is going we can of course make separate row vector jacobians for u and v however as we are considering u and v to be components of a single vector it makes more sense to extend our jacobian by stacking these vectors as rows of a matrix like this so now that we have the structure and motivation for building a jacobian matrix for vector-valued functions let's apply this to our example functions and see what we get so we have u of x y equals x minus 2 y and we've also got v of x y equals 3 y minus 2 x we can build the jacobian from these two by simply saying well the jacobian is going to be let's open some big brackets and say well it's d u d x d u d y d v d x d v d y so let's just work through it so d u d x is just going to be one d u d y is just minus two d v d x is -2 again and dvdy is 3. our jacobian matrix no longer even contains any variables which is what we should expect when we consider that clearly both u and v are linear functions of x and y so the gradient must be constant everywhere also this matrix is just the linear transformation from x y space to u v space so if we were to apply the x y vector 2 3 we'd get the following 2 3 and that's going to equal 2 minus 6 that's minus 4 and minus 4 plus 9 that's 5. now this is all well and good but of course many of the functions that you'll be confronted with will be highly non-linear and generally much more complicated than the simple linear example we've just looked at here however often they may still be smooth which means that if we zoom in close enough we can consider each little region of space to be approximately linear and therefore by adding up all the contributions from the jacobian determinants at each point in space we can still calculate the change in the size of a region after transformation a classic example of this occurs when transforming between cartesian and polar coordinate systems so if we have a vector expressed in terms of a radius r and the angle up from the x-axis theta but we'd like them expressed in terms of x and y instead we can write the following expressions just by thinking about trigonometry now we can build the jacobian matrix and take its determinant the fact that the result is simply the radius r and not the function theta tells us that as we move along r away from the origin small regions of space will scale as a function of r which i hope will make a lot of sense to you when we look at our little animation here that's all for this video i hope you will now be able to build jacobian vectors and matrices for yourself with confidence in the exercises and more importantly that your intuition on the meaning of this concept is starting to develop see you next time we've now seen that the jacobian describes the gradient of a multi-variable system and that if you calculate it for a scalar valued multivariable function you get a row vector pointing up the direction of greater slope with a length proportional to the local steepness in this video i'm going to briefly introduce a kind of gradient playground to help you further develop your intuition on the jacobian which will also set you up for the following exercises in everyday language we use the word optimization to describe the process of trying to make something as good as it can be in mathematics optimization basically means the same thing as much of the research is dedicated to finding the input values to functions which correspond to either a maximum or a minimum of a system examples of mathematical optimization in action in the real world include the planning of roots through busy cities the scheduling of production in a factory or strategy for selecting stocks when trading if we go back to the simplest function we saw in the last section and we said that we wanted to find the location of the maximum we could simply solve this system analytically by first building the jacobian and then finding the values of x and y which make it equal to zero however when the function gets a bit more complicated finding the maximum or minimum can get a bit tricky if as in this case we still have an analytical expression then we can at least still find the general expression for the jacobian but now simply setting it to zero is not only much more complicated but it also is not enough as this function has multiple locations with zero gradient if we assume that all of the maxima and minimum of this function can be seen in the region that we're plotting here then just looking at the surface plot of our function makes it very clear where the tallest peak and deepest trough are we refer to all the peaks as maxima but in this case we have a single tallest peak a which we will call the global maximum as well as several local maxima at c and e similarly we refer to all the troughs as minima and we also have a single deepest point at d which we call the global minimum as well as a local minimum at point b all fairly straightforward however there's a very important point here that's perhaps so obvious that you might have missed it imagine standing on the surface with its hills and valleys and we're trying to climb to the top of the highest peak that's no problem we just look around spot the tallest mountain and walk straight towards it but what if we were walking at night this would be much like the scenario where we didn't have a nice analytical expression for our function so we simply aren't able to plot the whole function and look around perhaps each data point is the result of a week-long simulation on a supercomputer or maybe the outcome of an actual real-world experiment these nighttime scenarios very commonly arise in optimization and can be very challenging to solve however if we're lucky we might find that using a torch we can see the jacobian vectors painted on road signs all around us and each one would say peak this way we would have to remember that although the jacobians all point uphill they don't necessarily point to the top of the tallest hill and you could find yourself walking up to one of the local maxima at c or e and even worse when you get there you'll find that all of the road signs are pointing directly at you this nighttime hill walking analogy is often used when discussing the problem of optimization however it does have some misleading features such as the fact that when you're really evaluating a function it's no problem to effectively transport all over the map by teleportation as you can try the function at many different places but there's no need to evaluate everywhere in between and the calculation costs the same essentially no matter how far apart the points are so we're not really walking instead we're going to switch to the analogy of a sand bit with an uneven base in the following exercises you're going to try to find the deepest point of a sandpit by measuring the depth at various points using a long stick this is a very deep sandpit so once you push the stick down to the bottom there's no way to move it around sideways you just have to pull it out and try somewhere else also crucially just like our night walking scenario you will have no idea what the peaks and troughs look like at the bottom of the pit because you can't see the sand is in the way as you work through the exercise i'm hoping that you'll start to pick up on a few of the subtleties of optimization and hopefully leave you with a few new questions as well see you next time and have fun in the sandpit so now that you've all spent some time playing in the sandpit we can introduce an additional concept which relates to multivariate systems called the hessian in many ways the hessian can be thought of as a simple extension of the jacobian vector for the jacobian we collected together all of the first order derivatives of a function into a vector now we're going to collect all of the second order derivatives together into a matrix which for a function of n variables would look like this however this is one of those scenarios where using an abbreviated notation style comes in really handy we saw in the previous module that we can just keep differentiating a function using the same method to find higher and higher order derivatives similarly for a partial derivative if you want to find the second derivative with respect to x1 then x2 it's as simple as just differentiating with respect to x1 assuming all the other variables are constant and then differentiating with respect to x2 again assuming all the other variables are constant as you can see from this general form our hessian matrix will be an n by n square matrix where n is the number of variables in our function f let's now take a look at a quick example it often makes life easier to find the jacobian first and then differentiate its terms again to find the hessian so for our function f of x y and z equals x squared y z so f of x y z equals x squared y z we're going to first build the jacobian for this thing which of course is going to be j equals so we've got differentiate with respect to x we get 2 x y z differentiate with respect to y we're just going to get x squared z and differentiate respect to z x squared y now using this we can then think about differentiating again with respect to each of the variables which will then give us our hessian matrix so h is just going to equal and put a big bracket here so we want to take this thing and differentiate it with respect to x again so we're going to get 2 y z so the next term along we're going to differentiate this thing with respect to y 2 x z and again with respect to z 2 x y okay now we'll take the next row and we'll say we're going to differentiate this thing with respect to x then y and z so you get 2 x z uh we're going to differentiate respect to y so we're going to get nothing we're going to differentiate this thing with respect to z and we get x squared lastly take this term make the last row differentiate respect to x we get 2 x y and then with respect to y we get x squared and with respect to z we get nothing so one thing to notice here is that our hessian matrix is symmetrical across the leading diagonal so actually once i'd worked out the top right region i could just have written these directly in for the bottom left region this will always be true if the function is continuous meaning that it has no sudden step changes we could now simply pass our hessian an xyz coordinate and it will return a matrix of numbers which hopefully tells us something about that point in the space in order to visualize this we're going to have to drop down to two dimensions again consider the simple function f of x and y equals x squared plus y squared calculating the jacobian and the hessian are both fairly straightforward and hopefully you could have visualized how this function would have looked in your head however if you hadn't known what function we were dealing with and calculated the value of the jacobian at the point zero zero you'd have seen that the gradient vector was also zero but how would you know whether this thing was a maximum or a minimum at that point you could of course go and check some other point and see if it was above or below but this isn't very robust instead we can look at the hessian which in this simple case is no longer even a function of x or y its determinant is clearly just two times two minus zero times zero which is four the power of the hessian is firstly that if its determinant is positive we know we are dealing with either a maximum or a minimum secondly we then just look at the first term which is sitting at the top left-hand corner of the hessian if this guy is also positive we know we've got a minimum as in this particular case whereas if it's negative we've got a maximum lastly slightly modifying our function to include a minus sign and recalculating our jacobian and our hessian and our hessian determinant we now see the third interesting case this time our hessian determinant is negative so we know that we're not dealing with a maximum or a minimum but clearly at this point zero zero the gradient is flat so what's going on well if you look at the animation what we've got here is a location with zero gradient but with slopes coming down towards it in one direction but up towards it in the other we call this kind of feature a saddle point and they can also cause a lot of confusion when searching for a peak in the last module of this course you're also going to see another way that the hessian can help us with optimization but for now we've simply got another tool to help you navigate the sandpit see you next time in this module we've discussed how to think about two-dimensional functions as landscapes and we've also seen that we can construct jacobian vectors which tell us both the direction and the magnitude of the gradient at each point in space last video we added one further tool to our toolbox which allowed us to double check what kind of feature we were standing on when we landed on a point with a zero gradient these concepts will all be very useful to develop your understanding of optimization problems and have also let you see why multivariate calculus is worth knowing however in this video we are going to remind ourselves about two features of real systems which so far we've avoided firstly for many applications of optimization such as in the training of neural networks you are going to be dealing with a lot more than two dimensions potentially hundreds or thousands of dimensions this means that we can no longer draw our nice surface and climb its mountains all the same math still applies but we now have to use our 2d intuition to guide us and enable us to trust the maths secondly as we mentioned briefly before even if you do just have a 2d problem very often you might not have a nice analytical function to describe it and calculating each point could be very expensive so even though in principle a plot could possibly be drawn you wouldn't be able to afford either the supercomputer time or perhaps the laboratory staff to fully populate this thing thirdly all the lovely functions that we've dealt with so far were smooth and well behaved however what if our function contains a sharp feature like a discontinuity this would certainly make navigating the sandpit a bit more confusing lastly there are a variety of factors that may result in a function being noisy which as i'm sure you can imagine might make our jacobian vectors pretty useless unless we were careful so this brings us nicely to the second topic in this video which is a question that i hope you've all been screaming at your screens for the past few minutes if as i said a minute ago we don't even have the function that we're trying to optimize how on earth are we supposed to build a jacobian out of the partial derivatives this is an excellent question and leads us to another massive area of research called numerical methods there are many problems which either don't have a nice explicit formula for the answer or do have a formula but solving it directly would take until the end of time to fight back against the universe mocking us in this way we have developed a range of techniques that allow us to generate approximate solutions one particular approach that is relevant to our discussion of the jacobian actually takes us right back to the first few lectures on this course where we defined the derivative we started by using an approximation based on the rise of a run calculated over a finite interval and then looked at what happens as this interval approached zero all we're doing with the finite difference method is accepting that we're not going to work out the value of the function at every single point in space so we're just going to use the points that we do have and build an approximation for the gradient based on that in the example shown here we have already calculated lots of points on this one-dimensional function but clearly that's not going to be practical for higher dimensional scenarios so all we do is take this logic one step further and say if we start from an initial location and we would like to approximate the jacobian we will simply approximate each partial derivative in turn so taking a small step in x allows us to calculate an approximate partial derivative in x and a small step in y gives our approximate partial in y two things to bear in mind here firstly how big should our little step be well this has to be a balance as if it's too big you will make a bad approximation for reasons that i hope will be obvious by this point but if it's too small then we might run into some numerical issues just remember when your computer calculates the value of the function at a point it only stores it to a certain number of significant figures so if your point is too close your computer might not register any change at all second as we mentioned earlier what happens if your data is a bit noisy to deal with this case many different approaches have been developed but perhaps the simplest is just to calculate the gradient using a few different step sizes and take some kind of average this brings us to the end of this video so i hope you can see that once we leave the world of nice smooth functions and enter the real world of noisy data and computationally expensive functions things start to get a lot more interesting see you next time that's it for module 2 of the course i hope you're now feeling reasonably confident about multivariate calculus and able to navigate multi-dimensional hypersand pits of your own in future see you in the next module where we will be discussing the multivariate chain rule and how it can help you optimize the weightings in a neural network see you then welcome to module 3 of the course in the previous two modules we've seen how to differentiate functions in single and multi-variable spaces as well as hopefully developing some intuition about what's really going on when we do this we've also learned a selection of useful tools for both rapidly calculating derivatives and for navigating high dimensional spaces in this module we're going to upgrade the univariate chain rule so that it can tackle multivariate functions and then see an example of where this might come in handy see you then in the last module we saw something called the total derivative which showed us that when we have a multi-variable function such as f of x y and z but the variables x y and z were themselves each a function of some additional variable t then if we want to calculate the derivative of f with respect to t we can use this expression which is simply the sum of the chains relating f to t through each of its three variables this allowed us to calculate the result in a piecewise manner rather than substituting everything in at the start and computers are really good at solving piecewise problems quickly what we're now going to do is generalize this concept and also simplify the notation a little if we had a function f of n variables x1 x2 x3 all the way up to xn i can write this as just f of x but now you'll notice that i've written the x in bold just to help you remember that this x represents a sequence of variables which we'll now more conveniently think of as an n dimensional vector once again each of the components of our x vector are themselves functions of some other variable t and what we'd like to know is the derivative of f with respect to t at this point we're going to need some more space to write so we'll keep our function f at the top but you'll just have to remember that each component of x is also a function of now we are looking to once again build an expression linking f to t through the sum of the chains of each of its variables so we're going to need all the partial derivatives of f with respect to x as well as the derivatives of each component of x with respect to t once again we're going to store all of these objects in a pair of n-dimensional vectors finally we're looking to build a multi-variable chain rule expression so we are looking to find the sum of the product of each pair of terms in the same position in each vector thinking back to our linear algebra this is exactly what the dot product does but there is no need for us to write out these vectors in full so we can simply write the dot of our two multi-variable derivative expressions and that's it we now have our generalized form of the multi-variable chain rule expressed nice and neatly so we can now update our list of tools to reflect this but actually although we've seen this in practice already in the last module it's worth pointing out that all the rest of our time-saving rules already work for multivariate problems as they are so we'll be ready to put these into practice very soon see you then last video we finished up with an expression for the multivariate chain rule in this video we're going to start by picking up one more little detail which you might have already spotted and then follow this up by adding another link to our chain if we have a function f of x where x is a vector in which each term in x depends on t which we can compactly write like this we also have our compact form of the multivariate chain rule to go with it what i hope you might have noticed last time is that our vector of partial derivatives df by dx is just the same as the jacobian vector which we saw last module except that we wrote it as a column instead of a row vector so from our knowledge of linear algebra we can say that df by dx must be the transpose of the jacobian of f the last thing to realize is that taking the dot product of two column vectors is the same operation as multiplying a row vector by a column vector so finally we can see that our old friend the jacobian offers us perhaps the most convenient representation of the multivariate chain rule next we're going to see that the chain rule still works for more than two links to start us off we'll work through a really quick univariate example where we're going to add in another function separating f from t so we can say f of x is going to equal 5 x and we can have x of u now can equal 1 minus u and u of t finally can be t squared so we've got three functions and we're separating f from t now by an extra step of course we can just sub in each step into each other and find an expression for f as a function directly of t or we say well it's going to be 5 of whatever x is and x is 1 minus whatever u is and u is t squared so this thing goes to five minus five t squared and of course we can now directly differentiate this thing and say d f by d t equals minus 10 t or we can apply a two-step chain rule and say the following so we can have d f by d t is going to equal d f by d x times d x by d u and finally d u by d t okay now subbing in for each of our terms we just get well this thing's going to equal d f by d t is so df by dx is just five we're going to multiply that by dx by du which is just minus one and finally du by dt is just going to be two t so once again we're going to recover the same answer which is minus 10 t so we can see that this approach works for chains of univariate functions and we could extend it out to as many intermediary functions between f and t as we'd like but what about the multivariate case well the chain rule does work here too but we do just have to pay attention to a few extra details let's start by considering the function f of x of u of t again where the function f takes the vector x as an input but this time x is a vector valued function which also takes a vector u as its input as the last video the bold symbols indicate vectors so u is again a vector valued function and it takes the scalar t as its input ultimately we are still relating the scalar input t to a scalar output f but we're doing this via two intermediary vector valued functions x and u if once again we'd like to know the derivative of f with respect to t we can essentially write the same expression as the univariate case except that now several of our terms are in bold we've already seen that differentiating the scalar valued function f with respect to its input vector x gives us the jacobian row vector we've also seen that differentiating the vector-valued function u with respect to the scalar variable t gives us a column vector of derivatives but what about the middle term dx by du well for the function x we need to find the derivative of each of the two output variables with respect to each of the two input variables so we end up with four terms in total which as we saw in the last module can be conveniently arranged as a matrix we still refer to this object as a jacobian so we can now say that the derivative of f with respect to t is the product of the jacobian of f with the jacobian of x and the derivative vector of u notice that the dimensions of these vectors and matrices as shown here are such that this operation is possible and that they will return a scalar just as we expected that's it for this video i hope you're now starting to see the various threads of linear algebra and multivariate calculus weave together see you in the next one in this video i'm going to introduce to you the concept of artificial neural networks in just enough detail that we will be ready to see how the multivariate chain rule is crucial for bringing them to life i think it's safe to assume that everyone watching this video will at least have heard of neural networks and you're probably also aware that they've turned out to be an extremely powerful tool when applied to a wide variety of important real world problems including image recognition and language translation but how do they work you will often see nice diagrams like this one where the circles are our neurons and the lines are the network of connections between them this may sound a long way removed from the topics we've covered so far but fundamentally a neural network is just a mathematical function which takes a variable in and gives you another variable back where both of these variables could be vectors let's now have a look at the simplest possible case so that we can translate these diagrams into some formulae here we have a network which takes in a single scalar variable which we'll call a zero and returns another scalar a1 we can write this function down as follows a1 equals sigma of w times a0 plus b where b and w are just numbers but sigma is itself a function it's useful at this point to give each of these terms a name as it'll help you keep track of what's going on when things get a bit more complicated so our a terms are called activities w is a weight b is a bias and sigma is what we call an activation function now you might be thinking how come all the terms use a sensible letter except for sigma but the answer to this comes from the fact that it is sigma that gives neural networks their association to the brain neurons in the brain receive information from their neighbors through chemical and electrical stimulation and when the sum of all these stimulations goes beyond a certain threshold amount the neuron is suddenly activated and starts stimulating its neighbors in turn an example of a function which has this thresholding property is the hyperbolic tangent function tanch which is a nice well-behaved function with a range from -1 to 1. you may not have met tanch before but it's just the ratio of some exponential terms and nothing your calculus tools can't already handle tanch actually belongs to a family of similar functions all with this characteristic s shape called sigmoids hence why we use sigma for this term so here we are with our non-linear function that we can evaluate on a calculator and also now know what all the terms are called at the start of this video i mentioned that neural networks could for example be used for image recognition but so far our network with its two scalar parameters w and b doesn't look like it could do anything particularly interesting so what do we need to add well the short answer is more neurons so now we're just going to start building up some more complexity whilst keeping track of how the notation adapts to cope if we now add an additional neuron to our input layer we can still call this scalar output variable a1 but we will need to be able to tell the difference between the two inputs so we can call them a00 and a01 to include this new input in our equation we simply say that a1 equals sigma of the sum of these two inputs each multiplied by their own weighting plus the bias as you can see each link in our network is associated with a weight so we can even add these to our diagram adding a third node to our input layer a03 follows the same logic and we just add this weighting value to our sum however things are starting to get a bit messy so let's now generalize our expression to take n inputs for which we can just use the summation notation or even better notice that each input has a weight so we can make a vector of weights and a vector of inputs and then just take their dot product to achieve the same effect we can now have as many inputs as we want in our input vector so let's now apply the same logic to the outputs adding a second output neuron we'd call these two values a10 and a11 where we now have twice the number of connectors each one with its own weighting and each neuron has its own bias so we can write a pair of equations to describe this scenario with one for each of the outputs where each equation contains the same values of a0 but each has a different bias and vector of weights unsurprisingly we can again crunch these two equations down to a more compact vector form where the two outputs are each rows of a column vector meaning that we now hold our two weight vectors in a weight matrix and our two biases in a bias vector now let's have a look at what our compact equation contains in all its glory for what we call a single layer neural network with m outputs and n inputs we can fully describe the function it represents with this equation and peering inside we can see all the weights and biases at work the last piece of the puzzle is that as we saw at the very beginning neural networks often have one or several layers of neurons between the inputs and the outputs we refer to these as hidden layers and they behave in exactly the same way as we've seen so far except their outputs are now the inputs of the next layer and with that we have all the linear algebra in place for us to calculate the outputs of a simple feed-forward neural network however persuading your network to do something interesting such as image recognition then becomes a matter of teaching it all the right weights and biases which is what we're going to be looking at in the next video as we'll bring the multivariate chain rule into play see you then in the previous video we introduced the concept of neural networks and then we worked through the algebra required to describe a fully connected feed-forward network with hidden layers in this video we're going to see how the multivariate chain rule will enable us to iteratively update the values of all the weights and biases such that the network learns to classify input data based on a set of training examples when we say that we are training a network we typically mean using some kind of labeled data which are pairs of matching inputs and outputs for example if we were to build a network to recognize pictures of faces and predict if they were happy then for our training data each of the inputs might be the intensity of a single pixel from the image and this would be paired with an output which just says whether this image contains a face and whether it was a happy face the classic training method is called back propagation because it looks first at the output neurons and then works back through the network if we start by choosing a simple structure such as the one shown here with four input units three units in the hidden layer and two units in the output layer what we're trying to do is find the 18 weights and five biases that cause our network to best match the training inputs to their labels initially we will set all of our weights and biases to be a random number and so initially when we pass some data into our network what we get out will be meaningless however we can then define a cost function which is simply the sum of the squares of the differences between the desired output y and the output that our untrained network currently gives us if we were to focus on the relationship between one specific weight and the resulting cost function it might look something like this where if it's either too large or too small the cost is high but at one specific value the cost is at a minimum now based on our understanding of calculus if we were somehow able to work out the gradient of c with respect to the variable w at some initial point w0 then we can simply head in the opposite direction for example at the point shown on the graph the gradient is positive and therefore increasing w would also increase the cost so we should make w smaller to improve our network however at this point it's worth noting that our cost function may look something more like this wiggly curve here which has several local minima and is more complicated to navigate furthermore this plot is just considering one of our weights in isolation but what we'd really like to do is find the minimum of the multi-dimensional hyper surface much like the 2d examples we saw in the previous module so also like the previous module if we want to head downhill we will need to build the jacobian by gathering together the partial derivatives of the cost function with respect to all of the relevant variables so now that we know what we're after we just need to look again at our simple two node network and at this point we could immediately write down a chain rule expression for the partial derivative of the cost with respect to either our weight or our bias and i've highlighted the a1 term which links these two derivatives however it's often convenient to make use of an additional term which we'll call z1 that will hold our weighted activation plus bias terms this will allow us to think about differentiating the particular sigmoid function that we happen to choose separately so we must therefore include an additional link in our derivative chain we now have the two chain rule expressions we'd require to navigate the two-dimensional wb space in order to minimize the cost of this simple network for a set of training examples clearly things are going to get a little more complicated when we add more neurons but fundamentally we're still just applying the chain rule to link each of our weights and biases back to its effect on the cost ultimately allowing us to train our network in the following exercises we're going to work through how to extend what we saw for the simple case to multi-layer networks but i hope you've enjoyed already seeing calculus in action see you next time this brings us to the end of the third module on this calculus course hopefully you are now starting to see how both calculus and linear algebra are going to be important for developing a detailed understanding of machine learning techniques you already have a lot of powerful tools in your toolbox and you are nearly ready to put them into action next module we'll be looking at power series which will bring us all the way back to the rise over run approximation from module one and help us tie up a few loose ends see you then welcome to module 4 of the course in this module we are going to learn how to take a complicated function and build an approximation to it using a series of simpler functions the particular approach that we're going to be looking at is known as the taylor series and it will require all of the calculus skills you've learned so far to be put into practice both the jacobian and the hessian will be making appearances again when we consider the multivariate case which will hopefully allow you to see how all of the concepts that you've seen so far are actually linked together see you then the taylor series is one of a family of approaches used for building approximations to functions so before diving into the maths it's worth stopping for a minute to talk about when it might be useful to have an approximation one example that seems to stick in people's minds is to do with cooking a chicken imagine that you could write a function which describes the relationship between the mass of a chicken m and the amount of time t it should be put in the oven before it will be perfectly cooked clearly there are a lot of assumptions that we'd need to have in place for such a function to even exist for example this would only apply to a certain type of oven preheated to a certain temperature applied to a certain configuration of chicken i.e one that's in one piece rather than chopped up furthermore the heat transfer properties of a chicken will almost certainly vary in a highly non-linear fashion as a function of the mass let's have a look at the sample function that has some of the features that we've mentioned you can imagine that on the one hand the chicken cooking time tea will be quite sensitive to a lot of parameters that we might expect to vary case to case however on the other hand if we'd like to sell a nice recipe book we're going to need to be pragmatic and simplify this monster as people tend not to want to solve complicated equations while making dinner so what do we do firstly we're going to make two assumptions one about ovens and the other about chickens so let's assume that it's reasonable to suggest that everyone who buys your cookbook will have a sufficiently similar oven such that we can assume that they behave basically the same secondly that everyone will be cooking a sufficiently similar chicken in terms of their heat transfer properties as a function of mass that you can also assume that these will be the same in each case this allows us to remove two potentially problematic terms that might themselves have been a function of many other variables but we were still left with quite a messy function the next thing we're going to do is have a look at a plot of this function now already i'm telling you about what i view as relevant information in this system as i'm not showing you timings for chickens heavier than four kilograms or chickens of negative mass and in fact what i'm going to do now is go one step further and say that considering a typical chicken from a supermarket is about 1.5 kilograms so let's focus here and build a nice straight line approximation of the form y equals mx plus c by using the taylor series approach which we will be learning in the following videos it is possible for me to derive a function with the same height and slope as one of the points in my graph this line is a fairly reasonable approximation to the function in the region close to the point of interest but as you move further away the approximation becomes pretty poor however this cookbook is not for people either roasting giant or miniature chickens so we end up being able to write down an expression in a much more palatable format where our approximation t star is approximately equal to 50 times the mass in kilograms plus 15. so if you'd like to roast a 2 kilogram bird leave it for about 115 minutes in the next few videos we're going to go into a lot more detail about the taylor series and also find out how to derive higher order terms i really don't know whether the famous chefs around the world are knowingly using taylor series approximations when they write their cookbooks but i like to think they are see you next time in this short video i'm just going to give you one more teaser about what the taylor series is doing before we try and write down anything like a formal definition this will allow you to have a go at some graphical questions first which is much like how we approach london differentiation at the start of this course taylor series are also referred to as power series and this is because they are composed of coefficients in front of increasing powers of x so we can write a simple generalized expression for a power series as g of x equals a plus bx plus cx squared plus dx cubed etc potentially going off for infinitely many terms depending on what function we're considering when we calculate a taylor series in the next video we will build it up coefficient by coefficient where each term that we add improves the approximation and in many cases we will then be able to see a pattern emerging the coefficients which thankfully saves us the trouble of calculating infinitely many terms however many of the applications of taylor series involve making use of just the first few terms of the series in the hope that this will be a good enough approximation for a certain application starting from just a single term we call these expressions the zeroth first second and third order approximations etc collectively these short sections of the series are called truncated series so let's begin by looking at some arbitrary but fairly complicated function if we start with something like this where we have a function and we're just making it up perhaps shaped like this all we're going to do is focus on one particular point on this curve so let's say this point here and then we're going to start building our function by trying to make it more and more like the point that we've chosen so as the first term of our generalized power series is just a number a and we're ignoring all the other terms for now we know that our opening approximation must just be a number that goes through the same point so we can just dive straight in and add our zeroth order approximation function to our plot it's going to be something like that clearly this hasn't done a great job at approximating the red curve so let's now go to our first order approximation this thing can also have a gradient and if we'd like to match our function at this point it should have the same gradient so we can have something a bit more like this which is supposed to be a straight line clearly our approximation has improved a little in the region around our point although there is still plenty of room for improvement we can of course move on to the second order function which as we can see is a parabola although at this point things get a little tough to draw and matching second derivatives by i is also not easy but it might look something like this so we can say okay let's go up and we're going to come down like that hopefully without having gone into any of the details about the maths you'll now be able to match up some mystery functions to their corresponding truncated taylor series approximations in the following exercise in the next video we're going to work through the detailed derivation of the terms but i hope this activity will help you not to lose sight of what we're trying to achieve in the end see you then in this session we're going to work through the derivation of power series representation of functions which in short is the idea that you can use a series of increasing powers of x to re-express functions just to warm up by blowing your minds we can take the function e to the x which we met earlier in the course and re-express it as the series 1 plus x plus x squared over 2 plus x cubed over 6 plus higher order terms now i hope you would agree with me that this is pretty incredible and hopefully by the end of this video you will understand how to build series like this for many other interesting functions what the taylor series method tells us is that if we know everything about the function at some point whereby everything i mean the function's value its first derivative second derivative third derivative etc then we can use this information to reconstruct the function everywhere else so if i know everything about it at one place i also know everything about it everywhere however this is only true for a certain type of function that we call well-behaved which means functions that are continuous and that you can differentiate as many times as you want that said we know lots of functions like that so it turns out to be a very useful tool indeed so now we're going to try and explain this concept graphically by looking at a graph of some arbitrary function f of x which looks like this and building a sequence of gradually improving approximations so our first approximation which we can call g zero we're going to build using just one piece of information from our function f of x which will be the value of the function at our point of interest which in this case is just x equals zero as you can imagine if all we're going to build our function off is this one single value then clearly our guess function isn't going to be able to capture a very complicated shape like this and in fact all it can be is a horizontal line at the point f at zero so we can plot this first approximation and also write an expression for calculating g zero of x which is just f of zero we call this our zeroth order approximation and clearly it's not very good but also notice that as this line is flat it's actually not even a function of x we can do better so let's now find our first order approximation for this we're now going to use two pieces of information the value of the function at x equals zero but also the value of the gradient at x equals zero which we will call f dash at zero using these we can build another straight line of the form y equals mx plus c where the vertical axis intercept c is just f of zero but just substituting in for f dash of 0 as the gradient of our line so we can now plot our first order approximation to the function which has the same value of the gradient and of the function f of x and we can also write down its expression g one of x is f of zero f dash of zero times x this thing clearly does a better job than g zero at approximating f of x near the point x equals zero but it's still not great moving quickly on to our second order approximation g two of x we're going to use three pieces of information f of zero f dash of zero and the second derivative f double dash of zero now to have a function that can make use of these three pieces of information we're going to need a quadratic equation so we can write y equals a x squared plus b x plus c differentiating this thing twice we can just say well y prime equals 2 a x plus b and y double prime equals to a now what we want is for this function to be the same as f of x when we sub it in at the point equals zero x equals zero so we can say okay at x equals zero we want this thing to equal f double prime of zero so clearly our coefficient a because this thing's not even a function of x therefore a is just going to equal f double dash 0 divided by 2. now if we look up at this equation here we also want the first derivative to be equal to the function at zero so we can set this equal to f dash of zero and we're subbing in zero for this x here so this term just disappears so we can now say that also b equals f dash at 0. lastly subbing in 0 here and here and setting this thing just equal to f of 0 we can say that clearly c is also just equal to f of zero c equals f at zero so we now have all three coefficients for our equation and we can say let's now go back to our graph and add our second order approximation which as it's just an x squared term will be a parabola notice that each time we update our approximation the region in which it matches up with f of x grows a little so let's now take one more step down the rabbit hole and find the third order approximation so we can write y equals a x cubed plus b x squared plus c x plus d as based on what we've already seen from the first three steps it's only the coefficient a that we need to find as b c and d will be the same as we found for g2 so let's now differentiate this thing three times y prime equals three a x squared plus two b x plus c y double prime equals six a x plus 2 b and y 3 is just going to be 6 a so clearly we want this thing the third derivative of our approximation function to equal our function f of x when we differentiate it three times and evaluate it at the point x equals zero so we're setting this x to zero now there is no x in this bit we can say f 3 at 0 equals 6 a therefore a just equals f 3 at 0 divided by 6. finally we can add this third order approximation to our graph and write out its expression you can now see that not only has the approximation improved significantly but also that we can add higher order terms piecewise and in each case the lower order terms remain the same what we now want to build is a general expression the lower order terms remain just right down the fourth order approximation without working through it notice that the one over sixth coefficient in front of the cubic term was the result of having to differentiate a cubic term twice so when we differentiate x to the fourth power three times we're going to get four times three times 2 in front leading to a coefficient of 1 divided by 4 times 3 times 2 which is 1 divided by 24. we have a name for the operation 4 times 3 times 2 which is 4 factorial in fact all the terms can be thought of as having a factorial in front of them even 0 factorial which for reasons i won't go into here is in fact equal to 1. so with this last piece of the puzzle in place we can now say that the nth term in the approximation is just the nth derivative of f evaluated at zero divided by n factorial multiplied by x to the power of n and therefore the complete power series can be written as follows so you can see it's the sum from n equals 0 to infinity of these terms although what we've written here certainly does count as a taylor series because we're specifically looking at the point x equals zero we often refer to this case as a maclaurin series so you're still going to have to wait one more video before seeing the taylor series expression in all its glory in the rest of this module we're going to be applying the concept of power series to some interesting cases as well as generalizing it to higher dimensions where rather than building approximation curves we will be constructing approximation hypersurfaces see you then i began our last session by trying to motivate you with the fact that we can re-express the function e to the x as a power series so i want to make sure that your mind is well and truly blown by showing you that when we differentiate this function term by term which isn't very difficult to do as it's just a polynomial something rather satisfying happens where just as we'd expect for the derivative of e to the x this infinitely long series remains unchanged which i think is pretty awesome so now finally you're going to get to see the taylor series fundamentally we are applying the same logic as the maclaurin series that we derived previously but whereas the maclaurin series says that if you know everything about a function at the point x equals zero then you can reconstruct everything about it everywhere the taylor series simply acknowledges that there is nothing special about the point x equals zero and so says that if you know everything about the function at any point then you can reconstruct the function anywhere so a small change but an important one let's look again at the function e to the x as well as the first four approximation functions now using our understanding of the maclaurin series we can also rewrite this in a compact summation notation such that we could if we wanted build a polynomial approximation function to an arbitrary degree of accuracy so what if now instead of expanding around the point x equals zero we wanted to start from the point x equals p well to begin with let's have a look at the first four approximation functions at this point the expressions to describe these approximations are still going to require the value of the function as well as all of its derivatives at this point but how do we now adjust our general equation to allow for an arbitrary expansion point x equals p well clearly the zeroth order term this is just straightforward it's going to be a horizontal line that just uses the point f of p everywhere but what if we take a closer look at the first order approximation this should give us enough insight that we'll then be able to tackle all the other higher order terms so we're essentially looking to build a tangent to the curve at point p all straight lines are of the form y equals mx plus c so let's now work through this by putting in place all the information that's available to us so we can say okay here is some nice function that we're going to deal with and if we're looking at the point p here this is of course at f of p height so it's coordinates of this point our p f p and we want to build an approximation function that is first order so it's got a it's got a y-axis intercept and it's got a gradient and it's going to look something like this okay the same gradient the same height of the function at that point so of course that green line is going to have the equation y equals m x plus c now we know immediately the m is the gradient of this straight line and we know the gradient of our function here is just f dash at p so we can say y equals f dash at p x plus c so all that's left is for us to find c and to do that we're going to need to know x and y but we do know a point on our line it's the point p f of p so we just sub these in f of p equals f dash of p at p plus c rearranging for c we can just see that c is going to equal f of p minus f dash of p at p subbing the c back into our general equation we can now bring it round and say well this line has the equation y equals m which is just f dash of p times x plus c which is just f of p minus f dash of p multiplied by p okay now the last thing i'm going to do is take this and factorize this f dash of p out and that'll leave you with a nice form of this expression y equals f dash of p multiplied by there's an x of it here and there's a minus one of it here and minus p of it here so we can say x minus p plus just this term here f of p what this shows us is that by building our approximation around the point p when we use our gradient term f dash of p rather than applying it directly to x we instead now apply it to x minus p which you can think of as how far are you away from p so we can now write down our first two approximation functions to f x at the point p the zeroth and the first thinking back to the maclaurin series from the previous video all we need to do to convert to taylor series is use the second derivative at p rather than zero and also replace x with x minus p but notice that the factor of one half still remains so we can now look at our concise summation notation for the maclaurin series and upgrade the whole thing to the more general taylor series form noticing of course that if we decide to set p to zero then the expressions would actually become identical so we now have our one-dimensional taylor series expression in all its glory which will allow us to conveniently re-express functions into a polynomial series in the remainder of this module we're going to play around with this result as well as extending it to higher dimensions see you then in this video we're going to put our understanding of power series to the test by looking at two interesting examples first off we're going to build a maclaurin series expansion of the cosine function cosine is the epitome of a well-behaved function as it is certainly continuous everywhere as well as being infinitely differentiable as we're building a maclaurin series we're going to want to find out everything about the function at the point x equals zero so let's start by doing some differentiation and what we get if you remember from earlier in the course is this cyclic pattern of cosines and sines positive and negative which takes us back to the cosine again after four steps if we now evaluate this derivative at the point x equals zero we see that the cosine terms are either one or minus one and the sine terms are all zero this must mean from a power series perspective that every other term will have a zero coefficient notice that these zeros will occur whenever we differentiate an odd number of times which means that all the odd powers of x like x to the 1 x cubed x to the 5 etc will all be absent from the series the even powers of x are all what we call even functions which means that they are all symmetrical around the vertical axis just like cosine is so we can now bring back our general expression for the maclaurin series and just start writing out the terms the icing on the cake is that at this point we just notice that we can build a neat summation notation which fully describes this series without having to write out all the terms notice that this expression doesn't even contain any reference to cosine as all that we need to know is captured by the minus 1 to the power of n which just keeps flipping from negative to positive and negative again now that we've done all the hard work we can simply ask our computer to plot this sequence of increasingly accurate maclaurin series which hopefully starting from a horizontal line at y equals 1 will line up with your expectations notice that outside of a region of fairly close to the point x equals zero the approximation explodes off and becomes useless by the time we get our 16th order approximation we've pretty much nailed the region shown in our graph here although just outside of these axes the function would also be growing hugely positive so you must always be careful when handling series approximations that you know the domain in which it's acceptable in the second example we're going to take a look at the function f of x equals 1 over x which of course looks like this it's a nice simple function but notice the discontinuity at x equals zero this is absolutely not a well-behaved function in fact it's so badly behaved that when we even try and build the zeroth order approximation we immediately run into problems because we have to perform the operation one divided by zero which is not defined and if you try to ask your computer to do this it may give you back the answer nan which stands for not a number so we're going to need to try a different angle of attack clearly we aren't going to have much luck at the point x equals zero so why not try going somewhere else anywhere else let's look at the point x equals one certainly it passes the first test of being able to evaluate the function at this point however moving away from x equals 0 means that we're now going to need to use the taylor series instead of the maclaurin series so we now need to find a few derivatives of the function and see if we can spot a pattern when we evaluate these functions at the point x equals one hopefully you'll recognize that we get a sequence of factorials emerging just as we did when deriving the power series formula in the first place so if we now substitute these values into our taylor series formula the factorial terms will cancel and all we are left with is a sum of x minus 1 to the power of n terms with alternating signs which we can simplify to this neat summation notation so once again let's now pass this formula to the computer and ask it to plot a sequence of improving approximations starting as ever with a horizontal line but this time starting at the height of the function at x equals one there are several really interesting features of this particular example which tell you interesting things about the power series more generally firstly the approximations ignore the asymptote going straight across it and furthermore the region of the function where x is less than zero is not described at all by the approximations secondly although the function is gradually improving for larger values of x you can see the tail wildly flailing around as the sign of each additional term flips from positive to negative and back again i hope these two examples have made it clear both how the taylor series manages to reconstruct well-behaved functions like cosine x but also why they can struggle to deal with something badly behaved like one over x as we bring this video to a close let's now watch an action replay of these two sequences of improving approximations in the next video we're going to talk briefly about what it means to linearize a function and see how this relates to the taylor series analysis that we've seen so far see you then in this session we're just going to take what we've learned about taylor series so far and reframe it to a form that will help us understand the expected error in an approximation so as we've already seen we can build up a sequence of gradually improving approximations to functions by adding higher power terms all i'm going to do now is change the notation of some of the terms so if we take our first order approximation as an example what this expression is saying to us is starting from the height f of p as you move away from p your corresponding change in height is equal to your distance away from p times the gradient of your function at p so in a sense rather than using gradient equals rise over run we are just rearranging to rise equals gradient times run you are ultimately going to use your approximation to evaluate the function near p as you must already know about it at p so i'm now going to say that our distance from p which we currently call x minus p we will now call delta p to signify that it represents a small step size away from p and we can also recast x now in terms of p saying that x is just p plus delta p finally it actually is now time to say goodbye to p as in our first order approximation everything is actually in terms of p so all i'm going to do is swap all the p's for x's as this is the form that people would usually expect to see it in i'm going to do it all at once to make it as painless as possible i haven't changed anything conceptually i've just written an x everywhere where we used to have a p and we can now of course also rewrite our taylor series expression in this new form with just x and delta x so we're in good shape to talk about approximations what i now want to know is when i use the first order approximation instead of evaluating the base function how big should i expect the error to be you can see for example that the gap between the white and green lines grows as we move along the x-axis away from the point x well one way to think about it is that we know our function can be exactly represented by this infinitely long series so although we won't be able to evaluate all of the terms we do know that the next term along i.e the first term that we ignore when we're using our first order approximation has a delta x squared in it this means that if we can say that delta x is a small number then delta x squared must be a really small number and similarly delta x cubed must be a ridiculously small number so we can now rewrite our first order approximation to include an error term which we just say is on the order of delta x squared or equally that it is second order accurate this process of taking a function and ignoring the terms above delta x is referred to as linearization and i hope it's now clear to you why that's the case we've taken some potentially very nasty function and just approximated it with a straight line the last idea that i'm going to share with you in this video is perhaps the most interesting as it brings us right back to our rise over run approximation that we met at the beginning of the course the green line is our first order taylor series approximation to the function at the point x which is of course also the tangent to the curve at x but let's now add another line which is the approximation to the gradient at x using our rise of a run method with a second point delta x away we use this forward difference method to help us build the definition of a derivative at the beginning of the course and we noticed that as delta x goes to zero the approximation becomes exact however what happens if delta x does not go to zero and the second point remains some finite distance away from x well your calculated gradient will now contain an error and once again we can rearrange the full taylor series to work out how big we'd expect that error to be with a bit of slightly fiddly algebra we can rearrange this expression such that the gradient term f dash of x is isolated on the left hand side because the series is infinite this is still an exact expression for the gradient to x but what we get on the right hand side is something that looks suspiciously like the rise of a run expression plus a collection of higher order terms if we notice that the first of the higher order terms has a delta x in it we know that we can now lump them all together and say that using the rise of a run method between two points with a finite separation will give us an approximation to the gradient that contains an error proportional to delta x or more simply put the forward difference method is first order accurate that's all for this video it might seem a little odd to go to all that trouble just to get an idea of the error in an approximation but it turns out to be a hugely important concept when as is typical we ask computers to solve numerically rather than analytically see you next time so far in this module i've introduced you to the concept of power series and then shown you how we can use them to approximate functions we've also seen how power series can then be used to give you some indication of the size of the error that results from using these approximations which is very useful when applying numerical methods the last idea that we're going to very briefly look at on this topic is upgrading the power series from the one-dimensional version that we've seen so far to its more general multivariate form just to recap on the notational options from the previous video we saw that we can re-express the taylor series from a form that emphasizes building the approximation of a function at a point p to a totally equivalent form that emphasizes using that function to evaluate other points there are a small distance delta x away just to make sure you can see that they're the same we can write out the first four terms one above the other to make the comparison clear in this video we are going to continue using the delta x notation as it's more compact which will come in handy when we're in higher dimensions we won't be working through the multivariate taylor series derivation in any detail as all i really want you to take away from this video is what a multi-dimensional power series will look like both as an equation and in a graph so keeping in mind our one-dimensional expression let's start by looking at the two-dimensional case where f is now a function of the two variables x and y so our truncated taylor series expressions will enable us to approximate the function at some nearby point x plus delta x y plus delta y let's look at the simple two-dimensional gaussian function which many of you may be familiar with from studying the normal distribution in statistics it's a nice well-behaved function with a simple maximum at the point zero zero now in a one-dimensional analysis our power series would give us an expression for a one-dimensional function however in a two-dimensional case our power series would of course now give us a two dimensional function which we would more intuitively refer to as a surface just as with the one-dimensional case our zeroth order approximation are simply the value of the function at that point applied everywhere which means in 2d this would just be a flat surface so a zeroth order approximation at the peak would look like this and a zeroth order approximation somewhere on the side of the bell would look a bit more like this this is fairly straightforward but now let's think about the first order approximation drawing the analogy from the 1d case again the first order approximation should have a height and a gradient so we're still expecting a straight surface but this time it can be at an angle now if we are calculating it at the peak it wouldn't be a very exciting case as it's a turning point and so the gradient is zero however let's look again at the point on the side of the slope taking the first order approximation here gives us a surface with the same height and gradient at the point finally let's look at the second order approximation for these two points we're now expecting some kind of parabolic surface however if i calculate it at the peak nothing seems to appear but that's just because we've created a parabola inside our bell curve so we need to look from underneath to even see it finally if we plot the second order approximation at the point on the side of the bell curve you can see that we end up with some kind of interesting saddle function but it's still fairly clear even by eye that the gradient and the curvature are matching up nicely at that point although this approximation is evidently not useful outside of a fairly small region around the point we're now going to take a look at how we would write expressions for these functions so if we wanted to build a taylor series expansion of the two-dimensional function f at the point x comma y and then use it to evaluate the function at the point x plus delta x comma y comma delta y our zeroth order approximation is just a flat surface with the same height as the function at our expansion point the first order approximation incorporates the gradient information in the two directions and once again we are thinking about how rise equals the gradient times the run notice here for compactness i'm using the partial symbol with a subscript to signify a derivative with respect to a certain variable if we look now at the second order approximation we can see that the coefficient of one-half still remains as per the one-dimensional case but now we have three terms all of which are second derivatives the first has been differentiated with respect to x twice the last with respect to y twice and the middle term has been differentiated with respect to each of x and y now there are of course higher order terms but we've already got more than enough to play with it so let's look again at the first order term it's the sum of the products of the first derivatives with the step sizes but hopefully this will remind you of our discussion of the jacobian so we can actually re-express this as just the jacobian multiplied by a vector containing delta x and delta y which we can write as delta bold x where the bold x signifies a vector containing those two terms finally if we look at the second order term in the same way and notice that these second derivatives can be collected into a matrix which we've previously defined as our hessian then to make the sum we need we now have to multiply our delta x vector by the hessian and then again by the transpose of the delta x vector and that's it so we now have a nice compact expression for the second order multivariate taylor series expansion which brings together so much of our calculus and linear algebra skills and makes use of the jacobian and hessian concepts which we defined earlier in the course and of course although we've been talking about the two-dimensional case in this video so far we could actually have any number of dimensions contained in our j h and delta x terms so this immediately generalizes from 2d to multi-dimensional hypersurfaces which is pretty cool see you in the next one this brings us to the end of the fourth module of this calculus course you now have all the tools in place to do something really useful and in the next two lectures david is going to walk you through a method for fitting functions to data which will make use of a lot of the calculus including all the four time saving rules in their multivariate form as well as putting the jacobian into action good luck and i'll talk to you again at the end of the course hi there i'm david dye and i'll be taking you through the last few modules of this course in this module we'll start to use the calculus we've done and put it together with vectors in order to start solving equations in this first video we'll look at a nice simple case where we just need to find the gradient uh the derivative in order to solve an equation using what's called the newton-raphson method now say we've got that distribution of heights again uh with a a mean an average a mu and a width sigma and we want to fit an equation to that distribution that um so we don't have to after we fitted it bother about carrying around all the data points we just have a model with two parameters a mean and a width and we can do everything using just the model and that would be loads faster and simpler and would let us make predictions and so on so it'd be much much nicer but how do we find the right parameters for the model how do we find the best mu and sigma we can well what we're going to do is we're going to find some expression for how well the model fits the data and then look at how that goodness of fit varies as the fitting parameters mu and sigma vary so we're trying to solve an equation where the fitting perhaps the variables in the equation but in order to get there in the next module actually we're first going to need to do a bit more calculus so first let's look at the equation of a line say here y equals x cubed minus 2x plus 2. if we differentiate this equation uh we get the quadratic 3x squared minus 2 and that quadratic will have two solutions and therefore two turning points will exist one a maximum what a minimum just as we see here now say that i don't know what the equation looks like i'm blind and i haven't got enough computer resources to graph out the values at every point or more likely in reality say the function exists in so many dimensions that i can't visualize it at all but say i only need to find the solution to the equation where y equals norm so where x cubed minus 2x plus 2 is equal to 0. we can see there's actually only one solution here on this graph but there could be more depending on the exact form of the equation i was trying to solve now say that i have an idea that i want to hunt for a solution somewhere near some initial guess at the red dot here for instance the constant's pretty small and positive so i might guess that for i need a slightly negative value of x say minus 2 might be somewhere near a solution now if i can evaluate the value of the function at my guess of x equals minus two i find that the function has a value of minus two and if i ask what the gradient is at that value of x equals minus two i find that the gradient is positive and it's ten now i can extrapolate the gradient to the intercept with the y-axis which is uh would be my first guess of the solution of the equation which i'm trying to solve where it finds that intercept with the y-axis so i can use that value of x at the intercept as a new estimate for what the solution to the equation is effectively i'm guessing that the function is a straight line and then i'm using the gradient to extrapolate to find the solution it isn't really a straight line of course but the first order approximation would be that it's a straight line and we'll use that to update our guess and then go again and evaluate so i can write down an equation for my new guess x i plus one based on my previous guess i'd attempt x i as being my original guess minus the value of the function divided by its gradient so let's see how it plays out we can make a table starting with our initial guess at i equals naught and then we can find the gradient and the intercept and then use that to generate a new guess in this case minus 2 minus minus 2 divided by 10 gives us -2 plus 0.2 which is minus 1.8 then we can evaluate the result for that guess and find that it's just a little less than zero minus 0.23 and the gradient is 7.7 so we've gone from being out by two on y to being out by just 0.23 so in some sense we've got like 10 times better in our estimate just in our first go if we carry on then we get the next guess for x2 is minus 1.77 and that's just 0.005 away from the axis it's really close then if we have another go after just three iterations we get an answer of x equals minus 1.769 which is just 2.3 times 10 to the minus 6 from the axis so in just three iterations we pretty much solve the problem which is pretty cool this method is called the newton-raphson method and it's really pretty neat to solve an equation all we need to be able to do is evaluate it and differentiate it we don't need to graph and visualize it everywhere calculating it lots and lots of times we don't need to be able to solve it algebraically either which if we have lots of dimensions to a data set say and a big multi multi multi-multi-variable function we're trying to fit to that data it's going to be much too expensive to try and solve it analytically or even plot it out for all the possible values of the variables this sort of method where we try a solution and evaluate it and then generate a new guess and then evaluate that and again and again and again it's called iteration and it's a very fundamental computational approach now there are some things that can go wrong sometimes with this method so let's briefly look at those say i started off with a guess of x equals zero which evaluates to y equals two when i find the gradient for that and extrapolate it that takes me away from the solution to the other side of the turning point it gives me a new guess that x equals one when i evaluate that then i get a value for y at x equals one of one when i find the gradient and extrapolate back then my new estimate lands me back at x equals zero which is where i've begun so i have a problem i seem to have magically landed in a closed loop where my estimates just cycle back between x equals naught and x equals one and i never get close i never even go anywhere near to the solution x equals minus 1.769 there's another problem which is that if i'm close to a turning point this bottom here to a minimum or a maxima then because my gradient will be very small when i divide by the gradient in the newton-raphson equation here my next estimate will take me zapping off to some crazy value um and therefore it won't converge to the dive off somewhere those are the problems so that's the newton-raphson method we iterate to a solution to an equation by each time making a new estimate for the solution using the gradient to extrapolate towards the solution then going again and again and again most of the time this works really well as a means to step towards the solution so what we've done in this video is look at a method for using just the gradient to step our way towards solving a problem this method is called the newton-raphson method and it's a really powerful way to solve an equation just by evaluating it and its gradient a few times it's as if you're standing on a hill in the fog and you can know your height and you can know locally what's going on around you you can know how steep the hill is but you can't see the whole landscape around you you don't know what it looks like you don't know how to get down the mountain if you like down to a nice safe place that doesn't have cliffs so what you do is you guess based on how steep the hill is locally around you which way to go so you want to go down to sea level so you go down the hill then you take that step blindfolded and when you get there you ask again what height you're at and how steep it is and then you keep making more steps down the hill until either something goes wrong and you need to go back down the other way or you get home to where you want to want it to be the point is you don't need to know what the landscape looks like the function you just need an altimeter the value of the function and to be able to feel with your toe what the gradient is like locally around you what we'll do in the next video is look at how to apply this where we've got multiple variables not just x and that will involve finding the gradient vector how to go down a hill on a contour plot okay so now we've looked at the newton-raphson method which uses the gradient to iteratively solve a 1d function of x say now we'll generalize that to figure out how to do something similar with a multi-dimensional function a function of multiple variables and how to use the gradient to find the maxima and minima of such a function later on this will let us optimize find the best fit for the parameters of a function that we're trying to fit say i've got a function like f equals x squared y here this function looks like this and what i've got here is a function that gets big and positive when x is big and y is positive and it gets negative when y is negative so if i look down the y-axis i get a projection of a straight line and if i spin to look down the x axis i get an upright parabola for y positive and one going down for y negative and the function is equal to zero along both the axes now at the bottom here in matlab i've used the surfc function to plot contours when the function has a constant value just like the contours of constant height on a topographical map so the question we want to answer in this video is how do i find the fastest or steepest way to get down this graph now previously you found out what the gradient of a function is with respect to each of its axes so you can find the fdx just by differentiating the function f treating all the other variables as constants so in this case we've got a function f of x y is equal to x squared y and so df dx is equal to differentiate that treating y as a constant so that's just 2x times y and uh df dy is equal to well the x squared we treated the constant and y differentiates just to one so it's just x squared now we can write down uh these two gradients in a vector which we call the grad of f so this guy we call grad um and this vector is just the vector uh where we write them down as uh the components of a vector so we say that grad is that it's just the vector found by writing down d f by d x and d f by d y in the x and y locations of the vector and glad is like the most awesome vector ever grad is the little vector that combines linear algebra and calculus together so grads a lot of fun so if we run along a little vector r is equal to dx zero then if we dotted that with grad f then we get df dx times dx plus zero um so we'd move the amount dx along the x-axis so this is doing the right sort of thing right a dot product of grad f with a vector is going to take it looks like some amounts of df dx along the x bit of that vector and some amounts of d f d y along the y bits of that vector so it's going to tell us how much we've gone down that's sort of how grad works and the important thing is to remember this is evaluated at f has some values in the space a b so it's evaluated at a location so then if we want to know how much the function will change when we move along some unit vector in an arbitrary direction so we'll call that unit vector r hat and we'll give it components c and d so c squared plus d squared is equal to 1. so df is going to be df dx times c plus df dy times d so what we're doing here is we're doing grad of f dotted with our hat and we call this thing the directional gradient so another question we can ask is what's the maximum value this directional gradient can take so we've got a grad f as a vector and we're dotting it uh with our hat and our question is what's the maximum value they can take given that r hat is a unit vector well the only thing then left when we dot them together is going to be the cos theta term because it's a b cos theta and that's going to be maximized when cos theta is 1 which means theta's 0 which means r hat is parallel to grad f so in order to find the maximum value of the directional gradient we want an r hat that's actually the normalized version of grad so we can write that down we can do that calculation so we'll have grad f dotted with the normalized version of itself so grad f divided by its modulus but grad f dotted with itself is just equal to the size of grad f squared we've got to divide by the size so the maximum value the directional gradient can take is just the size of grad f and that's therefore the steepest gradient we can possibly have the size of the steepest gradient we can possibly have is just the size of grad f the sum of the squares of the components of grad so that's really fun the other question to ask is which way does grad point this is less easy to see but grad points up the direction steepest descent perpendicular to the contour lines so think if you're on a mountain and it's foggy and you can only see locally around you and you want to go down the hill you don't walk around the contour you go down the hill so if the contour's like this and i'm staring down the hill down the hill is at 90 degrees to the contour line and think about the way it's defined is it up or down um dfdx is positive if you're going up um so actually grad f points up the hill um in the steepest way and minus grad points down the hill the steepest way so grad points up the direction of steepest descent now if we have some data science problem where we want to minimize the difference between our data values and our model fit then what we want to do is find the lowest point in the function the function is kind of the badness so we want to find the best so we want to find the point where the badness is minimized and like newton-raphson we can use the gradient to go from some trial point down towards the solution but in newton-raphson we're trying to find the zero point here we don't know what the minimum value of the function is so we don't know how far we down we need to go as if we're somewhere on our mountain but we don't know the altitude of the valley so what we do in what's called the gradient descent method is we take a series of little steps down the hill that is if we started at some position sn then our our next position sn plus 1 is given by s n plus some little step down the hill and that little step is given by minus some amount times grad and grad evaluated at the previous position sn on the graph that's going to look like taking a little step the little blue one down the hill and then we re-evaluate to make another sn plus one and make another step and take a series of steps down the hill if we overshoot that's okay because grab will just take us back to the minimum and notice that as the gradient gets shallower as we come towards the turning point then the steps automatically get smaller because grad gets smaller so this is quite a nice method there are lots of ways to enhance it but that's the main idea it's very powerful and simple the other thing to notice is that there are multiple local minima in the landscape then we might get stuck in one of them and of course which one we find depends on our starting point we won't find the others we'll only find one at a time so in the general case there are some problems but nevertheless this is quite a neat method this just following little steps down the hill and this is probably the main way in which most numerical optimizers work that people use in the real world so what we've done in this video is we've introduced ourselves to grad the gradient vector a merged calculus and vectors together to take our first steps in vector calculus and that's allowed us to find a method for just using the gradient to step our way towards solving a problem for multi-variable cases and that method is called gradient descent so we found out about grad and that led us on to sketch out a neat method for finding the minima and maxima of multi-variable functions which we call gradient descent in this video we'll look at what happens if we want to find the minimum or maximum subject to some constraint that we want to find the maximum somewhere along a line or something like that this is called the method using lagrange multipliers so what we did in the last video is we looked at a function like this one f here this function f is x times the exponential of minus x squared plus y squared and it's one of the standard matlab examples and i've plotted below it here a contour map of the values if i now plot just the contour map i can plot out the vectors of grad f as we said these vectors are perpendicular to the contour lines and they point up the steepest gradient now let's return to the function we were looking at last time f equals x squared y which is the khan academy standard example for this problem last time we plotted grad f on both the 3d version and then drop that down onto the corresponding contour map actually it's going to be easier if we now just work with the contour map itself so here's the contour map and what i've done is i've used the gradient function in matlab to just plot out the gradient perpendicular to the contours everywhere in the space so the low sides are at negative y down here and the high sides of a positive y up here now what happens if i want the maximum value for this function f equals x squared y but constrain it to lie on a circle where's the highest point anywhere on that circle so i have a circle of radius a squared and i want to find the minimum maxima on that path as i go around that circle now you can imagine that describing an equation for how x squared y varies as i go around a circle apart it's going to be a complete nightmare but that's not the question we are actually asked what we wanted to know was what's the maximum along that circle not what's the value everywhere on the circle now lagrange was one of those french mathematicians whose name is inscribed around the eiffel tower and what legrand noticed was that when the contours just touched the path then you'll have found the maximum minimum point that is f when f is a little bit smaller it won't quite touch the path see it here in 3d and when it's a bit bigger it might cross a couple of times see here in 2d on the contours but when the contour just touches then we'll have found the minima and maximum of the function f as we go around the path our circle in this case and what lagrange noticed was that when the contour touches the path then the vector perpendicular to the contour is in the same direction up to a minus sign as the vector of the uh of the path itself that's perpendicular to the path so if we can find grad we can find the minimum maximum points and solve the problem if we can find grad perpendicular contour on both the path and the function we're away so we want to maximize the function f of x y which is equal to x squared y subject to some constraint which we're going to call the constraint equation g of x y and that's the equation of a circle x squared plus y squared and it has some value some particular value to the problem we want to solve of a squared so what we're saying is if we've got the function doing something like this and it has its grad that way and the circle has some path that's doing that and it's got its grad that way what we're doing is we're solving grad f is equal to lambda some number times grad g where lambda is lagrange it's lagrange's multiply or verlagrand multiplier and that's all we need to do so we just need to set up this set of equations and then solve them so if i take grad f is equal to the grad of x squared y well that's equal to if i differentiate for the df dx i've got 2xy and if i take d f d y the y just goes and i've got x squared and i'm saying that that's equal to lambda times grad g which is equal to lambda times if i differentiate g x squared plus y squared with respect to x i've got 2x if i differentiate it with respect to y i've got 2y and that's giving me two equations with two unknowns and i've got a third equation which is the constraint equation itself that brings in the actual value of the circle i'm particularly interested in so i've just got to solve that so if i take the first line i've got 2xy is equal to lambda times 2x i cancel the two x's and therefore i've got y is equal to lambda straight out if i take the second row the y row if you like i've got x squared is equal to lambda times two y but lambda is itself y so that's equal to 2y squared so i can say that x is equal to the square root of 2 times y and because i've done the square root i've got a plus minus here and now if i take the the constraint equation itself the third one i've got x squared plus y squared is equal to a squared but x squared is equal to 2y squared so that's equal to 3y squared so then if i square root that i can say that y is equal to a divided by the square root of 3 and again because i've done this square root i've got a plus minus so that's my solutions so i can write those out and i've got the solutions are going to be a over the square root of 3 times if i take y is 1 then x is root 2 times that so root 2 1 and i've got a over root 3 times root 2 minus 1 for y and i've got all the possibilities a over root 3 minus root 2 and 1 and i've got a over the square root of 3 of minus root 2 and minus 1. now if i find the values of the function f of x y as i go uh take all of those so i've got to find x squared y for that so that's a cubed over 3 root 3 because i'm cubing that bit in effect because they're both in x and y and i've got x squared is 2 plus y is what times y is 1 so that's 2. here i've got the same thing but i've got what y is negative so now i've got a cubed over three root three times minus two this one's y is part uh plus and when this squares the minus sign is going to disappear so this is going to give me another plus solution two over 3 root 3 a cubed and i've got here here y is negative so i'm going to get a minus here uh a cubed over 3 root 3. so i've got a max uh a max i've got a min i've got a max the way i've written out and another min so i've got two positive solutions and i wanted to find the maximums so those are there and i've got the minima as well for free so let's look at what that looks like now on the graph so two of our solutions are here and two here when we switch to the 3d view we can see that the two with positive wire maxima and the two with negative y of the minima so that's really neat what we've done here is we've used our understanding of gradients to find the minima or maxima subject to some constraint equation like a straight line or a circle and very often when we want to make it that some of the variables in a function that we want to fit are fixed in relation to each other they have some fixed relationship like that around a circle so this is a very handy thing to be able to do it's really useful so hopefully that was good fun this week you've taken all the multivariable calculus you've done so far and looked at how to put that together with vectors and linear algebra to do vector calculus we've been looking at optimization problems together how to solve equations and optimize functions for example to optimize the fit of a function to some data we started off with the newton-raphson method where we use the gradient to estimate how far we should step to get from a current guess to a solution to our next guess and then we kept doing that until our estimate for the solution converged to an answer we were happy with we then moved on to look at how to find the gradient in the multi-variable case we define the gradient vector grad which is perpendicular to the contour lines and has elements equal to the differential of the function along each of the axes df dx dx fdy and so on and it has magnitude given by the size of grad the sum of the squares of the element we then figured out that we could use grad to go down a hill to find the minimum values of a function by taking little steps down the hill at a time and that's really neat because it means we don't need to evaluate the function everywhere and then find the minimum or solve the function using algebra as long as we can find the gradient an initial estimate we can just take steps down the hill until we find ourselves in the bottom of the valley and this gradient descent method is probably the most powerful method for finding minima that exists finally we looked at how to solve a problem subject to a constraint we found that this reduced to equating the gradient of the function to the tangent of the constraint taking two grads and then solving the simultaneous equations resulting from equating those two vectors that was the lagrange multipliers method so this is really neat the big picture is we've learned how to optimize functions and we've taken ourselves from the realm of doing algebra to the realm of solving problems with computers in the next module what we'll do is we'll move on to look at how to fit functions to data using what's called the least squares method so this week we're finally going to apply all this multivariate calculus to help us fit functions to data this is really handy it allows us to make sense of our data to start doing statistics testing and to start to really apply all of the stuff we've been doing into the real world if you have some big bundle of data the first thing you need to do is to clean it up there are other courses on this but the first thing you need to do is clean it up to get it into shape to start doing the maths with that means figuring out what the sensible thing to do is with things like partially empty data entries zeros those sorts of things to figure out if you can do any dimensionality reduction by binning or grouping the data together eliminating duplicates garbage data all those sorts of things often the easiest thing to do is to tag all of your data so that you can reorder them and put them back in when you've changed the order then sort them and look for funny values and then figure out what to do with those to get rid of them all together or to replace them with something sensible and then once you've cleaned it up you can start graphing it take averages look at standard deviations and all those sorts of things of course in doing this it really helps if you're interested in the data you need it to be kind of like a person you that you want to figure out and understand until it becomes an old friend if over time as you collect more data when it starts to change you want to get to the point where you're going to notice those changes actually so you really want to get it intimate and friendly with your data once you've graphed it in a sensible way you'll often get a plot like this guy here which is a simple xy plot of some data this data seems to plot like a straight line if you know something physically about the processes involved in generating the data or if you have some hypothesis as to how the variables are related you can try and fit that model to the data alternatively you can just try fitting something sensible based on how it looks like a straight line to this data here for example now i can model my straight line y here as being a function of the i observations x i and a vector a of the fitting parameters in the case of a straight line equals y equals mx plus c then the parameters in the vector a would be the the gradient m and the intercept c of the straight line so here i've plotted the optimal straight line for this data it happens to have a gradient of 215 gigapascal's gpa and an intercept of 0.3 gpa for c i can also find the mean of x x bar and the mean of y y bar which are the geometric center of mass of that data set now in order to find the optimal value of m and c let's first define a residual r which we define as the difference between the data items y i and the predicted location of those on the line at y which would be mx plus c so r is y i minus mxi minus c then i can take a measure of the overall quality of the fit being a quantity i'll call chi squared which is the sum of the squares the residuals are i do this so that i penalize both data that are above and data that are below the line i don't want the pluses and minuses to net off against each other also i really want to badly penalize uh data items that are a long way away from the line and then i'm going to try and find the best chi-squared possible the one that's lowest i'm doing a minimization so now it's worth plotting out what chi squared is going to look like for lots of different possible values of m and c which is what i've done on this contour plot here in the middle at about 215 and near an intercept of zero i find my minimum and the further i get away from those the worse it is and note that in terms of chi-squared it's slanted there seems to be some kind of trade-off the bigger c gets the lower the optimum value of the gradient m and vice versa and if i look on this plot that's kind of obvious if i make the line steeper on the original uh fit then in order for it to fit well the intercept is going to have to get smaller actually i'm pivoting about the center of mass and also this shallow trough here in the chi-squared value is going to be really quite shallow so this is actually going to be quite a tricky problem for a steepest descent algorithm it's going to be easy to get down the size but it's going to be difficult to get down the bottom of the valley to find the actual minimum but nevertheless it looks like it's going to be quite an okay problem to solve it has one easy to spot minimum and therefore we can find it and note that to do this with any precision if i do it simply by doing lots of computations here like i've done uh for different m's and c's and uh finding the minimum that way plotting it all out finding the minimum on this graph to do this i have to do a lot of maths in matlab this contour plot took about 200 000 computations to make so even for a simple problem like this we really do want to find an algorithm that's going to let us get there a bit more efficiently now the minimum is going to be found when the gradient of chi squared is zero so if we just take the grad of chi squared with respect to the fitting parameters and set it to zero that's going to be our solution now the neat thing is that in this particular case we can actually solve this problem explicitly so that's what we're going to do in this video but then we'll go on to see how to do it by linear descent and then we'll find a better algorithm and then we'll explore how to do this sort of problem where it's not so easy to do explicitly so if we differentiate the first row with respect to m then the first thing to worry about is all these sums over the data items i but actually it turns out that we don't need to worry about these sums because we're not differentiating the x i or the y i themselves so they'll just sit benignly in the sum you know mx1 plus mx2 plus mx3 is the same as uh if we differentiate that with respect to m it's to just get x one plus x two plus x three um so uh we don't have to worry about those sums and then it's easy right we differentiate a square uh that drops in power by one uh and we multiply by two and then we take the differential of the inside of the bracket with respect to m is minus x i uh we can then uh take that that minus two outside of the sum altogether in fact for the second row it's easier because the differential the inside of the bracket with respect to c is just minus one so we just get the two down from the power and the minus sign and so it all looks quite easy keeping on uh looking at the second row then the sum of c times the number of data items we can take out of the sum altogether and then we've just got the sum of the y i's and the sum of m times the x i's um and if we divide that through by the number of data items uh we get our result that c is going to be y-bar minus m times x-bar at y-bar and x-bar being the average uh we can carry on in that way and generate an answer to m which i'm just going to leave here i don't think there's any point in showing all the mass to you blow by bow it's a bit trickier to see and i'm not going to go through actually the math and the derivation but you can also find estimates for the uncertainties in c m which i've put up here which i'll call sigma c and sigma m and it's very important actually when you're doing a fit to get an idea of the uncertainties um in those fitting parameters and to quote those in your fit so i'm just going to leave those here in case you need to use them so coming back to our uh fitted data we can plot it out again here now the amazing thing here is just how accurate this sort of fitting really is it's really cool you know we've got uh quite noisy data but and a gradient of 215 but the uncertainty is only nine it's about five percent it's really amazing um now you should always plot your fit and visually compare it just as a sanity check and we can see why this is a good idea here this is anscombe's famous quartet you can find the uh the graph on wikipedia all these four data sets have the same chi squared means best fit lines and uncertainties in the fitting parameters but they're obviously very different data in the right hand two cases probably fitting a straight line is just the wrong thing to do the bottom left if you remove the flyer data point the gradient's different and the intercept it's only the top left where the fit's actually doing the right thing all together and there's another subtlety actually which if we go back and look at c the intercept we can see that the intercept depends on the gradient m which is what we said earlier when we looked at a plot of chi squared now there's a way to recast the problem which is to look at the deviations from the center of the mass of the data at x bar instead and then the intercept b now rather than c is the location of the center of mass in y at y-bar and then the constant term in the fit b that constant b doesn't depend on the gradient anymore and neither therefore does its uncertainty include a term from the uncertainty in m in fact if i plot out uh the contour plot for chi square when i do that i find that it then isn't slanted it's a nice sort of circular looking thing because i've removed the interaction between m and the constant term so it's a mathematically much more reasonable well-postulated problem so that's the essence of regression of how to fit a line to some data and this is a totally really useful life skill whatever almost whatever your professional job what we'll do in the next couple of videos is look at how to do this in more complicated cases with more complicated functions and how to extend the idea of regression to those cases the main thing really that we've defined here that's important to remember is this goodness of fit estimator chi squared the sum of the squares the deviations of the fit from the data and chi squared is going to be really useful to us going forward in this video we're going to look at how to finally learn how to fit our distribution of heights data then in the following exercise you'll actually do that for yourself and then in the next video we'll wrap up with looking at how to do this practically in matlab or python so we're looking at how to fit a function that's arbitrarily complicated compared to the simplest case of linear regression y equals mx plus c that we looked at last time of course there's intermediate possibilities between very complicated and the simplest possible but this gives you the general case before we move on to look at how to the computer using tools instead of writing our own so let's say we have a function y of some variable x and that function has parameters in it ak where k goes from uh 1 all the way up to m so there's m of these a's so for example we could have uh x minus a1 squared plus a2 that would be an example of some function y this function isn't linear in a1 notice if i double a1 i don't double the function um so it's a non-linear least squares we're going to do now say i want to fit the parameters ak to some data so i've got some data observations i've got i equals 1 to n of them and i've got pairs of data y i and x i so for every x i've got a y and i have an associated uncertainty sigma i that is uh the more uncertain i am about the data point y i the bigger the uncertainty sigma is going to be so i can i can sketch that out something like uh like this for instance uh as an example um of my y eyes and my x eyes are there so i've got an x i y i and each one's got a an uncertainty uh sigma i there then i'm going to define a goodness of fit parameter chi squared chi squared there as being the sum over all the data points i um of the difference between the y i and the model of x i uh with its parameters ak and i'm going to divide all of those by sigma squared and i'm going to take the squares of the differences so what i'm doing here is i'm penalizing each of these differences by an uncertainty sigma squared when i make chi squared so that uncertain data points have a low weight in my sum of chi squared so that they don't balance they don't affect the fit too much if we don't know what the sigmas are we could assign them all to be one right this will just drop out but if we have an idea of the measurement uncertainties this gives us a way to include it and my minimum of chi squared is of course going to be when the grad of chi squared is equal to zero now in the general case i might be able to write down an expression for grad here but i might not be able to solve it algebraically so instead i'm going to look to solve grad chi squared equals zero by steepest descent going down the contours simply by updating the vector of fitting parameters a so i've got my vector a i'm going to say that my next iteration is going to be my current iteration minus some constant times the grad of chi squared so i'm going to go down the gradient here by an amount given by the constant which is sort of a pull handle for how aggressive i want to be in going down the steepest descent and that i'm going to use to make my next um guess for what the fitting parameter should be and i'm going to write them down as a vector and i'll keep doing that until that i reach this criteria that the grad of chi squared is zero or which means i've found the minima or failing that until chi squared stops changing which should be the same thing right or i just give up because it's been so many iterations and i get bored and i decide that something's gone wrong so to do this grad i've got to differentiate chi squared so i've got to do d chi squared by d a k for each of the k's in turn and when i do that well the sum has nothing to do with k because it's to do with i um and i'm going to get a 2 down and i'm going to get the sigma squared has nothing to do with k when i differentiate this i'll get the 2 down so and then i'll have the bracket itself y i minus y of x i and the aks and then i'm going to get the differential of the bracket i get a minus sign out of that and i get d y d x a k so i get d y d a k so that's going to be my differential there the minus 2 i can just take out so when i come to update here if i wrap the minus 2 into the constant i can just make that a plus ignore the two so i'm then going to get uh a current plus this thing the sum from i equals 1 to n because the minus signs will go i'll wrap the 2 into the constant then i'll just get this y i minus y over x i and the aks divided by sigma squared times the differential evaluated at a current because i don't know a next yet so the steepest descent formula here is just going to be that a next is equal to a current plus this sum and i've got to be able to differentiate y with respect to a k and this is one of these formulas that look really intimidating but really isn't when you actually try to use it for our example here we take this example here if we differentiate that with respect to a1 then we'll get that dy by a1 is equal to minus 2 x minus a1 um i take the 2 down and i get a minus sign when i differentiate the stuff in the bracket and when i do dy by da2 i'm just going to get 1. so it's actually really easy when we come to finally use it but the expression looks intimidating so that's the steepest descent formula for the case of fitting a non-linear function where we're trying to minimize the sum of the squares of the residuals and this is therefore called non-linear least squares fitting there are lots of more complicated methods than steepest descent for solving these sorts of problems which we'll look at a little bit next time but first i want you to try and give this a go and code it up and see how it looks for the sandpit problem that you were doing before so that's the simplest version of how to do a general fitting a finding of the minimum or least value of the sum of the squares of the residuals for a model that's non-linear in both the functions and the fitting parameters so it's called generalized nonlinear least squares fitting in this video we're going to make some final comments on the least squares regression fitting of data and we're going to look at how we really do this in anger in the world using computational tools like matlab or python or r so you've made a gradient descent least squares minimizer and then use that to solve the sample problem already there are a few comments to make before we move on in reality there are a huge number of solvers for non-linear least squares problems we can observe that if we do a taylor series expansion of chi squared then the second term the second derivative is the hessian which gives us information about the curvature or the gradient of the gradient the gradient of the jacobian and therefore we can shoot directly for where the jacobian is zero just as in newton-raphson using that second derivative now using the hessian would be faster than simply taking steps along a steepest descent algorithm effectively we'd be using the hessian to give us a guess as to the size of the step we should take in gradient descent the problem is is that often the hessian isn't very stable especially far from the minimum so the levenberg marquatt method uses steepest descent far from the minimum and then switches to using the hessian as it gets close to the minimum based on a criteria as to whether chi squared is getting better or not if it's getting better it uses the hessian and if it's her in trouble it uses steepest descent there's also the gauss-newton method and the bfgs method amongst many others that either use the hessian directly or build up information about the hessian over successive iterations and depending on the convergence then different methods may be better than others robust fitting is another topic you should be aware of in case you need to look it up later if we come back to anscum's quartet here we see that the bottom left data set in our problem there's just that one flyer data point a truly robust fitting method would be unbothered by such a data point one approach to robust fitting in minimizes instead of the least squares the absolute are the square deviations so it doesn't weight the points that are far away from the line as strongly and that means it fits a little bit what visually looks a bit better now let's turn to look at how you do this in the real world in matlab it's easy you simply import your data using the import data tab at the top of the screen and then flick over to the apps tab and start up curve fitting the curve fitting app there you can even symbolically define your own algorithm you have to pick a starting guess and it will fit your function for you or you can use a pre-built function that already knows its jacobians and will therefore be faster and more efficient in python it's very nearly as simple in the scientific python scipy set of modules the optimizer module includes a least squares curve fitting minimizer curve fit which is at the link below this video accompanying this video we've left you one of the psyp examples for fitting some data so you can see how to use it uh this is it um and it's amazing uh the fitting uh takes uh really works very well produces this this nice graph here of this apparently noisy data with this crazy function and the fitting only takes three lines the three bulb ones here two lines to define the function and one line to do the fit um the rest of that code is all about plotting it and importing the data and everything else while generating it in this case actually effectively this set of routines in scipy is the modern implementation of what's called min pac which was a fortran set of routines published by george moore in 1980 and described in the book numerical recipes and it's absolutely astonishing how easy it is to do this stuff now you know when i was a student you were reduced to reading this long difficult textbook now you just effectively write one line in python just as in python in the r statistical programming language there's also a minimizer for doing non-linear least squares fitting of models to data so if you like r for looking at data you can also do all this stuff just as easily in r so what i want you to do now is write a python code block to fit the gaussian distribution shown here you'll need to give it a starting guess and we give you the input data for the height distribution in the population this is the final height distribution data set that we've been talking about schematically all the way through these two courses and what you'll find is the mean height b here is about 178 centimeters and the characteristic width of this distribution is about 11 centimeters that's parameter c in the equation here now it's useful at this point to think about why we need to have the starting guess if we started with a guess here for the mean of hundred centimeters the model curve wouldn't overlap with the data at all so when we did a little move to b we get no change and therefore the gradient of chi squared with respect to the mean b would be zero so the algorithm wouldn't know what direction to go in we wouldn't get a sensible answer for the jacobian or grad and therefore our algorithm wouldn't know where to go to find the minimum so in doing any of this data fitting it's vital to come up with a good means for generating a starting guess here it's easy you pick the biggest value for instance not coincidentally it's also equally important to compare the fit to the data and ask if you believe the final fit so what we've done in this video is we've finished our little discussion of using vectors and multivariate calculus together to help us do optimizations of functions and to fit data to functions and it's turned out in the end to be really easy computationally we can fit functions in just a few lines of code in python or matlab or r but now after all this work you understand something of how those algorithms work under the hood and that means that hopefully you'll be much better off at figuring out how to fix them when they go wrong for instance whether jacobian doesn't make any sense and also you've got all the maths you need to access the next course in this specialization on machine learning that brings us to the end of this introductory course on multivariate calculus for machine learning congratulations to you all for sticking with the course and thank you for your contributions to the forums we've moved very quickly starting right at the beginning with the definition of the derivative then exploring multi-dimensional mountain ranges training neural networks and finally finishing up by seeing regression in action although we certainly didn't cover all of the details of a traditional calculus course what i hope is that you come away with an understanding of the fundamentals and language of calculus as well as some intuition as to where it might be usefully applied in machine learning it's been such a pleasure to teach you and best of luck with the next course on the mathematics for machine learning specialization hi i'm mark i'm a lecturer in statistical machine learning at imperial college in this course we're going through the mathematical foundations that we need in order to do dimensionality reduction with principal component analysis data in real life is often high dimensional for example if we want to estimate the price of our house in years time we can use data that helps us to do this the type of the house the size the number of bedrooms and bathrooms the value of houses in the neighborhood when they were bought the distance to the next train station and park the number of crimes committed in the neighborhood the economic climate and so on and so forth there are many things that influence the house price and we collect this information in a data set that we can use to estimate the price of our house another example is the 640 by 480 pixels color image which is a data point in a one-dimensional space where every pixel corresponds to three dimensions one for each color channel red green and blue working with high dimensional data comes with some difficulties it is hard to analyze interpretation is difficult visualization is nearly impossible and from a practical point of view storage can be quite expensive however high dimensional data also has some nice properties for example high dimensional data is often over complete that means many dimensions are redundant that can be explained by a combination of other dimensions for example if we took a color image with four channels red green blue and a grayscale channel then the grayscale channel can be explained by a combination of the other three channels and the image could equally be represented by red green and blue alone as we can reconstruct the grayscale channel just using that information dimensionality reduction exploits structure and correlation and allows us to work with a more compact representation of the data ideally without losing information we can think of dimensionality reduction as a compression technique similar to jpeg or mp3 which are compression algorithms for images and music let's take a small binary image of the handwritten digit 8. we're looking at 28 by 28 pixels each of which can be either black or white therefore this image can be represented as a 784 dimensional vector however in this example the pixels are not randomly black or white there is structure they often have the same value as the neighboring pixels in this data set there are many examples of an eight they differ a little bit but they look sufficiently similar that we can identify them as eight we can use dimensionality reduction now to find a lower dimensional representation of all eights that is easier to work with than a 784 dimensional vector the lower dimensional representation of a high dimensional data point is often called a feature or a code in this course we will look at a classical algorithm for linear dimensionality reduction principal component analysis or pca the purpose of this course is to go through the necessary mathematical details to derive pca in the first module we will look at statistical representations of data for example using means and variances and we also describe how means and variances change if we linearly transform our data set in the second module we will look at the geometry in vector spaces and define how to compute distances and angles between vectors in the third module we'll use these results to project data onto lower dimensional subspaces and in the fourth module we'll derive pca as a way to reduce dimensionality of data using orthogonal projections when we work with data we often need to find compact ways to describe some of its properties it wouldn't be particularly handy or useful to give somebody the entire data set instead we can use some statistical properties to describe the data for example we can say something about the center or the average of the data or the spread or orientation in the following videos we'll be looking at exactly these kind of statistical properties and discuss mean values to describe average data points variances that make statements about the spread and covariances which characterize orientation and correlations data is often compactly described by some of its statistical properties such as the mean and the variance in this video we'll explain how to compute means of data sets the mean of a data set describes the average data point the mean does not have to be a typical data point and it also does not need to be part of the data set itself for example if we look at a set of images of digit 8 the average image looks like this it has properties of all images in the data set but it is not part of the data set itself we obtain the average 8 as follows remember that an image can be represented as a long vector in a high dimensional vector space by stacking all pixels together after transforming all images into these vectors we take all image vectors in our data set add them together and divide by the number of images in the data set this gives us the average image vector if we then reshape that vector into an image again we get the average digit in the data set here's an example with four eighths the mean of the first one is just the image itself but when we add the second image we see that the average image now contains properties from both images when we add the third image the mean image is all three images on top of each other divided by three after the fourth image we can still see characteristics of all four images in the average image when we add more images to our data set the average digit becomes a bit more blurry and if we take all eights in our data set we get this eight as the average image generally if we have a data set x1 to xn let's say d consists of data points x12 xn we get the mean value or the expected value of this data set as follows we write well the expected value of d is one over the number of data points so this is n times the sum n equals 1 to capital n of x n so we sum up all data points in our data set and divide by the number of data points that we have let's look at an example i create a data set consisting of the five numbers that i get when i roll five dice we got one two four six and 6. so let's write this down so we're going to call this d prime is 1 2 4 6 and 6. and now the expected value or the average of this data set is the sum of all elements in this data set so this is 1 plus 2 plus 4 plus 6 plus 6 divided by the number of elements in our data set and that is 5. so if we sum these things together and divide by 5 we get 19 over 5 or 3.8 we can clearly see that 3.8 is not part of the data set and cannot even be achieved by rolling a dice therefore it is also worthwhile keeping in mind that although the mean is the average of the data set it does not have to be a typical instance in this video we computed the mean value of data sets as the average data point the mean value doesn't have to be part of the data set in the next video we'll introduce variances to describe the spread of the data around the mean value in the last video we discussed mean values of data sets which represent an average data point in this video we will look at variances to describe other properties of a data set let us have a look at two different data sets d1 and d2 d1 is represented by the blue dots located at 1 2 4 and 5 and d2 is represented by the red squares at -1 3 and 7. d1 and d2 have the same mean which is 3 but the data points in d2 are less concentrated around the mean than the data points in d1 remember the mean value is the data point we would expect on average but to describe the concentration of data points around the mean value we can use the concept of the variance the variance is used to characterize the variability or spread of data points in a data set in one dimension we can look at the average square distance of a data point from the mean value of this data set okay let's do this for d1 and d2 so d1 was 1 2 4 and 5 and the mean value or expected value of d1 was 3 and d2 was -1 3 and 7 with exactly the same mean value so now we want to compute the average squared distance of d1 from the mean and from d2 from the same mean so let's do this for d1 first so we get 1 minus 3 squared plus 2 minus 3 squared plus 4 minus 3 squared plus 5 minus 3 squared so these are the sum of the average of the square distances and to get the average we divide by 4 which is the number of data points in d1 so if we do the computation we get 4 plus 1 plus 1 plus 4 divided by 4 which is 10 over 4. so now we do the same for d2 and for d2 we get minus 1 minus 3 squared plus 3 minus 3 squared plus 7 minus 3 squared and we divide by the number of data points in d2 which is 3 and we get 16 plus 0 plus 16 divided by 3 which is 32 over 3. so now this number is now bigger than this number which means that the average squared distance of d2 from the mean value is bigger than the average squared distance of d1 from the from the mean value which indicates that the spread of the data is higher in d2 then in d1 so what we have done can be formalized so assuming we have a data set consisting of n data points x1 to xn then we can define the average squared distance as the following so we have x1 up to xn and then we define this to be our data set x and we define now the variance of this data set to be 1 over n times the sum of small n equals 1 to big n of x n minus mu squared where mu is the mean value of the data set x yeah so what we have done here is exactly the same as what we did before with d1 and d2 we computed an average squared distance of the data points in the data set from the mean value of the data set and now we can also make some statements about this first the variance as defined here can never be negative because we just sum up squared values and that also means we can take the square root of the variance and this is called the standard deviation the standard deviation is expressed in the same units as the mean value whereas the variance unfortunately expressed in squared units so comparing them is quite difficult therefore when we talk about spread of the data we usually look at standard deviations so in this video we looked at variances of one-dimensional data sets and in the next video we will generalize this to higher dimensions in the last video we looked at variances for one-dimensional data sets and in this video we were looking at variances for high-dimensional data sets the intuition and definition of the variance we had earlier does not really work in the same way in high dimensions and squaring vectors is not really defined assuming we have a two dimensional data set we can now compute variances in the x direction in the y direction but these variances are insufficient to fully describe what is going on in the data set in particular we only have the variation of the data in either direction independent of the other direction but we may also be interested in the relationship between the x and y variables and this is where the concept of the covariance between these components comes into play let's have a look at an example in two dimensions for this data set we can compute the variances in the y direction and the variances in the x direction which are indicated by these vertical and horizontal bars but this can be insufficient because we can look at other examples where the variances in the x and y directions are the same but the data set look very different if we look at this particular example we have a different shape of the data set but the variances in the x direction and the variance in the y direction are exactly the same and the mean values of these data sets are also identical and i can look at different sets like this one here and this one these four data sets look very different we've seen four different examples with four different properties or shapes of the data sets but the variances in x and the variances in y and the mean values are identical if we exclusively focus on the horizontal and vertical spread of the data we can't explain any correlation between x and y in this last figure we can clearly see that on average if the x value of a data point increases then on average the y value decreases so that x and y are negatively correlated this correlation can be captured by extending the notion of the variance to what is called the covariance of the data the covariance between x and y is defined as follows so covariance between x and y is defined as the expected value of x minus the mean in the x direction times y minus the mean in the y direction just right mu x is the expected value in the x direction and mu y is the expected value of the y coordinates for the 2d data we can therefore obtain four quantities of interest we obtain the variance of x we get the variance of y and we also have the covariance terms the covariance between x and y and the covariance between y and x so we summarize these values in this matrix called the covariance matrix with four entries and in the top left corner we have the variance in the x direction then we get the covariance term between x and y in the top right corner the covariance between y and x in the bottom left corner and the variance of y in the bottom right corner if the covariance between x and y is positive then on average the y value increases if we increase x and if the covariance between x and y is negative then the y value decreases if we increase x on average if the covariance between x and y is zero then x and y have nothing to do with each other they are uncorrelated the covariance matrix is always a symmetric positive definite matrix with the variances on the diagonal and the cross-covariances or covariances on the off-diagonals if we now look at d-dimensional data sets let's say we have a data set consisting of n vectors x 1 2 x n and every x i is in rd then we can compute the variance of this data set as 1 over n times the sum i equals 1 to capital n x i minus mu times x i minus mu transpose where mu is the mean of the data set and this is called the covariance matrix of the data and this is a d by d matrix so previously we looked at means and variances to describe statistical properties of data sets in this video we'll discuss what happens to means and variances when we linear transform the data set that means we shift it around or stretch it so let's look at a data set d which is given by these blue dots over here so we have a data set d which is -1 2 and 3. now let's compute the mean value of this data set to start with so the expected value of d is minus 1 plus 2 plus 3 divided by 3 which is 4 over 3. so i'm indicating the mean value of this data set by this blue star so now the question is what happens to this data set when we shift it so let's have a look at a shift by two to the right so we end up with these red dots now what happens to the mean value of this data set if we shift the data set the original data set to the right well the answer is the mean also shifts by 2. so let's define d prime to be the data set one plus sorry one four and five which is d plus two for every individual component in d then the expected value of d prime is 1 plus 4 plus 5 divided by 3 which is 10 over 3 and in particular we can write this in a slightly different way we can write this as 4 over 3 which comes from here plus 2 which is the shift or the offset of the original data set so we can generalize this now to general shifts we can write an expected value of d plus a where a is a constant factor is a plus the expected value of d so up to this point we know what happens to the mean of the data set when we shift the data set but what happens if we stretch the data set let's say we also stretch by a factor 2 and that means we multiply every individual component in the data set d by 2. so then we end up with this data set on the indicated by the red dots here and we define d double prime to be minus 2 4 and 6. and if we now compute the mean of d double prime then we end up with minus 2 plus 4 plus 6 divided by 3 which is 8 over 3 and we can rewrite that again in a convenient way which is four over three which again comes from here plus sorry times 2 which is the scaling factor so in general we can write that the expected value of alpha times d is alpha times the expected value of d putting everything together we can shift and scale our original data set and the new mean of this linear transformation is given as follows so if we compute the expected value of alpha times d plus a we get alpha times the expected value of d plus a where alpha is the scaling factor and a is the shift in this video we looked at the effect of linear transformations on the mean value of a data set in the next video we will look at exactly the same for the variances we've seen what shifting and scaling a data set does to the mean now let's have a look at the effects on the variance remember that the variance is a measure of the spread of the data what do we expect when a data set is shifted let's have a look at this data set over here we have three data points given at minus one plus two and plus three and now we are shifting the data set towards the right the variance of the data set is indicated by the blue bar on the at the bottom the shifted data set is now given by the red dots and we shift every individual data point by two so question is now what happens to the variance well the shift of the data does not really affect the relation of the data points among themselves therefore the variance does not change the variance is identical so the values of the blue data set is identical to the variance of the red data set therefore a general result is that if we have the variance of d that is exactly the same as the variance of our data set d plus a where a is an offset applied to every individual element of d let's now scale the data set and see what effect that has on the variance of the data so we're going to take exactly the same data set as before the variance is indicated by the blue bar and we're going to scale every individual data point by two so the question is what is the variance of two times d so new data set is indicated by these red dots remember that the variance is the average squared distance of the data points from the mean if we scale the data set by a factor of 2 the distance of every data point to the mean is scaled by 2 but the squared distance is scaled by 4 and the variance is therefore 4 times as big as it used to be and our next result is that the variance of alpha times d is alpha squared times the variance of d where alpha is a real number that scales every individual element in the data set d now let's have a look at high dimensional problems assume we have a data set d which is a collection of data points x1 to xn and the xi live in rp remember the variance of this data set is given by a covariance matrix if you perform a linear transformation of every data point say ax i plus b for a given matrix a and an offset vector b the question is what happens to our data set if we do this to every single data point while we get the covariance matrix of the transformed data set as follows we're getting the variance of a times d plus b is a times the variance of d times a transpose in this video we saw what effects a linear transformation of a data set has on the mean and the variance in particular we saw that shifting data has only an effect on the mean but not the variance where scaling the data affects both the mean and the variance we've already come to the end of this module we looked at statistics to summarize and characterize data sets means variances and covariances we looked at how these statistics change when we shift or scale the data set we'll use means and covariances in module 4 when we discuss the pca algorithm for dimensionality reduction well done it is often necessary to measure similarity between data points in the context of dimensionality reduction we are interested in finding compact representations of data that live in a lower dimensional space but which are similar to the original data one of the key concepts we will be looking at in this course is orthogonality we use orthogonal projections of data points as a way to compress data while minimizing the loss of information thinking of data points as vectors in the vector space will allow us to look at two types of similarity between data points distances of data points from each other and the angle between these two data points let's have a look at an example assume we have we're living in a two-dimensional world and we have two vectors x and y things that we will be discussing in this course are how to compute the length of individual vectors so that means for x that means the length of this line here we will be looking at angles between two vectors and we would be looking at distances between individual data points or vectors in this vector space so in this case you would be interested in the distance between x and y in order to measure angles and compute lengths and distances we need to equip the vector space with an inner product which allows us to talk about geometric properties in this vector space in the next videos we will be looking at computing lengths distances between vectors and angles between vectors in order to measure angles lengths and distances we need to equip the vector space with an inner product which allows us to talk about geometric properties in a vector space an example of an inner product that we may know already is the dot product between two vectors x and y if x and y are two vectors in rn so then the dot product is defined as uh x transpose y is the sum i equals 1 to n of x i times y i where x and y are n dimensional vectors the length of x is then defined as the square root of the dot product of x with itself so the length of x is the square root of x transpose x which we can also write as the square root of the sum of i equals 1 to n of x i squared let's have a look at an example we take a vector x as 1 2 and the vector y as 2 1 in a 2 dimensional plane then we can compute the length of vector of the vector of x as the square root of 1 squared plus 2 squared which is the square root of 5 and the length of y similarly is the square root of 5 as well if we're interested in the distance between two vectors x and y we simply compute the length of the difference vector so we generally define then the distance between x and y to be the length or the norm of x minus y which is the square root of x minus y transpose times x minus y so now let's compute the distance between our two vectors over here so the distance between these two vectors is effectively just the length of this difference so we can write this as the norm of 1 2 minus 2 1 so this is our difference vector and that is the norm of minus 1 and 1 and this is the square root of 1 plus 1 which is the square root of 2. so the length of this yellow segment is square root 2. so the last thing we are still interested in is the angle between two vectors and we can compute the angle also using the dot product the cosine of the angle let's call this angle alpha so the cosine of alpha is given by x transpose y divided by the length of x times the length of y and in our example over here we'll get that the cosine of the angle between x and y is x transpose times y which is 4 divided by the length of x times the length of y but we've done these computations already which come from here so it's square root 5 in any case so therefore we get the cosine alpha is 4 over 5 which corresponds to approximately 0.64 radians and that is our angle alpha between these two vectors in this video we looked at dot products as special cases of inner products to compute lengths of vectors to compute distances between vectors and angles between two vectors and in part two of this video we will be looking at general inner products to compute exactly the same quantities previously we looked at the dot product to compute angles lengths and distances in this video we will be looking at generalizations of the dot products in order to compute angles lengths and distances sometimes it is necessary to use an unconventional way to measure these geometric properties and the inner product allows us to do exactly this kind of thing an inner product is a generalization of the dot product but with the same idea in mind we want to express geometric properties such as lengths and angles between vectors let's define what an inner product actually is so let's write the definition we're looking at two vectors for any vectors x and y in a vector space v an inner product is defined as a symmetric positive definite bilinear mapping so let's see what that means we take a mapping that takes two inputs out of this vector space it's a mapping from v times v to the real numbers and we say this function is symmetric positive definite and bilinear so let's unpack this a little bit let's start with bilinearity bilinear means that for vectors x y and z in this vector space and real numbers lambda we get that lambda times x plus z and y so the scalar so the inner product between lambda x plus z and y can be written as lambda times the inner product between x and y plus the inner product between set and y so this is linearity only in the first argument of this function and we require to have linearity also for the second argument of this function so similarly we will then require that the inner product between x and lambda y plus z is lambda times the inner product between x and y plus the inner product between x and z so this means linearity also in the second argument and that's the reason why this is called bilinear it means linearity in both arguments of this function so positive definite means that the inner product of x with itself is greater or equal to zero and equality holds if and only if x is zero vector and the last component that we need is symmetry and symmetric means that the inner product of x and y is the same as the inner product of y and x so the order does not matter let's have a look at an example in r2 if we define our inner product to be x transpose times the identity matrix times y then we get exactly the dot product that we are very familiar with but now let's have a look at a different example where we define our inner product to be x transpose times a times y where a is the matrix 2 1 1 2 then we can also write our inner product to be 2 times x 1 y 1 plus x 2 y 1 plus x 1 y 2 plus 2 times x 2 y 2 and this inner product is different from the dot product any symmetric positive definite matrix in this equation defines a valid inner product in this video we introduce the concept of an inner product which we will use in the next videos to discuss geometric properties of vectors such as lengths and angles in the last video we defined inner products now we will use inner products to compute the lengths of vectors and distances between vectors the length of a vector is defined by the inner product using the following equation so the length of a vector x is defined as the square root of the inner product of x with itself remember that the inner product is positive definite that means this expression is greater or equal than zero therefore we can take this square root we can now also see that the length of a vector depends on the inner product and depending on the choice of the inner product the length of a vector can be quite different similarly the geometry in the vector space can be very different the length of x is also called the norm of x so let's have a look at an example assume we're interested in computing the length of a vector in two dimensions and x is given as the vector 1 1. so in a diagram we have 1 here and approximately here then our vector x would be here and now we're interested in computing the length of this vector so in order to compute the length of the vector we need to define an inner product so why don't we start with the standard dot product so if we define x y to be x transpose y so the inner part of x and y to be x transpose times y then the length of x is the square root of 2. so it's 1 squared plus 1 squared and we take the square root of this now let's have a look at a different inner product let's define x y to be x transpose times 1 minus a half minus a half 1 times y which we can also write as x 1 times y 1 minus a half x1 y2 plus x2 y1 plus x2 y2 using the definition of this inner product then the length of a vector is the square root of x1 squared minus a half x1 x2 plus x2 x1 plus x2 squared and this is identical to the square root of x1 squared minus x1 x2 plus x2 squared will get smaller values than the dot product definition up here if this expression is positive if we now use the definition of this inner product to compute the length of our vector up here we will get that the squared norm or the inner product of x with itself is 1 plus 1 minus 1 which is 1 and therefore the norm of x is just one and one in this case would be the length of the vector using this unusual definition of an inner product whereas the same vector would be longer had we used the dot product up here the norm that we just looked at also has some nice properties let me write a few of those down so in particular one of the properties is that if we take a vector and stretch it by a scalar lambda then the norm of this stretched version is the absolute value of lambda times the norm of x the second property is the triangle inequality which says that the norm of x plus y is smaller or equal to than the norm of x plus the norm of y let's have a look at an illustration so let's assume we have our coordinate system in two dimensions and we use the standard vectors x equals 1 0 so x would be sitting exactly here so this is x and y is 0 1 then x plus y is sitting here if we use the dot product as our inner product then the norm of x is one which is the same as the norm of y and the norm of x plus y is square root 2. so what the triangle inequality says that x plus y norm is smaller or equal than the norm of x plus the norm of y and that is also true in our case because squared of 2 is smaller equal than 2. and there's another inequality i would like to mention which is the koshi schwarz inequality and that one says that the absolute value of the inner product of x with y is smaller or equal than the product of the individual norms of the two vectors in this video we looked at lengths of vectors using the definition of inner products and now we're going to use this to compute distances between vectors in the next video now that we know how to compute the length of a vector we can also compute distances between any two vectors x and y the distance between two vectors is defined as the length of the difference vector so we can write the distance between x and y to be the norm of x minus y and as we know this depends on the definition of our inner product if we use a dot product then the distance is called the euclidean distance let's have a look at an example we're going to look at two vectors x and y we're going to say x is 2 3 and y is 4 1. so let's draw this so this is x and y is over here in order to compute the distance between these two vectors the first thing we actually need to do is let's have a look at this difference vector so x minus y is 2 minus 4 in the first component and 3 minus 1 in the second component that means we get -2 and plus 2 as the difference vector so now we can define inner products and let's now use the dot product as our first example if we use the dot product to compute the length of this difference vector we will get the square root of the first component squared plus the second component squared which is 4 plus 4 and that's the square root of 8. and if we use a different inner product let's say we define the inner product between x and y to be x transpose times 1 minus half minus a half and one times y then if we now use the difference vector here the result will be the square root of 12. so we can see that depending on the choice of our inner product we will get different answers of what the distance between x and y actually is in this video we computed distances between two vectors using inner products and we saw that depending on the inner product the distance between these two vectors can differ previously we have looked at lengths of vectors and distances between vectors in this video we'll introduce angles as a second important geometric concept that will allow us to define orthogonality orthogonality is central to projections and dimensionality reduction similar to lengths and distances the angle between two vectors is defined through the inner product if we have two vectors x and y and we want to determine the angle between them we can use the following relationship the cosine of this angle between the two vectors is given by the inner product between the two vectors divided by the norm of x times the norm of y let us have a look at an example and let's compute an angle between two vectors x which is 1 1 and y which is 1 two let's quickly draw this this is x and this is y and we're interested in the angle omega between those if we use a dot product as the inner product we get that the cosine of omega is x transpose y divided by the square root of x transpose x times y transpose y which is 3 divided by the square root of 10. this means the angle is approximately 0.32 radians or 18 degrees intuitively the angle between two vectors tells us how similar their orientations are let's look at another example in 2d again with the dot product as the inner product we're going to look at the same vector x that we used to have before so x equals 1 1 which is this vector over here and now we choose y to be minus 1 and plus 1 and this is this vector now we're going to compute the angle between these two vectors and we see that the cosine of this angle between x and y is with the dot product x transpose times y divided by the norm of x times the norm of y and this evaluates to zero this means that omega is pi over two in in radians or if we want to say this in degrees we have 90 degrees this is an example where two vectors are orthogonal generally the inner product allows us to characterize orthogonality two vectors x and y where x and y are non-zero vectors are orthogonal if and only if the inner product is zero this also means that orthogonality is defined with respect to an inner product and vectors that are orthogonal with respect to one inner product do not have to be orthogonal with respect to another inner product let's take these two vectors that we just had where the dot product between them gave that they are orthogonal but now we're going to choose a different inner product in particular we're going to choose the inner product between x and y to be x transpose times the matrix 2 0 0 1 times y and if we choose this inner product it follows that the inner product between x and y is -1 this means that the two vectors are not orthogonal with respect to this particular inner product from a geometric point of view we can think of two orthogonal vectors as two vectors that are most dissimilar and have nothing in common besides the origin we can also find the basis of a vector space such that the basis vectors are all orthogonal to each other that means we get the inner product between bi and bj is zero if i is not the same index as j and we can also use the inner product to normalize these basis vectors that means we can make sure that every bi has length one then we call this an orthonormal basis in this video we discussed how to compute angles between vectors using inner products we also introduce the concept of orthogonality and so that vectors may be orthogonal with respect to one inner product but not necessarily if we change the inner product we will be exploiting orthogonality later on in the course if we have a vector and we want to compute the smallest difference vector to any point on a line that does not contain the vector then we will end up finding a point on the line such that the segment between the point and the original vector is orthogonal to that line in the previous videos we looked at properties of inner products to compute lengths angles and distances we focus on inner products of finite dimensional vector spaces in this video we will look at two examples of inner products of other types of vectors inner products of functions and inner products of random variables the inner products we discussed so far were defined for vectors with a finite number of entries and we can think of these vectors as discrete functions with a finite number of function values the concept of an inner product can be generalized to continuous valued functions as well and then the sum over individual components of vectors turns into an integral and the inner product between two functions is defined as follows we can write that the inner product between two functions u and v is the integral of an interval from a to b of u of x times v of x dx and as with our normal inner product we can define norms and orthogonality by looking at this inner product if that integral evaluates to zero the functions u and v are orthogonal let's have a look at an example if we choose u of x equals sine x and v of x is cosine of x and we define f of x to be u of x times v of x which is sine x times cosine x then we're going to end up with this function this function is sine x times cosine x we see that this function is odd which means that f of minus x equals minus f of x if you choose the integral limit to be minus pi and plus pi then the integral of this product sine of x times cosine of x evaluates to zero and that means that sine and cosine are orthogonal and it actually holds that if you look at a set of functions say 1 cosine x cosine of 2x cosine 3x and so on that all of these functions are orthogonal to each other if we integrate from minus pi to plus pi another example for defining an inner product between unusual types are random variables or random vectors if we have two random variables which are uncorrelated then we know the following relationship we know that the variance of x plus y is the variance of x plus the variance of y where x and y are random variables if you remember that variances are measured in squared units this looks very much like the pythagorean theorem for right triangles that one states that c squared equals a squared plus b squared if we look at triangles of this form where this is a b and this is c let's see whether we can find a geometric interpretation of the variance relation of uncorrelated random variables random variables can be considered vectors in a vector space and we can define inner products to obtain geometric properties of these random variables if we define the inner product between two random variables between x and y to be the covariance between x and y we see that the covariance is symmetric positive definite and linear so linearity would mean that the coherence of lambda times x plus y and z where x y and z are random variables and lambda is a real number is lambda times the covariance between x and z plus the covariance between y and z and if the length of a random variable is the square root of the covariance between of x with itself which is the square root of the variance of x then this is the standard deviation of the random variable x so this is the length of a random variable therefore the zero vector is a vector that has no uncertainty that means standard deviation is zero if we now look at the angle between two random variables we get the following relationship we get the cosine of theta which is the angle between two random variables is by definition the inner product between the two random variables divided by the length of the first random variable times the length of the second random variable and if we now write this out using the definition of our inner product we get the covariance between x and y divided by the square root of the variance of x times the variance of y and this evaluates to zero if and only if the covariance between x and y is zero and that is the case when x and y are uncorrelated coming back now to our geometric interpretation we would now replace a with the standard deviation of x b is the standard deviation of y and c is the square root of the variance of x plus the variance of y and this is how we get our geometric interpretation of random variables in this video we looked at inner products of rather unusual objects functions and random variables however even with functions and random variables the inner product allows us to think about lengths and angles between these objects in the case of random variables we saw that the variance of the sum of two uncorrelated random variables can be geometrically interpreted using the pythagorean theorem this is the end of this module congratulations in this module we introduced the concept of an inner product a generalization of the dot product the inner product allows us to talk about geometric concepts such as length distances and angles between vectors one special case we discussed was orthogonality in the next modules we will use orthogonality heavily for projections of high dimensional data onto lower dimensional subspaces and integral part of principal component analysis high dimensional data is often hard to analyze or visualize however high dimensional data quite often possesses the property that only a few dimensions contain most information and most other dimensions are not essential to describe key properties of the data when we compress or visualize high dimensional data we lose information ideally we find the most informative dimensions in the data and keep them when we compress the data and ignore the irrelevant dimensions in this course we will have a close look at orthogonal projections which play a central role in algorithms for dimensionality reduction in this video we'll look at orthogonal projections of vectors onto one-dimensional subspaces let's look at an illustration we're given vector x in two dimensions and x can be represented as a linear combination of the basis vectors of r2 we also have a one-dimensional subspace u with a basis vector b that means all vectors in u can be represented as lambda times b for some lambda now we're interested in finding a vector in u that is closest to x let's have a look at this when i compute the length of the difference of all vectors in u and the vector x i'm getting the graph on the right it turns out that we can find the vector in u that is closest to x by an orthogonal projection of x onto u that means the difference vector of x and its projection is orthogonal to u overall we are looking for the orthogonal projection of x onto u and we will denote this projection by pi u of x the projection has two important properties first since pi u of x is in u it follows that there exists a lambda in r such that pi u of x can be written as lambda times p a multiple of the basis vector that spans u then lambda is the coordinate of the projection with respect to the basis b of the subspace u the second property is that the difference vector of x and its projection onto u is orthogonal to u that means it's orthogonal to the basis vector that spans u so the second property is that the inner product between b and the difference between pi u of x and x is zero so that's the orthogonality condition these properties generally hold for any x in rd and one dimensional subspace is u now let's exploit these two properties to find pi u of x i've flown the setting once more over here we have a two-dimensional vector x we have a one-dimensional subspace u which is spanned by the vector b and we're interested in finding the orthogonal projection of x onto u which we call pi u of x and we have two conditions for pi u of x the first thing is that since pi u of x is an element of u we can write it as a scaled version of the vector b so there must be a lambda in r such that pi u of x is lambda times b and the second condition is the orthogonality condition that the difference vector between x and pi u of x is orthogonal to to u which means it's orthogonal to the spanning vector b and now let's explore these two properties to find pi u of x so first we start writing we use the the condition that b and pi u of x minus x inner product is 0 which is equivalent to that the inner product with of b and pi u of x minus the inner product of b and x is zero where we exploited now the the linearity of the inner product now we're going to rewrite pi u of x as lambda times b so this is equivalent to b times lambda b or inner product with lambda b minus the inner product of b and x that must be zero now we can move the lambda out again because of the linearity of the inner product which is then lambda times the squared norm of b minus the inner product of b and x must be zero and that is equivalent to lambda is the inner product of b with its with x divided by the squared norm of b so now we've found lambda which is the coordinate of our projection with respect to the basis b and that means that our projection using the second condition sorry first condition is lambda times b which is now the inner product of b with x divided by the square norm of b times b if we choose the dot product as the inner product we can rewrite this in a slightly different way so we would we would get b transpose times x times b divided by the squared norm of b so now given that this one is a scalar we can just move it over here so this is equivalent then just saying b times b transpose divided by the square norm of b times x is our projected point and if we look at this this is a matrix and this matrix is a projection matrix that projects any point in two dimensions onto the one-dimensional subspace so now if we look at the special case of b having norm 1 then we get a much simpler result and we would get so if if the norm of b equals 1 then we will get that lambda is b transpose times x and pi u of x is b times b transpose times x so you would get the coordinate of the projected point with respect to the basis v just by looking at the the dot product of b with x and the the projection matrix is simply given by b times b transpose let me make a comment at the end our projection pi u of x is still a vector in rd however we no longer require d coordinates to represent it but we only need a single one which is the lambda in this video we discussed orthogonal projections of vectors onto one-dimensional subspaces we arrived at the solution by making two observations we must be able to represent the projected point using a multiple of the basis vector that spans the subspace and the difference vector between the original vector and its projection is orthogonal to the subspace in the next video we will look at an example in the last video we derived the formula for projections of vectors onto a one-dimensional subspace in particular we arrived at this equation if we choose the dot product as the inner product here x is a d-dimensional vector and b is the basis vector that spans that one-dimensional subspace that we want to project x onto in this video we're going to look at an example assume our vector b that spans our one dimensional subspace is the vector 2 1 and the vector x we want to project onto that subspace is given by one two let's quickly draw this so x is the vector 1 2. so x is living over here and the vector b is going to be the vector 2 1 and so this is b and b spanning our subspace means that our subspace u is going to extend along this line so now we're interested in computing the orthogonal projection of x onto u so using now this equation here we get so x transpose times b is 2 plus 2 and now we divide by the squared norm of b so the length of b is 2 squared plus 1 squared square root so overall we get the squared norm to be 5 and then times b will get us to 2 1 so overall 4 over 5 times the vector 2 1. this means that our orthogonal projection is four over five times the vector b that means if we take four fifth of this vector then we will get to our orthogonal projection so this point here is now pi u of x so the orthogonal projection of x onto u and the length of this segment is four-fifth times the length of b we've gone through orthogonal projections onto one-dimensional subspaces in the next video we're going to look at orthogonal projections onto high dimensional subspaces in the last video we learned about orthogonal projections onto one-dimensional subspaces in this video we look at the general case of orthogonal projections onto n-dimensional subspaces for this we'll exploit the same concepts that worked in the one dimensional case let's start with an illustration we're going to look at a case where we have a vector x that is living in a three-dimensional space and we define a subspace a two-dimensional subspace u which has basis vectors b1 and b2 which are for example this vector and b2 is this vector so we write u is spanned by b1 and b2 so u in this case would be the plane down here so this is u so now we are looking at the orthogonal projection of x onto u and we're going to denote this by pi u of x so that projection is going to look something like this that's the projection point so this is the orthogonal projection of x onto the subspace u so we can already make two observations the first thing is that because pi u of x is an element of u it can be represented as a linear combination of the basis vectors of u so that means we can write pi u of x is lambda 1 times b 1 plus lambda 2 times b 2 for appropriate values of lambda 1 and lambda 2. and the second property is that the difference vector of x minus pi u of x of this this vector over here is orthogonal to u which means it's orthogonal to all basis vectors of u and we can now use the inner product for this and we can write that x minus pi u of x inner product with b1 must be zero and the same is true for b2 but now let's formulate our intuition for the general case where x is a d-dimensional vector and we're going to look at an m-dimensional subspace u okay let's derive this result i copied our two insights up here and i have defined two quantities a lambda vector which consists of all these lambda i here and a b matrix where we just concatenate all basis vectors of our subspace u now with this definition we can also write pi u of x equals b times lambda let's assume we use the dot product as our inner product now if we use our second property we'll get that so pi u of x minus x inner product with bi is now equivalently written as the inner product of b lambda minus x and bi and this needs to be zero where i just used the definition of p u of x in here so now we can simplify this by exploiting the linearity of the inner product and we'll get b lambda times or inner product with bi minus the inner product of x with bi needs to be zero and this holds for i equals 1 to m with the inner product we can now write this in the following way we can write it as lambda transpose times b transpose times bi minus x transpose times bi equals 0 for i equals 1 to m and now we can write this as a set of conditions and if we summarize this we would get lambda transpose times b transpose times b minus x transpose times b must be zero now we need to talk here about an m dimensional zero vector what we would like to do now is we would like to identify lambda for this we are going to right multiply the inverse of b transpose times b onto the entire equation and then we get lambda transpose equals x transpose times so x transpose times b times b transpose b inverse which then also means we can write lambda as the transpose of this entire expression we get b transpose b inverse so this matrix is symmetric so it's tran transpose is the same as the original matrix times b transpose x so now we have identified lambda to be this but we also know that our projection point can be written as b times lambda so this means we will get pi u of x as b times lambda which is b times b transpose b inverse times b transpose x we can now identify this expression as the projection matrix similar to the one dimensional case and in the special case of an orthonormal basis b transpose times b is the identity matrix so we would get pi u of x is b times b transpose times x the projected vector pi u of x is still a vector in rd but we only require m coordinates the lambda vector over here to represent it as a linear combination of the basis vectors of the subspace u we also effectively got the same result as in the one dimensional case remember in 1d we got lambda equals b transpose x divided by b transpose b and we got a projection point pi u of x which was b transpose x divided by b transpose b times b now b transpose b is now expressed as matrix b transpose times matrix b but we now have the inverse matrix sitting here instead of dividing by a scalar that's the only difference between these two results in this video we looked at orthogonal projections of a vector onto a subspace of dimension m we arrived at the solution by exploiting two properties we must be able to represent the projection using a linear combination of the bases of the subspace and the difference vector between the original vector and its projection is orthogonal to the subspace in the next video we're going to look at a concrete example the last video we derived orthogonal projections of vectors onto m-dimensional subspaces in this video we'll run through a simple example we're going to define x to be a three dimensional vectors given by 2 1 1 which is over here and we define two basis vectors for our two-dimensional subspace b1 to be one two zero and b2 to be one one zero so that means u which is spanned by b1 and b2 is going to be effectively the plane and its extension so this all is u the orthogonal projection was given as pi u of x is b times lambda and we define b now to be b1 and b2 concatenated which is 1 2 0 1 1 0 and lambda was given as b transpose b inverse times b transpose x b transpose times x is given as 4 3 vector b transpose b is a 2 by 2 matrix which is 5 3 3 2. now we solve for lambda as b transpose b inverse times b transpose x which means we find lambda such that b transpose b lambda equals b transpose x using gaussian elimination we arrive at lambda equals minus 1 3 and this implies our projection of x onto the space spanned by the two b vectors is minus one times b one plus three times b two which is two one zero in our diagram over here this would correspond to this vector here this is pi u of x this result makes sense because our projected point has as a third component the zero and our subspace requires that a third component is always zero our projected vector is still a three-dimensional vector but we can represent it using two coordinates if we use the bases defined by b1 and b2 therefore that is the compact representation of the projection of x onto this lower dimensional subspace in this video we looked at a concrete example of the orthogonal projection of a three-dimensional vector onto a two-dimensional subspace in the next video we're going to exploit orthogonal projections and derive a dimensionality reduction algorithm called principal component analysis in this module we discussed orthogonal projections of vectors onto subspaces we exploited two things to get to the solution the projected vector can be represented as a linear combination of the basis of the subspace and the vector that connects the data point and its projection must be orthogonal to the subspace we will use orthogonal projections in the next module when we introduce the pca algorithm this was a tricky module but we made it through well done in this course we will introduce principal component analysis of pca an algorithm for linear dimensionality reduction pca has been around for about 100 years and is still one of the most commonly used techniques for data compression and visualization high dimensional data for example images often has the property that it lies on a low dimensional subspace and that many dimensions are highly correlated here's an illustration in two dimensions although the data does not quite lie on a straight line the data does not vary much in one dimension so that we can express it as if it was on a line with nearly no loss a key idea behind pca is to use orthogonal projections to find lower dimensional representations of data that retain as much information as possible similar to the example that we just looked at in the following videos we will derive pca as an algorithm that minimizes average reconstruction errors by orthogonal projections in this video we will introduce the setting of pca and the high level idea assume we have a data set x in rd consisting of n vectors so x is a data set and we have n vectors x 1 to x n where the x i are d dimensional vectors our objective is to find the low dimensional representation of the data that is as similar to x as possible before we start let's briefly review three important concepts the first one is that every vector in rd can be represented as a linear combination of the basis vectors so let's write this down so we write x n can be written as the sum of i equals 1 to d of beta i n times b i where in the following we will assume that the bi are an orthonormal basis of rd if we assume that we use the dot products our inner product and b1 to bd on orthonormal basis we can also write the beta i n as x and transpose times b i which means we can interpret beta i n to be the orthogonal projection of x n onto the one dimension subspace spanned by the ith basis vector the third property is that if we have an orthonormal basis b1 to bm of rd and we define b to be the matrix that consists of these orthonormal basis vectors then the projection of x onto the subspace we can write as x tilde is b times b transpose times x that means x tilde is the orthogonal projection of x onto the subspace spanned by the m basis vectors and b transpose times x are the coordinates of x tilde with respect to the basis vectors collected in the matrix b this is also called the code so coordinates or code now let's have a look at pca the key idea in pca is to find a lower dimensional representation x and tilde of xn that can be expressed using fewer basis vectors let's say m we assume the data is centered that means the data set has mean 0 and we also assume that b1 to bd on orthonormal basis of rd generally we can write any x and tilde in the following way x and tilde can be written as a sum i equals 1 to m of beta i n times b i plus the sum of i equals m plus 1 to d of beta i n times bi so we took this entire thing is still living in rd so we took our general way of writing any vector in rd which comes from property one and we split the sum in property one into two sums one is living in a m-dimensional subspace and the other one is living in a d minus m-dimensional subspace which is an orthogonal complement to this particular subspace in pca we ignore the second term so we get rid of this part and then we call the subspace that is spent by the basis vectors b1 to bm the principal subspace so b1 to bm span the principal subspace although x n tilde is still a d dimensional vector it lives in an m dimensional subspace of r d and only m coordinates beta n 1 to beta n m are necessary to represent it so these ones are the coordinate of this x and tilde vector the betas are then also called the code or the coordinates of tilde x n with respect to the basis vectors b1 to bn and the setting now is as follows assuming we have data x1 to xn we want to find parameters beta i n and orthonormal basis vectors bi such that the average squared reconstruction error is minimized and we can write the average squared reconstruction error as follows you can write j that's the average squared reconstruction error is going to be 1 over n times the sum n equals 1 to n and then we write x n minus x and tilde square let's have a look at an example we have data living in two dimensions and now we want to find a good one-dimensional subspace such that the squared or average squared reconstruction error of the data original data points and their corresponding projections is minimized here i'm plotting the original data set with their corresponding projections onto one-dimensional subspaces and i'm cycling through a couple of options of subspaces and you can see that some of these projections are significantly more informative than others and in pca we are going to find the best one our approach is to compute the partial derivatives of j with respect to the parameters the parameters are the beta i n and the bis we set the partial derivatives of j with respect to these parameters to zero and solve for the optimal parameters but one observation we can already make and that observation is that the parameters only enter this loss function through n tilde this means that in order to get our partial derivatives we need to apply the chain rule so we can write dj by d either beta i n or b i can be written as d j by d x n tilde times d x n tilde by d either beta i n or bi and the first part we can already compute and we get d j by d x and tilde is minus 2 over n times x n minus x and tilde transpose and the other derivatives we compute in the next videos in the last video we set up the pca objective and in this video we will determine our first set of optimal parameters we make two general assumptions in the beginning the first thing is that we have centered data that means that the expected value of our data set is zero and the second one is that the basis vectors form an orthonormal basis from the previous video we carry over the following results first we can write our projected data point x and tilde as a linear combination bjn times bj where bj form the orthonormal basis of our subspace our loss function is the average squared reconstruction error between our original data point and the projection and the partial derivative of our loss function with respect to x and tilde is given by this expression and now we are ready to compute the partial derivative of j with respect to the beta i n parameters as follows so dj with respect to d b i n is the derivative of j with respect to x n tilde times the derivative of x and tilde with respect to beta i n so now we're going to have a closer look at this one here so d x n tilde by d beta i n is simply given by b i for i equals 1 to m and the reason for this is that if we take the derivative with respect to one fixed beta i n then only the i component of this sum will play a role and with that that's the reason why we end up simply with bi but that also means that our derivative of j with respect to d beta i n is now given as dj by d beta i n is minus 2 over n times x n minus x and tilde transpose times b i where here we used equation c to get the first part and we plugged in this bi over here and what we're going to do now is we are going to replace x and tilde using equation a so we end up with minus 2 over n times x n minus the sum of j equals 1 to m b j n times so beta j n times b j transpose times b i and this is given as minus 2 over n times x n transpose times bi minus beta i n times b i transpose times b i where we exploited that the bi form an orthonormal basis if we multiply b i onto both components here we end up with the sum of b j n times b j transpose times b i and since b j transpose times b i is one if and only if i equals j and otherwise zero we end up with only one term which will be one so we end up with 2 n times x n transpose bi minus beta i n so now we need to set this to 0 in order to find our beta in parameters and this is zero if and only if the beta i n parameters are given by x and transpose times b i which are going to define as equation d what this means is that the optimal coordinates of x and tilde with respect to our bases are the orthogonal projections of the coordinates of our original data point onto the ith basis vector that spans our principal subspace in this video we determine the coordinates of the lower dimensional data as the orthogonal projection of the original data onto the basis vectors that span the principal subspace in the next videos we will determine the orthonormal basis that spans that principle subspace in the last video we determined the coordinates of the optimal projection with respect to the orthonormal basis that spans our principle subspace before we go on and determine the optimal basis factors let's rephrase our loss function first i have copied over the results that we have so far so the the description of our projected data point our loss function the partial derivative of our loss function with respect to our projected data point and the optimal coordinates that we found in the last video before we go on and determine the optimal basis vectors let's rephrase our loss function this will make it much easier to find our basis vectors for this let's have a closer look at the difference vector between our original data point and our projected data point so you can write so x and tilde is given by equation a which is the sum over j equals 1 to m beta j n times b j if we now use the results for our optimal beta jn parameters from here we get this is j equals 1 to m x n transpose times b j times b j where we used d now we rewrite this in the following way this is just a scalar or dot product in this particular case dot products are symmetric so we can swap the order and we can also move the scalar over here so what we end up with is bj times bj transpose times xn and this one we can write generally as j equals 1 to m times bj times bj transpose xn where we moved the xn out of the sum and if we look at this this is a projection matrix so this means that x n tilde is the orthogonal projection of x n onto the subspace spanned by the m basis vectors b j where j equals 1 to m similarly we can write x n as the sum j equals 1 to m of bj times bj transpose times xn plus a term that runs from m plus one to d b j times b j transpose times x n so we write x n as a projection onto the principle subspace plus a projection onto the orthogonal complement and this term is the one that is missing over here that's the reason why x n tilde is the approximation to x n so if we now look at the difference vector between x and tilde and x n what remains is exactly this term so x n minus x n tilde is the sum j equals m plus 1 to d of bj times bj transpose times xn so now we can look at this displacement vector so the difference between xn and its projection and we can see that the displacement vector lies exclusively in the subspace that we ignore that means the orthogonal complement to the principal subspace let's look at an example in two dimensions we have a data set in two dimensions represented by these dots and now we are interested in projecting them onto the u1 subspace when we do this and then look at the difference vector between the original data and the project data we get these vertical lines that means they have no x component or no variation in x that means they only have a component that lives in the subspace u2 which is the orthogonal complement to u1 which is the subspace that we projected onto so with this illustration let's quickly rewrite this in a slightly different way i'm going to write this as sum of j equals m plus 1 to d of b j transpose x n times b j and we're going to call this now equation e we looked at the displacement vector between xn and its orthogonal projection onto the principal subspace xn tilde and now we're going to use this to reformulate our loss function so from equation b we get that our loss function is 1 over n times the sum n equals 1 to n of x n minus x and tilde squared so this is the average squared reconstruction error and now we're going to use equation e for the displacement vector here so we rewrite this now using equation e as 1 over n times the sum n equals 1 to capital n and now we're going to use inside that squared norm this expression here so we get the sum j equals m plus 1 to d of bj time bj transpose times xn times bj squared and now we're going to use the fact that the bjs form an orthonormal basis and this will greatly simplify this expression and we will get 1 over n times the sum n equals 1 to capital n times the sum j equals m plus 1 to d of b j transpose times x n squared and now we're going to multiply this out explicitly and we get 1 over n times the sum over n times the sum over j times bj times x n times x n transpose times b j so this part is now identical to this part and now i'm going to rearrange the sums so i'm going to move the sum over j outside so i'll have sum over j equals m plus 1 to d times bj transpose so this is independent of n times 1 over n the sum n equals 1 to capital n of x n times x and transpose and there's a bj from here missing times bj so i'm going to bracket it now in this way and what we can see now is that if we look very carefully we can identify this expression as the data covariance matrix s because we assumed we have centered data so the mean of the data is zero this means now we can rewrite our loss function using the data covariance matrix and we get that our loss is the sum over j equals m plus 1 to d of bj transpose times s times bj and we can also use a slightly different interpretation by rearranging a few terms and using the trace operator so we can now also write this as the trace of the sum of j equals m plus 1 to d of bj times bj transpose times s and we can now also interpret this matrix as a projection matrix this projection matrix takes our data covariance matrix and projects it onto the orthogonal complement of the principal subspace that means we can reformulate the loss function as the variance of the data projected onto the subspace that we ignore therefore minimizing this loss is equivalent to minimizing the variance of the data that lies in the subspace that is orthogonal to the principal subspace in other words we're interested in retaining as much variance after projection as possible the reformulation of the average squared reconstruction error in terms of the data covariance gives us an easy way to find the basis vector of the principal subspace which we'll do in the next video the last video we found that minimizing the average squared reconstruction error is equivalent to minimizing the projection of the variance of the data when projected onto the subspace that we'll ignore in pca in this video we will exploit this insight and determine an orthonormal basis of the m-dimensional principle subspace using the results from earlier over here we can write our loss function as the sum j equals m plus 1. to d of bj transpose s times bj where s is the data covariance matrix minimizing this objective requires us to find the orthonormal basis that spans the subspace that we will ignore and when we have that basis we take its orthogonal complement as the basis of the principle subspace remember that the orthogonal complement of a subspace u consists of all vectors in the original vector space that are orthogonal to every vector in u let us start with an example to determine the b vectors and let's start in two dimensions where we wish to find a one-dimensional subspace such that the variance of the data when projected onto that subspace is minimized so we're looking at two basis vectors b1 and b2 in r2 so b1 and b2 and b1 will be spanning the principle subspace and b2 its orthogonal complement that means the subspace that we will ignore we also have the constraint that b1 and b2 are orthonormal which means that bi transpose times bj is delta ij meaning that this dot product is 1 if i equals j and 0 otherwise in our example with our two vectors our loss function is j is b2 transpose times s times b2 with the constraint that b2 transpose times b2 is 1. to solve this optimization problem we write down the lagrangian and the lagrangian is b2 transpose s b2 plus lambda times 1 minus b2 transpose times b2 where lambda is the lagrange multiplier so now we compute the gradients of the lagrangian with respect to b2 and with respect to lambda and set them to zero so dl d lambda is 1 minus b2 transpose times b2 and this is 0 if and only if b2 transpose times b2 is 1 so we recover our constraint so now let's have a look at the partial derivative of l with respect to b 2. so we get 2 b 2 transpose times s from the first term and minus 2 lambda b 2 transpose from the second term and this needs to be 0 and that is 0 if and only if s times b2 is lambda times b2 here we end up with an eigenvalue problem b2 is an eigenvector of the data covariance matrix and the lagrange multiplier plays the role of the corresponding eigenvalue if we now go back to our loss function we can use this expression we can write j which was b2 transpose times s times b2 now we know that s times b2 can be written as lambda times b2 so we get b2 transpose times b2 times lambda and because we have an orthonormal basis we end up with lambda as our loss function therefore the average squared reconstruction error is minimized if lambda is the smallest eigenvalue of the data covariance matrix and that means we need to choose b2 as the corresponding eigenvector and that one will span the subspace that we will ignore b1 which spans the principle subspace is then the eigenvector that belongs to the largest eigenvalue of the data covariance matrix keep in mind that the eigenvectors of the covariance matrix are already orthogonal to each other because of the symmetry of the covariance matrix so if we look at a two-dimensional example if this is our data then the best projection that we can get that retains most information is the one that projects onto the subspace that is spanned by the eigenvector of the data covariance matrix which belongs to the largest eigenvalue and that is indicated by this long arrow over here now let's go to the general case if we want to find the m-dimensional principle subspace of a d-dimensional data set and we solve for the basis vectors b j where j equals m plus 1 to d we optimize these ones we end up with the same kind of eigenvalue problems that we had earlier with a simple example we end up with s times bj equals lambda j times bj for j equals m plus 1 to d and the loss function is given by the sum of the corresponding eigenvalues so we can write j is the sum from m plus 1 to d of all lambda j also in the general case the average reconstruction error is minimized if we choose the basis vectors that span the ignored subspace to be the eigenvectors of the data covariance matrix that belong to the smallest eigenvalues this equivalently means that the principal subspace is spanned by the eigenvectors belonging to the m largest eigenvalues of the data covariance matrix this nicely aligns with properties of the covariance matrix the eigenvectors of the covariance matrix are orthogonal to each other because of symmetry and the eigenvector belonging to the largest eigenvalue points in the direction of the data with the largest variance and the variance in that direction is given by the corresponding eigenvalue similarly the eigenvector belonging to the second largest eigenvalue points in the direction of the second largest variance of the data and so on in this video we identified the orthonormal basis of the principal subspace as the eigenvectors of the data covariance matrix that are associated with the largest eigenvalues in the next video we will put all the pieces together and run through the pca algorithm in detail in this video we'll go through the individual steps of pca but before we do this let me make two comments when we derived pca we made the assumption that our data is centered that means it has mean zero this assumption is not necessary to derive pca and we would have come to the same result but subtracting the mean from the data can avoid numerical difficulties assume the values of our data are centered around 10 to the 8 then computing the data covariance matrix requires us to multiply huge numbers which results in numerical instabilities an additional second step that is normally recommended after subtracting the mean is to divide every dimension of the centered data by the corresponding standard deviation this makes the data unit free and guarantees that the variance of the data in every dimension is one but it leaves the correlations intact let's have a look at an example clearly this data spreads much more in one dimensions than the other dimension and the best projection of pca is clear however there's a problem with this data set the two dimensions of the dataset are both distances but one is measured in centimeters and the other one in meters the one measured in centimeters naturally varies much more than the other one when we divide each dimension of the data set by the corresponding standard deviation we get rid of the units and make sure that the variance in each dimension is one when we look at the principle subspace of this normalized data set we can now see that there is actually quite a strong correlation between these two dimensions and the principal axis have changed but now let's go through pca step by step and we'll have a running example we're given a two-dimensional data set and we want to use pca to project it onto a one-dimensional subspace the first thing that we do is to subtract the mean the data is now centered next we divide by the standard deviation now the data is unit free and it has variance one along each axis which is indicated by these two arrows but keep in mind that the correlations are still intact third we compute the data covariance matrix and its eigenvalues and corresponding eigenvectors the eigenvectors are scaled by the magnitude of the corresponding eigenvalue in this picture the longer vector spans the principal subspace let's call it u and in the last step we can project any data point x star onto the principal subspace to get this right we need to normalize x star using the mean and standard deviation of the data set that we use to compute the data covariance matrix so we're going to have a new x star and the new x star is going to be the old x star minus the mean of the data set divided by the standard deviation and we do this for every dimension in x star now we can get the projection of x star as x-star tilde or the projection of x-star onto the principle subspace u as b times b transpose times x star where b is the matrix that contains the eigenvectors that belong to the largest eigenvalues as columns and b transpose times x star are the coordinates of the projection with respect to the basis of the principle subspace in this video we went through the steps of pca first we subtract the mean from the data and centered at zero to avoid numerical problems second we divide by the standard deviation to make the data unit free third we compute the eigenvalues and eigenvectors of a data covariance matrix and finally we can project any data point onto the principal subspace that is spanned by the eigenvectors that belong to the largest eigenvalues in the last video we've gone through the steps of the pca algorithm in order to do pca we need to compute the data covariance matrix in d dimensions the data covariance matrix is a d by d matrix if d is very high so in very high dimensions then computing the eigenvalues and eigenvectors of this matrix can be quite expensive it scales cubically in the number of rows and columns in this case the number of dimensions in this video we provide a solution to this problem for the case that we have substantially fewer data points than dimensions assume we have a data set given as x1 up to xn in rd and we assume that the data is centered so it has mean 0 then the data covariance matrix is given as s equals 1 over n times x transpose times x where we define x to be the matrix that consists of x1 transpose up to x n transpose and that is an n by d matrix we now assume that n is significantly smaller than d that means the number of data points is significantly smaller than the dimensionality of the data and then the rank of the covariance matrix is n so rank of s equals n and that also means it has d minus n plus one many eigenvalues which are zero that means that the matrix is not full rank and the rows and columns are linearly dependent in other words there are some redundancies in the next few minutes we'll exploit this and turn the d by d covariance matrix s into a full rank n by n covariance matrix without eigenvalues 0. i've just moved the covariance definition up here to have a bit more space on the board in pca we ended up with the following eigenvalue eigenvector equation we had s times b i equals lambda i times b i where b i is the basis vector of the orthogonal complement of the principal subspace now let's rewrite this equation a little bit we're now going to replace s with a definition up here so we'll get 1 over n times x transpose x this is s times b i equals lambda i times bi and now we multiply x from the left hand side so we will get x times x transpose x b i times 1 over n equals lambda i times x times bi and now we have a new eigenvector eigenvalue equation so lambda i is still an eigenvalue and now we have eigenvectors x times bi which we call ci of the matrix 1 over n times x times x transpose this means that 1 over n times x x transpose has the same non-zero eigenvalues as the data covariance matrix but this is now an n by n matrix so that we can compute the eigenvalues and eigenvectors much quicker than for the original data covariance matrix all right so this is an n by n matrix whereas s used to be a d by d matrix so now we can compute the eigenvectors of this matrix one over n times x x transpose and we use this to recover the original eigenvectors which we still need to do pca currently we know the eigenvectors of one over n times x x transpose and we want to recover the eigenvectors of s if we left multiply our eigenvalue eigenvector equation with x transpose we get the following so you get 1 over n times x transpose times x times x transpose times c i equals lambda i times x transpose times c i and now we find our s matrix again this is s and this also means that we will cover x transpose times c i as an eigenvector of s that belongs to the eigenvalue lambda i in this video we reformulated pca such that we can efficiently run pca on data sets where the dimensionality of the data is substantially bigger than the number of data points we derive pca from the perspective of minimizing the average squared reconstruction error however pca can also be interpreted from different perspectives in this video we'll have a brief look at some of these interpretations let's start with a recap what we have done so far we took a high dimensional vector x and we projected it onto a lower dimensional representation z using the matrix b transpose the columns of this matrix b are the eigenvectors of the data covariance matrix that are associated with the largest eigenvalues the z values are the coordinates of our data point with respect to the basis vectors which span the principle subspace and that is also called the code of our data point once we have that low dimensional representation z we can get a higher dimensional version of it by using the matrix b again so multiplying b onto z to get a higher dimensional version of the z in the original data space we found the pca parameters such that the reconstruction error between x and the reconstruction x tilde is minimized we can also think of pca as a linear autoencoder an autoencoder encodes a data point x and tries to decode it to something similar to the same data point the mapping from the data to the code is called an encoder let's write this down so this part here is called an encoder the mapping from the code to the original data space is called the decoder if the encoder and decoder are linear mappings then we get the pca solution when we minimize the squared autoencoding loss if we replace the linear mapping of pca with a non-linear mapping we get a nonlinear auto encoder a prominent example of this is a deep autoencoder where the linear functions of the encoder and decoder are replaced with deep neural networks another interpretation of pca is related to information theory we can think of the code as a smaller compressed version of the original data point when we reconstruct our original data using the code we don't get the exact data point back but a slightly distorted or noisy version of it this means that our compression is lossy intuitively we want to maximize the correlation between the original data and the lower dimensional code more formally this would be related to the mutual information we would then get the same solution to pca we discussed earlier in this course by maximizing the mutual information a core concept in information theory when we derived pca using projections we reformulated the average reconstruction error loss as minimizing the variance of the data that is projected onto the orthogonal complement of the principal subspace minimizing that variance is equivalent to maximizing the variance of the data when projected onto the principal subspace if we think of variance in the data as information contained in the data this means that pca can also be interpreted as a method that retains as much information as possible we can also look at pca from the perspective of a latent variable model we assume that an unknown lower dimensional code z generates data x and we assume that we have a linear relationship between z and x so generally we can then write that x is b times z plus mu and maybe some noise we assume that the noise is isotropic with mean zero and covariance matrix sigma squared times i we further assume that the distribution of this z is a standard normal so p of z is gaussian with mean zero and covariance matrix the identity matrix we can now write down the likelihood of this model so the likelihood is p of x given z and that is a gaussian distribution in x with mean b z plus mu and covariance matrix sigma squared i and we can also compute the marginal likelihood as p of x is the integral of p of x given z so that is the likelihood times the distribution on z dz and that turns out to be a gaussian distribution in x with mean mu and with covariance matrix b times b transpose plus sigma squared i the parameters of this model are mu b and sigma squared and we can write them explicitly down in our model up here so models model parameters are b and mu and sigma squared we can now determine the parameters of this model using maximum likelihood estimation and we will find that mu is the mean of the data and b is a matrix that contains the eigenvectors that correspond to the largest eigenvalues to get the low dimensional code of a data point we can apply bayes theorem to invert the linear relationship between z and x in particular we are going to get p of z given x as p of x given z so that is the the likelihood which is comes from here times p of z so that's our distribution that we have here divided by the marginal likelihood p of x which comes from here in this video we looked at five different perspectives of pca that lead to different objectives minimizing the squared reconstruction error minimizing the autoencoder loss maximizing the mutual information maximizing the variance of the projected data and maximizing the likelihood in a latent variable model all these different perspectives give us the same solution to the pca problem the strengths and weaknesses of individual perspectives become more clear and important when we consider properties of real data in this module we derived pca and looked at some of its properties by minimizing the average squared error between a data point and its projection onto the principle subspace we found that the best thing to do is to project onto the subspace that is spanned by the eigenvectors that belong to the largest eigenvalues pca has a lot of similarities to other machine learning algorithms so it's good that we had a closer look at it this module was very hard but i hope you gained some new and interesting insights either through the detailed derivation or through the practical exercises well done completing this module in this course we looked at pca a practical machine learning algorithm for dimensionality reduction and data compression we covered a lot of material to get to the point where we could derive and understand how pca works we started off with some summary statistics of data means and variances which we later used to compute the data covariance matrix in pca then we looked at inner products which allowed us to compute the length distances and angles between vectors we use these inner products for orthogonal projections of data onto lower dimensional subspaces and in the end we put everything together for the pca algorithm i'm aware that this course was challenging at times but i hope you enjoyed it nevertheless a big thanks to everyone who made it this far and completed the course very well done 